% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[
  12pt, krantz2,
]{krantz}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
  \setmonofont[Scale=0.85]{Inconsolata}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={HR Analytics in R},
  pdfauthor={Chester Ismay, Albert Y. Kim and Hendrik Feddersen},
  colorlinks=true,
  linkcolor=Maroon,
  filecolor=Maroon,
  citecolor=Blue,
  urlcolor=Blue,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.33,0.33,0.33}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.61,0.61,0.61}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.14,0.14,0.14}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.43,0.43,0.43}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{bbm}
\usepackage[bf,singlelinecheck=off]{caption}

\usepackage{framed,color}
\definecolor{shadecolor}{RGB}{248,248,248}

\usepackage{float}
\floatplacement{figure}{H}
\floatplacement{table}{H}
\setlength{\textfloatsep}{0.1cm}
\usepackage{array}
\usepackage{multirow}
%\usepackage[table]{xcolor}
\usepackage{wrapfig}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{kotex}

% Changed via Rob Hyndman's suggestions
% https://robjhyndman.com/hyndsight/latex-floats/
\setcounter{topnumber}{2}
\setcounter{bottomnumber}{2}
\setcounter{totalnumber}{4}
\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\textfraction}{0.15}
\renewcommand{\floatpagefraction}{0.7}
% Force images to appear after their definition
\usepackage{flafter}

\renewenvironment{quote}{\begin{VF}}{\end{VF}}
\let\oldhref\href
\renewcommand{\href}[2]{#2\footnote{\url{#1}}}

\makeatletter
\newenvironment{kframe}{%
\medskip{}
\setlength{\fboxsep}{.8em}
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\renewenvironment{Shaded}{\begin{kframe}}{\end{kframe}}

\usepackage{makeidx}
\makeindex

\urlstyle{tt}

%% Need to clean up
\newenvironment{rmdblock}[1]
  {\begin{shaded*}
  \begin{itemize}
  \renewcommand{\labelitemi}{
    \raisebox{-.7\height}[0pt][0pt]{
  %    {\setkeys{Gin}{width=3em,keepaspectratio}\includegraphics{images/#1}}
    }
  }
  \item
  }
  {
  \end{itemize}
  \end{shaded*}
  }

\newenvironment{rmdnote}
  {\begin{rmdblock}{note}}
  {\end{rmdblock}}
\newenvironment{rmdcaution}
  {\begin{rmdblock}{caution}}
  {\end{rmdblock}}
\newenvironment{rmdimportant}
  {\begin{rmdblock}{important}}
  {\end{rmdblock}}
\newenvironment{rmdtip}
  {\begin{rmdblock}{tip}}
  {\end{rmdblock}}
\newenvironment{rmdwarning}
  {\begin{rmdblock}{warning}}
  {\end{rmdblock}}
\newenvironment{learncheck}
  {\begin{rmdblock}{warning}}
  {\end{rmdblock}}
\newenvironment{review}
  {\begin{rmdblock}{warning}}
  {\end{rmdblock}}
\newenvironment{announcement}
  {\begin{rmdblock}{warning}}
  {\end{rmdblock}}

% No widow lines
% Copied from https://github.com/hadley/adv-r/blob/master/latex/preamble.tex
\widowpenalty=10000
\clubpenalty=10000

\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\listfiles

\frontmatter
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{HR Analytics in R}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Common tasks achieved with the power of R}
\author{Chester Ismay, Albert Y. Kim and Hendrik Feddersen}
\date{October 18, 2020}

\begin{document}
\maketitle

% you may need to leave a few empty pages before the dedication page (CRC guidelines are that the Table of Contents starts on page v. The title page is page i.)
\cleardoublepage\newpage
\thispagestyle{empty}

\begin{center}
Chester: To Karolyn, who has always been patient and continued to support me to write and finish edits on this book even sometimes when that meant working through a weekend.  Your continued pushes for me to think about new ways to teach novices about R have meant the world to me.

\vspace{0.3in}

Albert: 엄마와 아빠: 나 한번도 표현 못해도 그냥 다 고마워요. 행복하게 살수 있는 지금 모습이 모두가 다 당신때문이예요. To Ginna: Thanks for tolerating my playing of ``Nothing In This World Will Ever Break My Heart Again'' on repeat while I finished this book. I love you.
\end{center}

\setlength{\abovedisplayskip}{-5pt}
\setlength{\abovedisplayshortskip}{-5pt}

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoftables
\listoffigures
\#Bash does not work for me on Windows PC, I am purling manually.
\#\texttt{\{bash,\ include=FALSE,\ purl=FALSE\}\ \#Rscript\ -e\ "source(\textquotesingle{}purl.R\textquotesingle{},\ local\ =\ TRUE)"\ \#}

\hypertarget{intro}{%
\chapter{Introduction}\label{intro}}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{learncheck}
\textbf{Please note that you are currently looking at the latest version
of ``HR Analytics in R''. However, since work is still in progress
subject to change about twice a year.}
\end{learncheck}

The intention of this book is to encourage more `data driven' decisions in HR. HR Analytics is not anymore a nice-to-have addon but rather the way HR practitioners should conduct HR decision making in the future. Where applicable, human judgement is `added' onto a rigorous analysis of the data done in the first place.

To achieve this ideal world, I need to equip you with some fundamental knowledge of R and RStudio, which are open-source tools for data scientists. I am well aware that on one side you want to do something for your career in HR, however you are most likely completely new to coding.

\vspace{0.1in}

\includegraphics[width=\textwidth,height=0.1\textheight]{images/logos/Rlogo.png} \hfill       \includegraphics[width=\textwidth,height=0.1\textheight]{images/logos/RStudio-Logo-Blue-Gradient.png}

\textbf{Help! I'm new to R and RStudio and I need to learn about them! However, I'm completely new to coding! What do I do?}

\vspace{0.1in}

If you're asking yourself this question, then you have come to the right place! There is no better moment to ride the wave of disruptions taking place now in HR.

\begin{itemize}
\tightlist
\item
  \emph{Are you looking to learn about HR Analytics utilising the power of R"? Then start with our \protect\hyperlink{sec:intro-for-students}{Introduction for Students}.}
\item
  \emph{Are you looking to contribute to ``HR Analytics in R''? Then click \protect\hyperlink{sec:connect-contribute}{here} for information on how.}
\item
  \emph{Are you curious about the publishing of this book? Then click \protect\hyperlink{sec:about-book}{here} for more information on the open-source technology, in particular R Markdown and the bookdown package.}
\end{itemize}

This is version 1.1.0 of ``HR Analytics in R'' published on October 10, 2020. While a PDF version of this book can be found \href{hendrikfeddersen.pdf}{here}, this is very much a work in progress with many things that still need to be fixed. I appreciate your patience.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sec:intro-for-students}{%
\section{Introduction for students}\label{sec:intro-for-students}}

This book assumes no prerequisites: no algebra, no calculus, and no prior programming/coding experience. This is intended to be a gentle introduction to the practice of analyzing data and answering questions using data the way data scientists, statisticians and other researchers would.

\hypertarget{workingwithmaterials}{%
\subsection*{Working with the material}\label{workingwithmaterials}}


You can work your way through the materials by clicking on the arrows to the left and right at the bottom of each page. Alternatively, there is a collapsible contents bar on the left hand side.

If you need to find something specific, you can use the search icon. Typing in a word or phrase will filter the contents bar to relevant sections.

The book by default renders black sans serif on a white background. You can use the A to amend the appearance of the book to make it easier to process, whether that's a larger font, a serif font, or a different colour scheme.

The edit button takes you straight to github, where you can propose editorial changes.

\hypertarget{conventions}{%
\subsection*{Conventions}\label{conventions}}


Throughout this book various conventions will be used.

In terms of basic formatting:

\begin{itemize}
\tightlist
\item
  This is standard text.
\item
  \texttt{This\ is\ code\ or\ a\ symbol}
\item
  \emph{This is a Keyboard Key!}
\item
  \textbf{This is the first time I mention something important}
\end{itemize}

This is a book about coding, so expect code blocks. Code blocks will typically look like this:

\begin{Shaded}
\begin{Highlighting}[]
\StringTok{"this is a code block"}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "this is a code block"
\end{verbatim}

Directly underneath it, normally starting with two hash symbols (\#\#) is the result of the code executing.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# [1] \textquotesingle{}this is a code block\textquotesingle{}\textasciigrave{}}
\end{Highlighting}
\end{Shaded}

There will also be callouts throughout the book. Some are for information, some expect you to do things.

Anything written here should be read carefully before proceeding.

This is a tip relating to what I've just said.

This is kind of like a tip but is for when you're getting into trouble and need help.

This is something I recommend you do as you're reading.

In Figure \ref{fig:moderndive-figure} I present a flowchart of what you'll cover in this book. You'll first get started with data in Chapter \ref{getting-started}, where you'll learn about the difference between R and RStudio, start coding in R, understand what R packages are, and explore your first dataset: all domestic departure flights from a New York City airport in 2013. Then

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Data science}: You'll assemble your data science toolbox using \texttt{tidyverse} packages. In particular:

  \begin{itemize}
  \tightlist
  \item
    Ch.\ref{viz}: Visualizing data via the \texttt{ggplot2} package.
  \item
    Ch.\ref{tidy}: Understanding the concept of ``tidy'' data as a standardized data input format for all packages in the \texttt{tidyverse}
  \item
    Ch.\ref{wrangling}: Wrangling data via the \texttt{dplyr} package.
  \end{itemize}
\item
  \textbf{Data modeling}: Using these data science tools and helper functions from the \texttt{moderndive} package, you'll start performing data modeling. In particular:

  \begin{itemize}
  \tightlist
  \item
    Ch.\ref{regression}: Constructing basic regression models.
  \item
    Ch.\ref{multiple-regression}: Constructing multiple regression models.
  \end{itemize}
\item
  \textbf{Statistical inference}: Once again using your newly acquired data science tools, I'll unpack statistical inference using the \texttt{infer} package. In particular:

  \begin{itemize}
  \tightlist
  \item
    Ch.\ref{sampling}: Understanding the role that sampling variability plays in statistical inference using both tactile and virtual simulations of sampling from a ``bowl'' with an unknown proportion of red balls.
  \item
    Ch.\ref{confidence-intervals}: Building confidence intervals.
  \item
    Ch.\ref{hypothesis-testing}: Conducting hypothesis tests.
  \end{itemize}
\item
  \textbf{Data modeling revisited}: Armed with your new understanding of statistical inference, you'll revisit and review the models you constructed in Ch.\ref{regression} \& Ch.\ref{multiple-regression}. In particular:

  \begin{itemize}
  \tightlist
  \item
    Ch.\ref{inference-for-regression}: Interpreting both the statistical and practice significance of the results of the models.
  \item
    Ch.\ref{thinking-with-data}: I'll end the introductory chapters with a discussion on what it means to ``think with data'' and present an example case study data analysis of house prices in Seattle.
  \end{itemize}
\item
  \textbf{HR Analytics - data driven decision making}: The intention is to provide real tangible examples of the application of data science to HR, to illustrate the data science process in the HR context, and to show that the scope mentioned previously in this article, isn't just theoretical - it's real. The last and most important module shall illustrate current best practices of a structured process of thinking and analysis.
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{images/flowcharts/flowchart/flowchart.002} 

}

\caption{ModernDive Flowchart}\label{fig:moderndive-figure}
\end{figure}

\hypertarget{subsec:learning-goals}{%
\subsection{What you will learn from this book}\label{subsec:learning-goals}}

I hope that by the end of this book, you'll have learned

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  How to use R to explore data.\\
\item
  How to answer statistical questions using tools like confidence intervals and hypothesis tests.
\item
  How to effectively create ``data stories'' using these tools.
\end{enumerate}

What do I mean by data stories? I mean any analysis involving data that engages the reader in answering questions with careful visuals and thoughtful discussion, such as \href{http://rpubs.com/ry_lisa_elana/chicago}{How strong is the relationship between per capita income and crime in Chicago neighborhoods?} and \href{https://ismayc.github.io/soc301_s2017/group_projects/group4.html}{How many f**ks does Quentin Tarantino give (as measured by the amount of swearing in his films)?}. Further discussions on data stories can be found in this \href{https://www.thinkwithgoogle.com/marketing-resources/data-measurement/tell-meaningful-stories-with-data/}{Think With Google article}.

For other examples of data stories constructed by students like yourselves, look at the final projects for two courses that have previously used ModernDive:

\begin{itemize}
\tightlist
\item
  Middlebury College \href{https://rudeboybert.github.io/MATH116/PS/final_project/final_project_outline.html\#past_examples}{MATH 116 Introduction to Statistical and Data Sciences} using student collected data.
\item
  Pacific University \href{https://ismayc.github.io/soc301_s2017/group-projects/index.html}{SOC 301 Social Statistics} using data from the \href{https://cran.r-project.org/web/packages/fivethirtyeight/vignettes/fivethirtyeight.html}{fivethirtyeight R package}.
\end{itemize}

This book will help you develop your ``data science toolbox'', including tools such as data visualization, data formatting, data wrangling, and data modeling using regression. With these tools, you'll be able to perform the entirety of the ``data/science pipeline'' while building data communication skills (see Subsection \ref{subsec:pipeline} for more details).

In particular, this book will lean heavily on data visualization. In today's world, we are bombarded with graphics that attempt to convey ideas. I will explore what makes a good graphic and what the standard ways are to convey relationships with data. You'll also see the use of visualization to introduce concepts like mean, median, standard deviation, distributions, etc. In general, I'll use visualization as a way of building almost all of the ideas in this book.

To impart the statistical lessons in this book, I have intentionally minimized the number of mathematical formulas used and instead have focused on developing a conceptual understanding via data visualization, statistical computing, and simulations. I hope this is a more intuitive experience than the way statistics has traditionally been taught in the past and how it is commonly perceived.

Finally, you'll learn the importance of literate programming. By this I mean you'll learn how to write code that is useful not just for a computer to execute but also for readers to understand exactly what your analysis is doing and how you did it. This is part of a greater effort to encourage reproducible research (see Subsection \ref{subsec:reproducible} for more details). Hal Abelson coined the phrase that I will follow throughout this book:

\begin{quote}
``Programs must be written for people to read, and only incidentally for machines to execute.''
I understand that there may be challenging moments as you learn to program. Both of us continue to struggle and find ourselves often using web searches to find answers and reach out to colleagues for help. In the long run though, we all can solve problems faster and more elegantly via programming. I wrote this book as our way to help you get started and you should know that there is a huge community of R users that are always happy to help everyone along as well. This community exists in particular on the internet on various forums and websites such as \href{https://stackoverflow.com/}{stackoverflow.com}.
\end{quote}

\hypertarget{subsec:pipeline}{%
\subsection{Data/science pipeline}\label{subsec:pipeline}}

You may think of statistics as just being a bunch of numbers. I commonly hear the phrase ``statistician'' when listening to broadcasts of sporting events. Statistics (in particular, data analysis), in addition to describing numbers like with baseball batting averages, plays a vital role in all of the sciences. You'll commonly hear the phrase ``statistically significant'' thrown around in the media. You'll see articles that say ``Science now shows that chocolate is good for you.'' Underpinning these claims is data analysis. By the end of this book, you'll be able to better understand whether these claims should be trusted or whether we should be wary. Inside data analysis are many sub-fields that I will discuss throughout this book (though not necessarily in this order):

\begin{itemize}
\tightlist
\item
  data collection
\item
  data wrangling
\item
  data visualization
\item
  data modeling
\item
  inference
\item
  correlation and regression
\item
  interpretation of results
\item
  data communication/storytelling
\end{itemize}

These sub-fields are summarized in what Grolemund and Wickham term the \href{http://r4ds.had.co.nz/explore-intro.html}{``Data/Science Pipeline''} in Figure \ref{fig:pipeline-figure}.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{images/tidy1} 

}

\caption{Data/Science Pipeline}\label{fig:pipeline-figure}
\end{figure}

I will begin by digging into the gray \textbf{Understand} portion of the cycle with data visualization, then with a discussion on what is meant by tidy data and data wrangling, and then conclude by talking about interpreting and discussing the results of our models via \textbf{Communication}. These steps are vital to any statistical analysis. But why should you care about statistics? ``Why did they make me take this class?''

There's a reason so many fields require a statistics course. Scientific knowledge grows through an understanding of statistical significance and data analysis. You needn't be intimidated by statistics. It's not the beast that it used to be and, paired with computation, you'll see how reproducible research in the sciences particularly increases scientific knowledge.

\hypertarget{subsec:reproducible}{%
\subsection{Reproducible research}\label{subsec:reproducible}}

\begin{quote}
``The most important tool is the \emph{mindset}, when starting, that the end product will be reproducible.'' -- Keith Baggerly
Another goal of this book is to help readers understand the importance of reproducible analyses. The hope is to get readers into the habit of making their analyses reproducible from the very beginning. This means I'll be trying to help you build new habits. This will take practice and be difficult at times. You'll see just why it is so important for you to keep track of your code and well-document it to help yourself later and any potential collaborators as well.
\end{quote}

Copying and pasting results from one program into a word processor is not the way that efficient and effective scientific research is conducted. It's much more important for time to be spent on data collection and data analysis and not on copying and pasting plots back and forth across a variety of programs.

In a traditional analyses if an error was made with the original data, we'd need to step through the entire process again: recreate the plots and copy and paste all of the new plots and our statistical analysis into your document. This is error prone and a frustrating use of time. I'll see how to use R Markdown to get away from this tedious activity so that we can spend more time doing science.

\begin{quote}
``We are talking about \emph{computational} reproducibility.'' - Yihui Xie
Reproducibility means a lot of things in terms of different scientific fields. Are experiments conducted in a way that another researcher could follow the steps and get similar results? In this book, I will focus on what is known as \textbf{computational reproducibility}. This refers to being able to pass all of one's data analysis, data-sets, and conclusions to someone else and have them get exactly the same results on their machine. This allows for time to be spent interpreting results and considering assumptions instead of the more error prone way of starting from scratch or following a list of steps that may be different from machine to machine.
\end{quote}

\hypertarget{final-note-for-students}{%
\subsection{Final note for students}\label{final-note-for-students}}

At this point, if you are interested in instructor perspectives on this book, ways to contribute and collaborate, or the technical details of this book's construction and publishing, then continue with the rest of the chapter below. Otherwise, let's get started with R and RStudio in Chapter \ref{getting-started}!

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sec:intro-instructors}{%
\section{Introduction for instructors}\label{sec:intro-instructors}}

This book is inspired by the following books:

\begin{itemize}
\tightlist
\item
  ``Mathematical Statistics with Resampling and R'' \citep{hester2011},
\item
  ``OpenIntro: Intro Stat with Randomization and Simulation'' \citep{isrs2014}, and
\item
  ``R for Data Science'' \citep{rds2016}.
\end{itemize}

The first book, while designed for upper-level undergraduates and graduate students, provides an excellent resource on how to use resampling to impart statistical concepts like sampling distributions using computation instead of large-sample approximations and other mathematical formulas. The last two books are free options to learning introductory statistics and data science, providing an alternative to the many traditionally expensive introductory statistics textbooks.

When looking over the large number of introductory statistics textbooks that currently exist, I found that there wasn't one that incorporated many newly developed R packages directly into the text, in particular the many packages included in the \href{http://tidyverse.org/}{\texttt{tidyverse}} collection of packages, such as \texttt{ggplot2}, \texttt{dplyr}, \texttt{tidyr}, and \texttt{broom}. Additionally, there wasn't an open-source and easily reproducible textbook available that exposed new learners all of three of the learning goals listed at the outset of Subsection \ref{subsec:learning-goals}.

\hypertarget{who-is-this-book-for}{%
\subsection{Who is this book for?}\label{who-is-this-book-for}}

This book is intended for instructors of traditional introductory statistics classes using RStudio, either the desktop or server version, who would like to inject more data science topics into their syllabus. I assume that students taking the class will have no prior algebra, calculus, nor programming/coding experience.

Here are some principles and beliefs I kept in mind while writing this text. If you agree with them, this might be the book for you.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Blur the lines between lecture and lab}

  \begin{itemize}
  \tightlist
  \item
    With increased availability and accessibility of laptops and open-source non-proprietary statistical software, the strict dichotomy between lab and lecture can be loosened.
  \item
    It's much harder for students to understand the importance of using software if they only use it once a week or less. They forget the syntax in much the same way someone learning a foreign language forgets the rules. Frequent reinforcement is key.
  \end{itemize}
\item
  \textbf{Focus on the entire data/science research pipeline}

  \begin{itemize}
  \tightlist
  \item
    I believe that the entirety of Grolemund and Wickham's \href{http://r4ds.had.co.nz/introduction.html}{data/science pipeline} should be taught.
  \item
    I believe in \href{https://arxiv.org/abs/1507.05346}{``minimizing prerequisites to research''}: students should be answering questions with data as soon as possible.
  \end{itemize}
\item
  \textbf{It's all about the data}

  \begin{itemize}
  \tightlist
  \item
    I leverage R packages for rich, real, and realistic data-sets that at the same time are easy-to-load into R, such as the \texttt{nycflights13} and \texttt{fivethirtyeight} packages.
  \item
    I believe that \href{http://escholarship.org/uc/item/84v3774z}{data visualization is a gateway drug for statistics} and that the Grammar of Graphics as implemented in the \texttt{ggplot2} package is the best way to impart such lessons. However, I often hear: ``You can't teach \texttt{ggplot2} for data visualization in intro stats!'' I, like \href{http://varianceexplained.org/r/teach_ggplot2_to_beginners/}{David Robinson}, are much more optimistic.
  \item
    \texttt{dplyr} has made data wrangling much more \href{http://chance.amstat.org/2015/04/setting-the-stage/}{accessible} to novices, and hence much more interesting data-sets can be explored.
  \end{itemize}
\item
  \textbf{Use simulation/resampling to introduce statistical inference, not probability/mathematical formulas}

  \begin{itemize}
  \tightlist
  \item
    Instead of using formulas, large-sample approximations, and probability tables, statistical concepts using resampling-based inference.
  \item
    This allows for a de-emphasis of traditional probability topics, freeing up room in the syllabus for other topics.
  \end{itemize}
\item
  \textbf{Early exposure to analytics and computing}

  \begin{itemize}
  \tightlist
  \item
    Computing skills are essential to working with data in the 21st century even for HR managers. Given this fact, I feel that an early exposure to computing can only be of benefit to the whole HR community.
  \item
    I am not teaching a course on coding/programming per se, but rather just enough of the computational and algorithmic thinking necessary for performing a data analysis in HR.
  \end{itemize}
\item
  \textbf{Complete reproducibility and customisability}

  \begin{itemize}
  \tightlist
  \item
    I am frustrated when people talk about HR Analytics, without giving the source code and the data itself. I give you the source code for all examples as well as the whole book!
  \item
    If you want you can even use my book as a starting point and customise for your own non-profit training. For more about how to make this book your own, see \protect\hyperlink{sec:about-book}{About this Book}.
  \end{itemize}
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sec:connect-contribute}{%
\section{Connect and contribute}\label{sec:connect-contribute}}

If you would like to connect with ``HR Analytics in R'', check out the following links:

\begin{itemize}
\tightlist
\item
  If you would like to receive periodic updates about HR Analytics, then please sign up for my \href{https://hranalytics.live/signup/}{mailing list}. You will receive receive bi-weekly notififications about my new blog posts.
\item
  Please feel free to contact me at \href{mailto:info@hranalytics.live}{\nolinkurl{info@hranalytics.live}} .
\item
  I am on Twitter at \href{https://twitter.com/h_feddersen}{h\_feddersen}.
\end{itemize}

If you would like to contribute to ``HR Analytics in R'', there are many ways! Let's all work together to make this book as great as possible for as many students as possible!

\begin{itemize}
\tightlist
\item
  Please let me know if you find any errors, typos, or areas from improvement on my \href{https://github.com/Hendrik147/HR_Analytics_in_R_book/issues}{GitHub issue page} page. I will fix it as soon as possible.
\item
  If you are familiar with GitHub and would like to contribute even more, please see Section \ref{sec:about-book} below.
\end{itemize}

I would like to thank \href{https://github.com/moderndive/moderndive_book}{Moderndive} for their inspirational presentation at a recent R user conference and for their generous example on how to set up a bookdown book and for their introductory pages on how to start using R.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sec:about-book}{%
\section{About this book}\label{sec:about-book}}

This book was written using RStudio's \href{https://bookdown.org/}{bookdown} package by Yihui Xie \citep{R-bookdown}. This package simplifies the publishing of books by having all content written in \href{http://rmarkdown.rstudio.com/html_document_format.html}{R Markdown}.

\begin{itemize}
\item
  \textbf{Latest published version, still in development} The most up-to-date version, which is still in development is available at \url{https://hranalyticslive.netlify.com/}
\item
  \textbf{Source code} The bookdown/R Markdown source code for the latest version of ``HR Analytics in R'' is available on Hendrik Feddersen's \href{https://github.com/Hendrik147/HR_Analytics_in_R_book}{GitHub repository page}
\item
  \textbf{Usage} You can share this material with colleagues or for non-commercial purposes but you can't resell or incorporate them into stuff you make money from.

  \begin{itemize}
  \tightlist
  \item
    As a symbol of gratitude, I would expect at least that you to sign up for my \href{https://hranalytics.live/signup/}{mailing list}.
  \item
    If you think my material is awesome and want to use it for commercial purposes, please contact me at \href{mailto:info@hranalytics.live}{\nolinkurl{info@hranalytics.live}}
  \end{itemize}
\item
  \textbf{Licence} This work is licensed under a Creative Commons
  Attribution-NonCommercial 4.0 International License.

  \begin{figure}
  \centering
  \includegraphics{images/by-nc-sa.png}
  \caption{Creative Commons License}
  \end{figure}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{sec:about-authors}{%
\section{About the author}\label{sec:about-authors}}

Who am I? I am Hendrik Feddersen, a long-standing HR practitioner passionate about HR Analytics and living in Amsterdam, the Netherlands.

\includegraphics[width=0.4\textwidth,height=\textheight]{images/Photo HF Snappy.jpg}

\begin{itemize}
\tightlist
\item
  Email: \href{mailto:info@hranalytics.live}{\nolinkurl{info@hranalytics.live}}
\item
  Webpage: \url{https://hranalytics.live/}
\item
  Twitter: \href{https://twitter.com/h_feddersen}{h\_feddersen}
\item
  GitHub: \url{https://github.com/Hendrik147}
\end{itemize}

\mainmatter

\hypertarget{getting-started}{%
\chapter{Getting Started with Data in R}\label{getting-started}}

Before we can start exploring data in R, there are some key concepts to understand first:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What are R and RStudio?
\item
  How do I code in R?
\item
  What are R packages?
\end{enumerate}

We'll introduce these concepts in the upcoming Sections \ref{r-rstudio}-\ref{packages}. If you are already somewhat familiar with these concepts, feel free to skip to Section \ref{nycflights13} where we'll introduce our first dataset: all domestic flights departing one of the three main New York City (NYC) airports in 2013. This is a dataset we will explore in depth for much of the rest of this book.

\hypertarget{r-rstudio}{%
\section{What are R and RStudio?}\label{r-rstudio}}

Throughout this book, we will assume that you are using R via RStudio. First time users often confuse the two. At its simplest, R is like a car's engine\index{R} while RStudio is like a car's dashboard\index{RStudio} as illustrated in Figure \ref{fig:R-vs-RStudio-1}.

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{images/shutterstock/R_vs_RStudio_1} 

}

\caption{Analogy of difference between R and RStudio.}\label{fig:R-vs-RStudio-1}
\end{figure}

More precisely, R is a programming language that runs computations, while RStudio is an \emph{integrated development environment (IDE)} that provides an interface by adding many convenient features and tools. So just as the way of having access to a speedometer, rearview mirrors, and a navigation system makes driving much easier, using RStudio's interface makes using R much easier as well.

\hypertarget{installing}{%
\subsection{Installing R and RStudio}\label{installing}}

\begin{quote}
\textbf{Note about RStudio Server or RStudio Cloud}: If your instructor has provided you with a link and access to RStudio Server or RStudio Cloud, then you can skip this section. We do recommend after a few months of working on RStudio Server/Cloud that you return to these instructions to install this software on your own computer though.
\end{quote}

You will first need to download and install both R and RStudio (Desktop version) on your computer. It is important that you install R first and then install RStudio.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{You must do this first:} Download and install R by going to \url{https://cloud.r-project.org/}. \index{R!installation}

  \begin{itemize}
  \tightlist
  \item
    If you are a Windows user: Click on ``Download R for Windows'', then click on ``base'', then click on the Download link.
  \item
    If you are macOS user: Click on ``Download R for (Mac) OS X'', then under ``Latest release:'' click on R-X.X.X.pkg, where R-X.X.X is the version number. For example, the latest version of R as of November 25, 2019 was R-3.6.1.
  \item
    If you are a Linux user: Click on ``Download R for Linux'' and choose your distribution for more information on installing R for your setup.
  \end{itemize}
\item
  \textbf{You must do this second:} Download and install RStudio at \url{https://www.rstudio.com/products/rstudio/download/}.

  \begin{itemize}
  \tightlist
  \item
    Scroll down to ``Installers for Supported Platforms'' near the bottom of the page.
  \item
    Click on the download link corresponding to your computer's operating system. \index{RStudio!installation}
  \end{itemize}
\end{enumerate}

\hypertarget{using-r-via-rstudio}{%
\subsection{Using R via RStudio}\label{using-r-via-rstudio}}

Recall our car analogy from earlier. Much as we don't drive a car by interacting directly with the engine but rather by interacting with elements on the car's dashboard, we won't be using R directly but rather we will use RStudio's interface. After you install R and RStudio on your computer, you'll have two new \emph{programs} (also called \emph{applications}) you can open. We'll always work in RStudio and not in the R application. Figure \ref{fig:R-vs-RStudio-2} shows what icon you should be clicking on your computer.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{images/logos/R_vs_RStudio} 

}

\caption{Icons of R versus RStudio on your computer.}\label{fig:R-vs-RStudio-2}
\end{figure}

After you open RStudio, you should see something similar to Figure \ref{fig:RStudio-interface}. (Note that slight differences might exist if the RStudio interface is updated after 2019 to not be this by default.)

\begin{figure}

{\centering \includegraphics[width=0.93\linewidth]{images/rstudio_screenshots/rstudio} 

}

\caption{RStudio interface to R.}\label{fig:RStudio-interface}
\end{figure}

Note the three \emph{panes} which are three panels dividing the screen: the \emph{console pane}, the \emph{files pane}, and the \emph{environment pane}. Over the course of this chapter, you'll come to learn what purpose each of these panes serves.

\hypertarget{code}{%
\section{How do I code in R?}\label{code}}

Now that you're set up with R and RStudio, you are probably asking yourself, ``OK. Now how do I use R?''. The first thing to note is that unlike other statistical software programs like Excel, SPSS, or Minitab that provide \href{https://en.wikipedia.org/wiki/Point_and_click}{point-and-click} interfaces, R is an \href{https://en.wikipedia.org/wiki/Interpreted_language}{interpreted language}. This means you have to type in commands written in \emph{R code}. In other words, you have to code/program in R. Note that we'll use the terms ``coding'' and ``programming'' interchangeably in this book.

While it is not required to be a seasoned coder/computer programmer to use R, there is still a set of basic programming concepts that new R users need to understand. Consequently, while this book is not a book on programming, you will still learn just enough of these basic programming concepts needed to explore and analyze data effectively.

\hypertarget{programming-concepts}{%
\subsection{Basic programming concepts and terminology}\label{programming-concepts}}

We now introduce some basic programming concepts and terminology. Instead of asking you to memorize all these concepts and terminology right now, we'll guide you so that you'll ``learn by doing.'' To help you learn, we will always use a different font to distinguish regular text from \texttt{computer\_code}. The best way to master these topics is, in our opinions, through \href{https://jamesclear.com/deliberate-practice-theory}{deliberate practice} with R and lots of repetition.

\begin{itemize}
\tightlist
\item
  Basics: \index{programming language basics}

  \begin{itemize}
  \tightlist
  \item
    \emph{Console pane}: where you enter in commands. \index{console}
  \item
    \emph{Running code}: the act of telling R to perform an act by giving it commands in the console.
  \item
    \emph{Objects}: where values are saved in R. We'll show you how to \emph{assign} values to objects and how to display the contents of objects. \index{objects}
  \item
    \emph{Data types}: integers, doubles/numerics, logicals, and characters. \index{data types} Integers are values like -1, 0, 2, 4092. Doubles or numerics are a larger set of values containing both the integers but also fractions and decimal values like -24.932 and 0.8. Logicals are either \texttt{TRUE} or \texttt{FALSE} while characters are text such as ``cabbage'', ``Hamilton'', ``The Wire is the greatest TV show ever'', and ``This ramen is delicious.'' Note that characters are often denoted with the quotation marks around them.
  \end{itemize}
\item
  \emph{Vectors}: a series of values. These are created using the \texttt{c()} function, where \texttt{c()} stands for ``combine'' or ``concatenate.'' For example, \texttt{c(6,\ 11,\ 13,\ 31,\ 90,\ 92)} creates a six element series of positive integer values \index{vectors}.
\item
  \emph{Factors}: \emph{categorical data} are commonly represented in R as factors. \index{factors} Categorical data can also be represented as \emph{strings}. We'll study this difference as we progress through the book.
\item
  \emph{Data frames}: rectangular spreadsheets. They are representations of datasets in R where the rows correspond to \emph{observations} and the columns correspond to \emph{variables} that describe the observations. \index{data frame} We'll cover data frames later in Section \ref{nycflights13}.
\item
  \emph{Conditionals}: \index{conditionals}

  \begin{itemize}
  \tightlist
  \item
    Testing for equality in R using \texttt{==} (and not \texttt{=}, which is typically used for assignment). For example, \texttt{2\ +\ 1\ ==\ 3} compares \texttt{2\ +\ 1} to \texttt{3} and is correct R code, while \texttt{2\ +\ 1\ =\ 3} will return an error.
  \item
    Boolean algebra: \texttt{TRUE/FALSE} statements and mathematical operators such as \texttt{\textless{}} (less than), \texttt{\textless{}=} (less than or equal), and \texttt{!=} (not equal to). \index{Boolean algebra} For example, \texttt{4\ +\ 2\ \textgreater{}=\ 3} will return \texttt{TRUE}, but \texttt{3\ +\ 5\ \textless{}=\ 1} will return \texttt{FALSE}.
  \item
    Logical operators: \texttt{\&} representing ``and'' as well as \texttt{\textbar{}} representing ``or.'' For example, \texttt{(2\ +\ 1\ ==\ 3)\ \&\ (2\ +\ 1\ ==\ 4)} returns \texttt{FALSE} since both clauses are not \texttt{TRUE} (only the first clause is \texttt{TRUE}). On the other hand, \texttt{(2\ +\ 1\ ==\ 3)\ \textbar{}\ (2\ +\ 1\ ==\ 4)} returns \texttt{TRUE} since at least one of the two clauses is \texttt{TRUE}. \index{operators!logical}
  \end{itemize}
\item
  \emph{Functions}, also called \emph{commands}: Functions perform tasks in R. They take in inputs called \emph{arguments} and return outputs. You can either manually specify a function's arguments or use the function's \emph{default values}. \index{functions}

  \begin{itemize}
  \tightlist
  \item
    For example, the function \texttt{seq()} in R generates a sequence of numbers. If you just run \texttt{seq()} it will return the value 1. That doesn't seem very useful! This is because the default arguments are set as \texttt{seq(from\ =\ 1,\ to\ =\ 1)}. Thus, if you don't pass in different values for \texttt{from} and \texttt{to} to change this behavior, R just assumes all you want is the number 1. You can change the argument values by updating the values after the \texttt{=} sign. If we try out \texttt{seq(from\ =\ 2,\ to\ =\ 5)} we get the result \texttt{2\ 3\ 4\ 5} that we might expect.
  \item
    We'll work with functions a lot throughout this book and you'll get lots of practice in understanding their behaviors. To further assist you in understanding when a function is mentioned in the book, we'll also include the \texttt{()} after them as we did with \texttt{seq()} above.
  \end{itemize}
\end{itemize}

This list is by no means an exhaustive list of all the programming concepts and terminology needed to become a savvy R user; such a list would be so large it wouldn't be very useful, especially for novices. Rather, we feel this is a minimally viable list of programming concepts and terminology you need to know before getting started. We feel that you can learn the rest as you go. Remember that your mastery of all of these concepts and terminology will build as you practice more and more.

\hypertarget{messages}{%
\subsection{Errors, warnings, and messages}\label{messages}}

One thing that intimidates new R and RStudio users is how it reports \emph{errors}, \emph{warnings}, and \emph{messages}. R reports errors, warnings, and messages in a glaring red font, which makes it seem like it is scolding you. However, seeing red text in the console is not always bad.

R will show red text in the console pane in three different situations:

\begin{itemize}
\tightlist
\item
  \textbf{Errors}: \index{R!errors} When the red text is a legitimate error, it will be prefaced with ``Error in\ldots{}'' and will try to explain what went wrong. Generally when there's an error, the code will not run. For example, we'll see in Subsection \ref{package-use} if you see \texttt{Error\ in\ ggplot(...)\ :\ could\ not\ find\ function\ "ggplot"}, it means that the \texttt{ggplot()} function is not accessible because the package that contains the function (\texttt{ggplot2}) was not loaded with \texttt{library(ggplot2)}. Thus you cannot use the \texttt{ggplot()} function without the \texttt{ggplot2} package being loaded first.
\item
  \textbf{Warnings}: \index{R!warnings} When the red text is a warning, it will be prefaced with ``Warning:'' and R will try to explain why there's a warning. Generally your code will still work, but with some caveats. For example, you will see in Chapter \ref{viz} if you create a scatterplot based on a dataset where two of the rows of data have missing entries that would be needed to create points in the scatterplot, you will see this warning: \texttt{Warning:\ Removed\ 2\ rows\ containing\ missing\ values\ (geom\_point)}. R will still produce the scatterplot with all the remaining non-missing values, but it is warning you that two of the points aren't there.
\item
  \textbf{Messages}: \index{R!messages} When the red text doesn't start with either ``Error'' or ``Warning'', it's \emph{just a friendly message}. You'll see these messages when you load \emph{R packages} in the upcoming Subsection \ref{package-loading} or when you read data saved in spreadsheet files with the \texttt{read\_csv()} function as you'll see in Chapter \ref{tidy}. These are helpful diagnostic messages and they don't stop your code from working. Additionally, you'll see these messages when you install packages too using \texttt{install.packages()} as discussed in Subsection \ref{package-installation}.
\end{itemize}

Remember, when you see red text in the console, \emph{don't panic}. It doesn't necessarily mean anything is wrong. Rather:

\begin{itemize}
\tightlist
\item
  If the text starts with ``Error'', figure out what's causing it. {Think of errors as a red traffic light: something is wrong!}
\item
  If the text starts with ``Warning'', figure out if it's something to worry about. For instance, if you get a warning about missing values in a scatterplot and you know there are missing values, you're fine. If that's surprising, look at your data and see what's missing. {Think of warnings as a yellow traffic light: everything is working fine, but watch out/pay attention.}
\item
  Otherwise, the text is just a message. Read it, wave back at R, and thank it for talking to you. {Think of messages as a green traffic light: everything is working fine and keep on going!}
\end{itemize}

\hypertarget{tips-code}{%
\subsection{Tips on learning to code}\label{tips-code}}

Learning to code/program is quite similar to learning a foreign language. It can be daunting and frustrating at first. Such frustrations are common and it is normal to feel discouraged as you learn. However, just as with learning a foreign language, if you put in the effort and are not afraid to make mistakes, anybody can learn and improve.

Here are a few useful tips to keep in mind as you learn to program:

\begin{itemize}
\tightlist
\item
  \textbf{Remember that computers are not actually that smart}: You may think your computer or smartphone is ``smart,'' but really people spent a lot of time and energy designing them to appear ``smart.'' In reality, you have to tell a computer everything it needs to do. Furthermore, the instructions you give your computer can't have any mistakes in them, nor can they be ambiguous in any way.
\item
  \textbf{Take the ``copy, paste, and tweak'' approach}: Especially when you learn your first programming language or you need to understand particularly complicated code, it is often much easier to take existing code that you know works and modify it to suit your ends. This is as opposed to trying to type out the code from scratch. We call this the \emph{``copy, paste, and tweak''} approach. So early on, we suggest not trying to write code from memory, but rather take existing examples we have provided you, then copy, paste, and tweak them to suit your goals. After you start feeling more confident, you can slowly move away from this approach and write code from scratch. Think of the ``copy, paste, and tweak'' approach as training wheels for a child learning to ride a bike. After getting comfortable, they won't need them anymore.
\item
  \textbf{The best way to learn to code is by doing}: Rather than learning to code for its own sake, we find that learning to code goes much smoother when you have a goal in mind or when you are working on a particular project, like analyzing data that you are interested in and that is important to you.
\item
  \textbf{Practice is key}: Just as the only method to improve your foreign language skills is through lots of practice and speaking, the only method to improving your coding skills is through lots of practice. Don't worry, however, we'll give you plenty of opportunities to do so!
\end{itemize}

\hypertarget{packages}{%
\section{What are R packages?}\label{packages}}

Another point of confusion with many new R users is the idea of an R package. R packages \index{R!packages} extend the functionality of R by providing additional functions, data, and documentation. They are written by a worldwide community of R users and can be downloaded for free from the internet.

For example, among the many packages we will use in this book are the \texttt{ggplot2} package \citep{R-ggplot2} for data visualization in Chapter \ref{viz}\index{R packages!ggplot2}, the \texttt{dplyr} package \citep{R-dplyr} for data wrangling in Chapter \ref{wrangling}\index{R packages!dplyr}, the \texttt{moderndive} package \citep{R-moderndive} that accompanies this book\index{R packages!moderndive}, and the \texttt{infer} package \citep{R-infer} for ``tidy'' and transparent statistical inference in Chapters \ref{confidence-intervals}, \ref{hypothesis-testing}, and \ref{inference-for-regression}\index{R packages!infer}.

A good analogy for R packages \index{R packages} is they are like apps you can download onto a mobile phone:

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/shutterstock/R_vs_R_packages} 

}

\caption{Analogy of R versus R packages.}\label{fig:R-vs-R-packages}
\end{figure}

So R is like a new mobile phone: while it has a certain amount of features when you use it for the first time, it doesn't have everything. R packages are like the apps you can download onto your phone from Apple's App Store or Android's Google Play.

Let's continue this analogy by considering the Instagram app for editing and sharing pictures. Say you have purchased a new phone and you would like to share a photo you have just taken with friends on Instagram. You need to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Install the app}: Since your phone is new and does not include the Instagram app, you need to download the app from either the App Store or Google Play. You do this once and you're set for the time being. You might need to do this again in the future when there is an update to the app.
\item
  \emph{Open the app}: After you've installed Instagram, you need to open it.
\end{enumerate}

Once Instagram is open on your phone, you can then proceed to share your photo with your friends and family. The process is very similar for using an R package. You need to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \emph{Install the package}: This is like installing an app on your phone. Most packages are not installed by default when you install R and RStudio. Thus if you want to use a package for the first time, you need to install it first. Once you've installed a package, you likely won't install it again unless you want to update it to a newer version.
\item
  \emph{``Load'' the package}: ``Loading'' a package is like opening an app on your phone. Packages are not ``loaded'' by default when you start RStudio on your computer; you need to ``load'' each package you want to use every time you start RStudio.
\end{enumerate}

Let's perform these two steps for the \texttt{ggplot2} package for data visualization.

\hypertarget{package-installation}{%
\subsection{Package installation}\label{package-installation}}

\begin{quote}
\textbf{Note about RStudio Server or RStudio Cloud}: If your instructor has provided you with a link and access to RStudio Server or RStudio Cloud, you might not need to install packages, as they might be preinstalled for you by your instructor. That being said, it is still a good idea to know this process for later on when you are not using RStudio Server or Cloud, but rather RStudio Desktop on your own computer.
\end{quote}

There are two ways to install an R package: an easy way and a more advanced way. \index{R packages!installation} Let's install the \texttt{ggplot2} package the easy way first as shown in Figure \ref{fig:easy-way-install}. In the Files pane of RStudio:

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  Click on the ``Packages'' tab.
\item
  Click on ``Install'' next to Update.
\item
  Type the name of the package under ``Packages (separate multiple with space or comma):'' In this case, type \texttt{ggplot2}.
\item
  Click ``Install.''
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=0.55\linewidth,height=0.55\textheight]{images/rstudio_screenshots/install_packages_easy_way} 

}

\caption{Installing packages in R the easy way.}\label{fig:easy-way-install}
\end{figure}

An alternative but slightly less convenient way to install a package is by typing \texttt{install.packages("ggplot2")} in the console pane of RStudio and pressing Return/Enter on your keyboard. Note you must include the quotation marks around the name of the package.

Much like an app on your phone, you only have to install a package once. However, if you want to update a previously installed package to a newer version, you need to reinstall it by repeating the earlier steps.

\vspace{0.1in}

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC1.1)} Repeat the earlier installation steps, but for the \texttt{dplyr}, \texttt{nycflights13}, and \texttt{knitr} packages. This will install the earlier mentioned \texttt{dplyr} package for data wrangling, the \texttt{nycflights13} package containing data on all domestic flights leaving a NYC airport in 2013, and the \texttt{knitr} package for generating easy-to-read tables in R. We'll use these packages in the next section.

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\vspace{0.1in}

Note that if you'd like your output on your computer to match up exactly with the output presented throughout the book, you may want to use the exact versions of the packages that we used. You can find a full listing of these packages and their versions in Appendix \ref{appendixE}. This likely won't be relevant for novices, but we included it for reproducibility reasons.

\hypertarget{package-loading}{%
\subsection{Package loading}\label{package-loading}}

Recall that after you've installed a package, you need to ``load it.'' In other words, you need to ``open it.'' We do this by using the \texttt{library()} command. \index{R packages!loading}

For example, to load the \texttt{ggplot2} package, run the following code in the console pane. What do we mean by ``run the following code''? Either type or copy-and-paste the following code into the console pane and then hit the Enter key.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\end{Highlighting}
\end{Shaded}

If after running the earlier code, a blinking cursor returns next to the \texttt{\textgreater{}} ``prompt'' sign, it means you were successful and the \texttt{ggplot2} package is now loaded and ready to use. If, however, you get a red ``error message'' that reads \texttt{...} \index{R packages!loading error}

\begin{verbatim}
Error in library(ggplot2) : there is no package called ‘ggplot2’
\end{verbatim}

\texttt{...} it means that you didn't successfully install it. This is an example of an ``error message'' we discussed in Subsection \ref{messages}. If you get this error message, go back to Subsection \ref{package-installation} on R package installation and make sure to install the \texttt{ggplot2} package before proceeding.

\vspace{0.1in}

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC1.2)} ``Load'' the \texttt{dplyr}, \texttt{nycflights13}, and \texttt{knitr} packages as well by repeating the earlier steps.

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\vspace{0.1in}

\hypertarget{package-use}{%
\subsection{Package use}\label{package-use}}

One very common mistake new R users make when wanting to use particular packages is they forget to ``load'' them first by using the \texttt{library()} command we just saw. Remember: \emph{you have to load each package you want to use every time you start RStudio.} If you don't first ``load'' a package, but attempt to use one of its features, you'll see an error message similar to:

\begin{verbatim}
Error: could not find function
\end{verbatim}

This is a different error message than the one you just saw on a package not having been installed yet. R is telling you that you are trying to use a function in a package that has not yet been ``loaded.'' R doesn't know where to find the function you are using. Almost all new users forget to do this when starting out, and it is a little annoying to get used to doing it. However, you'll remember with practice and after some time it will become second nature for you.

\hypertarget{nycflights13}{%
\section{Explore your first datasets}\label{nycflights13}}

Let's put everything we've learned so far into practice and start exploring some real data! Data comes to us in a variety of formats, from pictures to text to numbers. Throughout this book, we'll focus on datasets that are saved in ``spreadsheet''-type format. This is probably the most common way data are collected and saved in many fields. Remember from Subsection \ref{programming-concepts} that these ``spreadsheet''-type datasets are called \emph{data frames} in R. \index{data frame} We'll focus on working with data saved as data frames throughout this book.

Let's first load all the packages needed for this chapter, assuming you've already installed them. Read Section \ref{packages} for information on how to install and load R packages if you haven't already.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(nycflights13)}
\FunctionTok{library}\NormalTok{(dplyr)}
\FunctionTok{library}\NormalTok{(knitr)}
\end{Highlighting}
\end{Shaded}

At the beginning of all subsequent chapters in this book, we'll always have a list of packages that you should have installed and loaded in order to work with that chapter's R code.

\hypertarget{nycflights13-package}{%
\subsection{\texorpdfstring{\texttt{nycflights13} package}{nycflights13 package}}\label{nycflights13-package}}

Many of us have flown on airplanes or know someone who has. Air travel has become an ever-present aspect of many people's lives. If you look at the Departures flight information board at an airport, you will frequently see that some flights are delayed for a variety of reasons. Are there ways that we can understand the reasons that cause flight delays?

We'd all like to arrive at our destinations on time whenever possible. (Unless you secretly love hanging out at airports. If you are one of these people, pretend for a moment that you are very much anticipating being at your final destination.) Throughout this book, we're going to analyze data related to all domestic flights departing from one of New York City's three main airports in 2013: Newark Liberty International (EWR), John F. Kennedy International (JFK), and LaGuardia Airport (LGA). We'll access this data using the \texttt{nycflights13} \index{R packages!nycflights13} R package, which contains five datasets saved in five data frames:

\begin{itemize}
\tightlist
\item
  \texttt{flights}: Information on all 336,776 flights.
\item
  \texttt{airlines}: A table matching airline names and their two-letter International Air Transport Association (IATA) airline codes (also known as carrier codes) for 16 airline companies. For example, ``DL'' is the two-letter code for Delta.
\item
  \texttt{planes}: Information about each of the 3,322 physical aircraft used.
\item
  \texttt{weather}: Hourly meteorological data for each of the three NYC airports. This data frame has 26,115 rows, roughly corresponding to the \(365 \times 24 \times 3 = 26,280\) possible hourly measurements one can observe at three locations over the course of a year.
\item
  \texttt{airports}: Names, codes, and locations of the 1,458 domestic destinations.
\end{itemize}

\hypertarget{flights-data-frame}{%
\subsection{\texorpdfstring{\texttt{flights} data frame}{flights data frame}}\label{flights-data-frame}}

We'll begin by exploring the \texttt{flights} data frame and get an idea of its structure. Run the following code in your console, either by typing it or by cutting-and-pasting it. It displays the contents of the \texttt{flights} data frame in your console. Note that depending on the size of your monitor, the output may vary slightly.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 336,776 x 22
    year month   day dep_time sched_dep_time dep_delay arr_time
   <int> <int> <int>    <int>          <int>     <dbl>    <int>
 1  2013     1     1      517            515         2      830
 2  2013     1     1      533            529         4      850
 3  2013     1     1      542            540         2      923
 4  2013     1     1      544            545        -1     1004
 5  2013     1     1      554            600        -6      812
 6  2013     1     1      554            558        -4      740
 7  2013     1     1      555            600        -5      913
 8  2013     1     1      557            600        -3      709
 9  2013     1     1      557            600        -3      838
10  2013     1     1      558            600        -2      753
# ... with 336,766 more rows, and 15 more variables: sched_arr_time <int>,
#   arr_delay <dbl>, carrier <chr>, flight <int>, tailnum <chr>,
#   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,
#   minute <dbl>, time_hour <dttm>, gain <dbl>, hours <dbl>,
#   gain_per_hour <dbl>
\end{verbatim}

Let's unpack this output:

\begin{itemize}
\tightlist
\item
  \texttt{A\ tibble:\ 336,776\ x\ 22}: A \texttt{tibble} is a specific kind of data frame in R.\index{tibble} This particular data frame has

  \begin{itemize}
  \tightlist
  \item
    \texttt{336,776} rows corresponding to different \emph{observations}. Here, each observation is a flight.
  \item
    \texttt{22} columns corresponding to 22 \emph{variables} describing each observation.
  \end{itemize}
\item
  \texttt{year}, \texttt{month}, \texttt{day}, \texttt{dep\_time}, \texttt{sched\_dep\_time}, \texttt{dep\_delay}, and \texttt{arr\_time} are the different columns, in other words, the different variables of this dataset.
\item
  We then have a preview of the first 10 rows of observations corresponding to the first 10 flights. R is only showing the first 10 rows, because if it showed all \texttt{336,776} rows, it would overwhelm your screen.
\item
  \texttt{...\ with\ 336,766\ more\ rows,\ and\ 14\ more\ variables:} indicating to us that 336,766 more rows of data and 11 more variables could not fit in this screen.
\end{itemize}

Unfortunately, this output does not allow us to explore the data very well, but it does give a nice preview. Let's look at some different ways to explore data frames.

\hypertarget{exploredataframes}{%
\subsection{Exploring data frames}\label{exploredataframes}}

There are many ways to get a feel for the data contained in a data frame such as \texttt{flights}. We present three functions that take as their ``argument'' (their input) the data frame in question. We also include a fourth method for exploring one particular column of a data frame:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Using the \texttt{View()} function, which brings up RStudio's built-in data viewer.
\item
  Using the \texttt{glimpse()} function, which is included in the \texttt{dplyr} package.
\item
  Using the \texttt{kable()} function, which is included in the \texttt{knitr} package.
\item
  Using the \texttt{\$} ``extraction operator,'' which is used to view a single variable/column in a data frame.
\end{enumerate}

\textbf{1. \texttt{View()}}:

Run \texttt{View(flights)} \index{R packages!utils!View()} in your console in RStudio, either by typing it or cutting-and-pasting it into the console pane. Explore this data frame in the resulting pop up viewer. You should get into the habit of viewing any data frames you encounter. Note the uppercase \texttt{V} in \texttt{View()}. R is case-sensitive, so you'll get an error message if you run \texttt{view(flights)} instead of \texttt{View(flights)}.

\vspace{0.1in}

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC1.3)} What does any \emph{ONE} row in this \texttt{flights} dataset refer to?

\begin{itemize}
\tightlist
\item
  A. Data on an airline
\item
  B. Data on a flight
\item
  C. Data on an airport
\item
  D. Data on multiple flights
\end{itemize}

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\vspace{0.1in}

By running \texttt{View(flights)}, we can explore the different \emph{variables} listed in the columns. Observe that there are many different types of variables. Some of the variables like \texttt{distance}, \texttt{day}, and \texttt{arr\_delay} are what we will call \emph{quantitative} variables. \index{quantitative} These variables are numerical in nature. Other variables here are \index{categorical} \emph{categorical}.

Note that if you look in the leftmost column of the \texttt{View(flights)} output, you will see a column of numbers. These are the row numbers of the dataset. If you glance across a row with the same number, say row 5, you can get an idea of what each row is representing. This will allow you to identify what object is being described in a given row by taking note of the values of the columns in that specific row. This is often called the \emph{observational unit}.\index{observational unit} The observational unit in this example is an individual flight departing from New York City in 2013. You can identify the observational unit by determining what ``thing'' is being measured or described by each of the variables. We'll talk more about observational units in Subsection \ref{identification-vs-measurement-variables} on \emph{identification} and \emph{measurement} variables.

\textbf{2. \texttt{glimpse()}}:

The second way we'll cover to explore a data frame is using the \texttt{glimpse()} function \index{dplyr!glimpse()} included in the \index{dplyr|seealso{R packages!dplyr}} \texttt{dplyr} package. Thus, you can only use the \texttt{glimpse()} function after you've loaded the \texttt{dplyr} package by running \texttt{library(dplyr)}. This function provides us with an alternative perspective for exploring a data frame than the \texttt{View()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(flights)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 336,776
Columns: 22
$ year           <int> 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013,...
$ month          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...
$ day            <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,...
$ dep_time       <int> 517, 533, 542, 544, 554, 554, 555, 557, 557, 55...
$ sched_dep_time <int> 515, 529, 540, 545, 600, 558, 600, 600, 600, 60...
$ dep_delay      <dbl> 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2...
$ arr_time       <int> 830, 850, 923, 1004, 812, 740, 913, 709, 838, 7...
$ sched_arr_time <int> 819, 830, 850, 1022, 837, 728, 854, 723, 846, 7...
$ arr_delay      <dbl> 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -...
$ carrier        <chr> "UA", "UA", "AA", "B6", "DL", "UA", "B6", "EV",...
$ flight         <int> 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79...
$ tailnum        <chr> "N14228", "N24211", "N619AA", "N804JB", "N668DN...
$ origin         <chr> "EWR", "LGA", "JFK", "JFK", "LGA", "EWR", "EWR"...
$ dest           <chr> "IAH", "IAH", "MIA", "BQN", "ATL", "ORD", "FLL"...
$ air_time       <dbl> 227, 227, 160, 183, 116, 150, 158, 53, 140, 138...
$ distance       <dbl> 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 94...
$ hour           <dbl> 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5,...
$ minute         <dbl> 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, ...
$ time_hour      <dttm> 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013...
$ gain           <dbl> -9, -16, -31, 17, 19, -16, -24, 11, 5, -10, 0, ...
$ hours          <dbl> 3.783, 3.783, 2.667, 3.050, 1.933, 2.500, 2.633...
$ gain_per_hour  <dbl> -2.38, -4.23, -11.62, 5.57, 9.83, -6.40, -9.11,...
\end{verbatim}

Observe that \texttt{glimpse()} will give you the first few entries of each variable in a row after the variable name. In addition, the \emph{data type} (see Subsection \ref{programming-concepts}) of the variable is given immediately after each variable's name inside \texttt{\textless{}\ \textgreater{}}. Here, \texttt{int} and \texttt{dbl} refer to ``integer'' and ``double'', which are computer coding terminology for quantitative/numerical variables. ``Doubles'' take up twice the size to store on a computer compared to integers.

In contrast, \texttt{chr} refers to ``character'', which is computer terminology for text data. In most forms, text data, such as the \texttt{carrier} or \texttt{origin} of a flight, are categorical variables. The \texttt{time\_hour} variable is another data type: \texttt{dttm}. These types of variables represent date and time combinations. However, we won't work with dates and times in this book; we leave this topic for other data science books like \href{https://ubc-dsci.github.io/introduction-to-datascience/}{\emph{Introduction to Data Science} by Tiffany-Anne Timbers, Melissa Lee, and Trevor Campbell} or \href{https://r4ds.had.co.nz/dates-and-times.html}{\emph{R for Data Science}} \citep{rds2016}.

\vspace*{0.05in}

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC1.4)} What are some other examples in this dataset of \emph{categorical} variables? What makes them different than \emph{quantitative} variables?

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\vfill\smallskip

\textbf{3. \texttt{kable()}}:

The final way to explore the entirety of a data frame is using the \texttt{kable()} \index{knitr!kable()} function from the \index{knitr|seealso{R packages!knitr}} \texttt{knitr} package. Let's explore the different carrier codes for all the airlines in our dataset two ways. Run both of these lines of code in the console:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airlines}
\FunctionTok{kable}\NormalTok{(airlines)}
\end{Highlighting}
\end{Shaded}

At first glance, it may not appear that there is much difference in the outputs. However, when using tools for producing reproducible reports such as \href{http://rmarkdown.rstudio.com/lesson-1.html}{R Markdown}, the latter code produces output that is much more legible and reader-friendly. You'll see us use this reader-friendly style in many places in the book when we want to print a data frame as a nice table.

\textbf{4. \texttt{\$} operator}

Lastly, the \texttt{\$} operator \index{operators!dollar sign} allows us to extract and then explore a single variable within a data frame. For example, run the following in your console

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airlines}\SpecialCharTok{$}\NormalTok{name}
\end{Highlighting}
\end{Shaded}

We used the \texttt{\$} operator to extract only the \texttt{name} variable and return it as a vector of length 16\index{vectors}. We'll only be occasionally exploring data frames using the \texttt{\$} operator, instead favoring the \texttt{View()} and \texttt{glimpse()} functions.

\hypertarget{identification-vs-measurement-variables}{%
\subsection{Identification and measurement variables}\label{identification-vs-measurement-variables}}

There is a subtle difference between the kinds of variables that you will encounter in data frames. There are \emph{identification variables} and \emph{measurement variables}. For example, let's explore the \texttt{airports} data frame by showing the output of \texttt{glimpse(airports)}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(airports)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 1,458
Columns: 8
$ faa   <chr> "04G", "06A", "06C", "06N", "09J", "0A9", "0G6", "0G7", ...
$ name  <chr> "Lansdowne Airport", "Moton Field Municipal Airport", "S...
$ lat   <dbl> 41.1, 32.5, 42.0, 41.4, 31.1, 36.4, 41.5, 42.9, 39.8, 48...
$ lon   <dbl> -80.6, -85.7, -88.1, -74.4, -81.4, -82.2, -84.5, -76.8, ...
$ alt   <dbl> 1044, 264, 801, 523, 11, 1593, 730, 492, 1000, 108, 409,...
$ tz    <dbl> -5, -6, -6, -5, -5, -5, -5, -5, -5, -8, -5, -6, -5, -5, ...
$ dst   <chr> "A", "A", "A", "A", "A", "A", "A", "A", "U", "A", "A", "...
$ tzone <chr> "America/New_York", "America/Chicago", "America/Chicago"...
\end{verbatim}

The variables \texttt{faa} and \texttt{name} are what we will call \emph{identification variables}, variables that uniquely identify each observational unit. In this case, the identification variables uniquely identify airports. Such variables are mainly used in practice to uniquely identify each row in a data frame. \texttt{faa} gives the unique code provided by the FAA for that airport, while the \texttt{name} variable gives the longer official name of the airport. The remaining variables (\texttt{lat}, \texttt{lon}, \texttt{alt}, \texttt{tz}, \texttt{dst}, \texttt{tzone}) are often called \emph{measurement} or \emph{characteristic} variables: variables that describe properties of each observational unit. For example, \texttt{lat} and \texttt{long} describe the latitude and longitude of each airport.

Furthermore, sometimes a single variable might not be enough to uniquely identify each observational unit: combinations of variables might be needed. While it is not an absolute rule, for organizational purposes it is considered good practice to have your identification variables in the leftmost columns of your data frame.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC1.5)} What properties of each airport do the variables \texttt{lat}, \texttt{lon}, \texttt{alt}, \texttt{tz}, \texttt{dst}, and \texttt{tzone} describe in the \texttt{airports} data frame? Take your best guess.

\textbf{(LC1.6)} Provide the names of variables in a data frame with at least three variables where one of them is an identification variable and the other two are not. Further, create your own tidy data frame that matches these conditions.

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{help-files}{%
\subsection{Help files}\label{help-files}}

Another nice feature of R are help files, which provide documentation for various functions and datasets. You can bring up help files by adding a \texttt{?} \index{operators!?} before the name of a function or data frame and then run this in the console. You will then be presented with a page showing the corresponding documentation if it exists. For example, let's look at the help file for the \texttt{flights} data frame.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{?flights}
\end{Highlighting}
\end{Shaded}

The help file should pop up in the Help pane of RStudio. If you have questions about a function or data frame included in an R package, you should get in the habit of consulting the help file right away.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC1.7)} Look at the help file for the \texttt{airports} data frame. Revise your earlier guesses about what the variables \texttt{lat}, \texttt{lon}, \texttt{alt}, \texttt{tz}, \texttt{dst}, and \texttt{tzone} each describe.

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

We've given you what we feel is a minimally viable set of tools to explore data in R. Does this chapter contain everything you need to know? Absolutely not. To try to include everything in this chapter would make the chapter so large it wouldn't be useful! As we said earlier, the best way to add to your toolbox is to get into RStudio and run and write code as much as possible.

\hypertarget{additional-resources}{%
\subsection{Additional resources}\label{additional-resources}}

Solutions to all \emph{Learning checks} can be found online in \href{https://moderndive.com/D-appendixD.html}{Appendix D}.

If you are new to the world of coding, R, and RStudio and feel you could benefit from a more detailed introduction, we suggest you check out the short book, \href{https://rbasics.netlify.app/}{\emph{Getting Used to R, RStudio, and R Markdown}} \citep{usedtor2016}. It includes screencast recordings that you can follow along and pause as you learn. This book also contains an introduction to R Markdown, a tool used for reproducible research in R.



\begin{figure}

{\centering \includegraphics[width=\textwidth,height=1.6in]{images/copyright/getting-used-to-R} 

}

\caption{Preview of \emph{Getting Used to R, RStudio, and R Markdown}.}\label{fig:unnamed-chunk-25}
\end{figure}

\hypertarget{whats-to-come}{%
\subsection{What's to come?}\label{whats-to-come}}

We're now going to start the ``Data Science with \texttt{tidyverse}'' portion of this book in Chapter \ref{viz} as shown in Figure \ref{fig:moderndive-flowchart} with what we feel is the most important tool in a data scientist's toolbox: data visualization. We'll continue to explore the data included in the \texttt{nycflights13} package using the \texttt{ggplot2} package for data visualization. You'll see that data visualization is a powerful tool to add to your toolbox for data exploration that provides additional insight to what the \texttt{View()} and \texttt{glimpse()} functions can provide.



\begin{figure}

{\centering \includegraphics[width=1\linewidth,height=1\textheight]{images/flowcharts/flowchart/flowchart.004} 

}

\caption{\emph{ModernDive} flowchart - on to Part I!}\label{fig:moderndive-flowchart}
\end{figure}



\hypertarget{part-reftidyversepart}{%
\part{Data Science with \texttt{tidyverse}}\label{part-reftidyversepart}}

\hypertarget{viz}{%
\chapter{Data Visualization}\label{viz}}

We begin the development of your data science toolbox with data visualization. By visualizing data, we gain valuable insights we couldn't initially obtain from just looking at the raw data values. We'll use the \texttt{ggplot2} package, as it provides an easy way to customize your plots. \texttt{ggplot2} is rooted in the data visualization theory known as \emph{the grammar of graphics} \citep{wilkinson2005}, developed by Leland Wilkinson. \index{Wilkinson, Leland}

At their most basic, graphics/plots/charts (we use these terms interchangeably in this book) provide a nice way to explore the patterns in data, such as the presence of \emph{outliers}, \emph{distributions} of individual variables, and \emph{relationships} between groups of variables. Graphics are designed to emphasize the findings and insights you want your audience to understand. This does, however, require a balancing act. On the one hand, you want to highlight as many interesting findings as possible. On the other hand, you don't want to include so much information that it overwhelms your audience.

As we will see, plots \index{plots} also help us to identify patterns and outliers in our data. We'll see that a common extension of these ideas is to compare the \emph{distribution} \index{distribution} of one numerical variable, such as what are the center and spread of the values, as we go across the levels of a different categorical variable.

\hypertarget{needed-packages}{%
\subsection*{Needed packages}\label{needed-packages}}


Let's load all the packages needed for this chapter (this assumes you've already installed them). Read Section \ref{packages} for information on how to install and load R packages.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(nycflights13)}
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(dplyr)}
\end{Highlighting}
\end{Shaded}

\hypertarget{grammarofgraphics}{%
\section{The grammar of graphics}\label{grammarofgraphics}}

We start with a discussion of a theoretical framework for data visualization known as ``the grammar of graphics.'' This framework serves as the foundation for the \index{R packages!ggplot2} \texttt{ggplot2} package which we'll use extensively in this chapter. \index{Grammar of Graphics, The} Think of how we construct and form sentences in English by combining different elements, like nouns, verbs, articles, subjects, objects, etc. We can't just combine these elements in any arbitrary order; we must do so following a set of rules known as a linguistic grammar. Similarly to a linguistic grammar, ``the grammar of graphics'' defines a set of rules for constructing \emph{statistical graphics} by combining different types of \emph{layers}. This grammar was created by Leland Wilkinson \citep{wilkinson2005} and has been implemented in a variety of data visualization software platforms like R, but also \href{https://plot.ly/}{Plotly} and \href{https://www.tableau.com/}{Tableau}.

\hypertarget{components-of-the-grammar}{%
\subsection{Components of the grammar}\label{components-of-the-grammar}}

In short, the grammar tells us that:

\begin{quote}
\textbf{A statistical graphic is a \texttt{mapping} of \texttt{data} variables to \texttt{aes}thetic attributes of \texttt{geom}etric objects.}
\end{quote}

Specifically, we can break a graphic into the following three essential components:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{data}: the dataset containing the variables of interest.
\item
  \texttt{geom}: the geometric object in question. This refers to the type of object we can observe in a plot. For example: points, lines, and bars.
\item
  \texttt{aes}: aesthetic attributes of the geometric object. For example, x/y position, color, shape, and size. Aesthetic attributes are \emph{mapped} to variables in the dataset.
\end{enumerate}

You might be wondering why we wrote the terms \texttt{data}, \texttt{geom}, and \texttt{aes} in a computer code type font. We'll see very shortly that we'll specify the elements of the grammar in R using these terms. However, let's first break down the grammar with an example.

\hypertarget{gapminder}{%
\subsection{Gapminder data}\label{gapminder}}

In February 2006, a Swedish physician and data advocate named Hans Rosling gave a TED talk titled \href{https://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen}{``The best stats you've ever seen''} where he presented global economic, health, and development data from the website \href{http://www.gapminder.org/tools/\#_locale_id=en;\&chart-type=bubbles}{gapminder.org}. For example, for data on 142 countries in 2007, let's consider only a few countries in Table \ref{tab:gapminder-2007} as a peek into the data.

\begin{table}[!h]

\caption{\label{tab:gapminder-2007}Gapminder 2007 Data: First 3 of 142 countries}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{l|l|r|r|r}
\hline
Country & Continent & Life Expectancy & Population & GDP per Capita\\
\hline
Afghanistan & Asia & 43.8 & 31889923 & 975\\
\hline
Albania & Europe & 76.4 & 3600523 & 5937\\
\hline
Algeria & Africa & 72.3 & 33333216 & 6223\\
\hline
\end{tabular}
\end{table}

Each row in this table corresponds to a country in 2007. For each row, we have 5 columns:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Country}: Name of country.
\item
  \textbf{Continent}: Which of the five continents the country is part of. Note that ``Americas'' includes countries in both North and South America and that Antarctica is excluded.
\item
  \textbf{Life Expectancy}: Life expectancy in years.
\item
  \textbf{Population}: Number of people living in the country.
\item
  \textbf{GDP per Capita}: Gross domestic product (in US dollars).
\end{enumerate}

Now consider Figure \ref{fig:gapminder}, which plots this for all 142 of the data's countries.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/gapminder-1} 

}

\caption{Life expectancy over GDP per capita in 2007.}\label{fig:gapminder}
\end{figure}

Let's view this plot through the grammar of graphics:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The \texttt{data} variable \textbf{GDP per Capita} gets mapped to the \texttt{x}-position \texttt{aes}thetic \index{ggplot2!aes()} of the points.
\item
  The \texttt{data} \index{ggplot2!data} variable \textbf{Life Expectancy} gets mapped to the \texttt{y}-position \texttt{aes}thetic of the points.
\item
  The \texttt{data} variable \textbf{Population} gets mapped to the \texttt{size} \texttt{aes}thetic of the points.
\item
  The \texttt{data} variable \textbf{Continent} gets mapped to the \texttt{color} \texttt{aes}thetic of the points.
\end{enumerate}

We'll see shortly that \texttt{data} corresponds to the particular data frame where our data is saved and that ``data variables'' correspond to particular columns in the data frame. Furthermore, the type of \texttt{geom}etric object \index{ggplot2!geom} considered in this plot are points. That being said, while in this example we are considering points, graphics are not limited to just points. We can also use lines, bars, and other geometric objects.

Let's summarize the three essential components of the grammar in Table \ref{tab:summary-table-gapminder}.

\begin{table}[!h]

\caption{\label{tab:summary-table-gapminder}Summary of the grammar of graphics for this plot}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{lll}
\toprule
data variable & aes & geom\\
\midrule
GDP per Capita & x & point\\
Life Expectancy & y & point\\
Population & size & point\\
Continent & color & point\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{other-components}{%
\subsection{Other components}\label{other-components}}

There are other components of the grammar of graphics we can control as well. As you start to delve deeper into the grammar of graphics, you'll start to encounter these topics more frequently. In this book, we'll keep things simple and only work with these two additional components:

\begin{itemize}
\tightlist
\item
  \texttt{facet}ing breaks up a plot into several plots split by the values of another variable (Section \ref{facets}) \index{ggplot2!facet}
\item
  \texttt{position} adjustments for barplots (Section \ref{geombar}) \index{ggplot2!position}
\end{itemize}

Other more complex components like \texttt{scales} and \texttt{coord}inate systems are left for a more advanced text such as \href{http://r4ds.had.co.nz/data-visualisation.html\#aesthetic-mappings}{\emph{R for Data Science}} \citep{rds2016}. Generally speaking, the grammar of graphics allows for a high degree of customization of plots and also a consistent framework for easily updating and modifying them.

\hypertarget{ggplot2-package}{%
\subsection{ggplot2 package}\label{ggplot2-package}}

In this book, we will use the \texttt{ggplot2} package for data visualization, which is an implementation of the \texttt{g}rammar of \texttt{g}raphics for R \citep{R-ggplot2}. As we noted earlier, a lot of the previous section was written in a computer code type font. This is because the various components of the grammar of graphics are specified in the \texttt{ggplot()} \index{ggplot2!ggplot()} function included in the \texttt{ggplot2} package. For the purposes of this book, we'll always provide the \texttt{ggplot()} function with the following arguments (i.e., inputs) at a minimum:

\begin{itemize}
\tightlist
\item
  The data frame where the variables exist: the \texttt{data} argument.
\item
  The mapping of the variables to aesthetic attributes: the \texttt{mapping} argument which specifies the \texttt{aes}thetic attributes involved.
\end{itemize}

After we've specified these components, we then add \emph{layers} to the plot using the \texttt{+} sign. The most essential layer to add to a plot is the layer that specifies which type of \texttt{geom}etric object we want the plot to involve: points, lines, bars, and others. Other layers we can add to a plot include the plot title, axes labels, visual themes for the plots, and facets (which we'll see in Section \ref{facets}).

Let's now put the theory of the grammar of graphics into practice.

\hypertarget{FiveNG}{%
\section{Five named graphs - the 5NG}\label{FiveNG}}

In order to keep things simple in this book, we will only focus on five different types of graphics, each with a commonly given name. We term these ``five named graphs'' or in abbreviated form, the \textbf{5NG}: \index{five named graphs}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  scatterplots
\item
  linegraphs
\item
  histograms
\item
  boxplots
\item
  barplots
\end{enumerate}

We'll also present some variations of these plots, but with this basic repertoire of five graphics in your toolbox, you can visualize a wide array of different variable types. Note that certain plots are only appropriate for categorical variables, while others are only appropriate for numerical variables.

\hypertarget{scatterplots}{%
\section{5NG\#1: Scatterplots}\label{scatterplots}}

The simplest of the 5NG are \emph{scatterplots}, \index{scatterplots} also called \emph{bivariate plots}. They allow you to visualize the \emph{relationship} between two numerical variables. While you may already be familiar with scatterplots, let's view them through the lens of the grammar of graphics we presented in Section \ref{grammarofgraphics}. Specifically, we will visualize the relationship between the following two numerical variables in the \texttt{flights} data frame included in the \index{R packages!nycflights13} \texttt{nycflights13} package:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{dep\_delay}: departure delay on the horizontal ``x'' axis and
\item
  \texttt{arr\_delay}: arrival delay on the vertical ``y'' axis
\end{enumerate}

for Alaska Airlines flights leaving NYC in 2013. This requires paring down the data from all 336,776 flights that left NYC in 2013, to only the 714 \emph{Alaska Airlines} flights that left NYC in 2013. We do this so our scatterplot will involve a manageable 714 points, and not an overwhelmingly large number like 336,776. To achieve this, we'll take the \texttt{flights} data frame, filter the rows so that only the 714 rows corresponding to Alaska Airlines flights are kept, and save this in a new data frame called \texttt{alaska\_flights} using the \texttt{\textless{}-} \emph{assignment} operator: \index{operators!assignment (<-)}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alaska\_flights }\OtherTok{\textless{}{-}}\NormalTok{ flights }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(carrier }\SpecialCharTok{==} \StringTok{"AS"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For now, we suggest you don't worry if you don't fully understand this code. We'll see later in Chapter \ref{wrangling} on data wrangling that this code uses the \texttt{dplyr} package for data wrangling to achieve our goal: it takes the \texttt{flights} data frame and \texttt{filter}s it to only return the rows where \texttt{carrier} is equal to \texttt{"AS"}, Alaska Airlines' carrier code. Recall from Section \ref{code} that testing for equality is specified with \index{using == instead of =} \texttt{==} and not \texttt{=}. Convince yourself that this code achieves what it is supposed to by exploring the resulting data frame by running \texttt{View(alaska\_flights)}. You'll see that it has 714 rows, consisting of only 714 Alaska Airlines flights.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC2.1)} Take a look at both the \texttt{flights} and \texttt{alaska\_flights} data frames by running \texttt{View(flights)} and \texttt{View(alaska\_flights)}. In what respect do these data frames differ? For example, think about the number of rows in each dataset.

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{geompoint}{%
\subsection{\texorpdfstring{Scatterplots via \texttt{geom\_point}}{Scatterplots via geom\_point}}\label{geompoint}}

Let's now go over the code that will create the desired scatterplot, while keeping in mind the grammar of graphics framework we introduced in Section \ref{grammarofgraphics}. Let's take a look at the code and break it down piece-by-piece.

\textbf{Note}: The printed version of this book uses \texttt{theme\_light()} instead of the default \texttt{theme\_grey()} for the plots created with \texttt{ggplot2} throughout the book. Bars and points are also converted to greyscale using \texttt{scale\_color\_grey()} and \texttt{scale\_fill\_grey()}. This helps with readability of the plots in the printed copy. As you follow along and run the code yourself, your plots will have a grey background instead of the white background in the printed book. Also, your plots will have colors beyond the greyscale versions provided in this printing.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ alaska\_flights, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ dep\_delay, }\AttributeTok{y =}\NormalTok{ arr\_delay)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Within the \texttt{ggplot()} \index{ggplot2!ggplot()} function, we specify two of the components of the grammar of graphics as arguments (i.e., inputs):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The \texttt{data} as the \texttt{alaska\_flights} data frame via \texttt{data\ =\ alaska\_flights}.
\item
  The \texttt{aes}thetic \index{ggplot2!mapping} \texttt{mapping} by setting \texttt{mapping\ =\ aes(x\ =\ dep\_delay,\ y\ =\ arr\_delay)}. Specifically, the variable \texttt{dep\_delay} maps to the \texttt{x} position aesthetic, while the variable \texttt{arr\_delay} maps to the \texttt{y} position.
\end{enumerate}

We then add a layer to the \texttt{ggplot()} function call using the \texttt{+} sign. The added layer in question specifies the third component of the grammar: the \texttt{geom}etric object. In this case, the geometric object is set to be points by specifying \texttt{geom\_point()}. After running these two lines of code in your console, you'll notice two outputs: a warning message and the graphic shown in Figure \ref{fig:noalpha}.

\begin{verbatim}
Warning: Removed 5 rows containing missing values (geom_point).
\end{verbatim}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/noalpha-1} 

}

\caption{Arrival delays versus departure delays for Alaska Airlines flights from NYC in 2013.}\label{fig:noalpha}
\end{figure}

Let's first unpack the graphic in Figure \ref{fig:noalpha}. Observe that a \emph{positive relationship} exists between \texttt{dep\_delay} and \texttt{arr\_delay}: as departure delays increase, arrival delays tend to also increase. Observe also the large mass of points clustered near (0, 0), the point indicating flights that neither departed nor arrived late.

Let's turn our attention to the warning message. R is alerting us to the fact that five rows were ignored due to them being missing. For these 5 rows, either the value for \texttt{dep\_delay} or \texttt{arr\_delay} or both were missing (recorded in R as \texttt{NA}), and thus these rows were ignored in our plot.

Before we continue, let's make a few more observations about this code that created the scatterplot. Note that the \texttt{+} sign comes at the end of lines, and not at the beginning. You'll get an error in R if you put it at the beginning of a line. \index{ggplot2!+} When adding layers to a plot, you are encouraged to start a new line after the \texttt{+} (by pressing the Return/Enter button on your keyboard) so that the code for each layer is on a new line. As we add more and more layers to plots, you'll see this will greatly improve the legibility of your code.

To stress the importance of adding the layer specifying the \texttt{geom}etric object, consider Figure \ref{fig:nolayers} where no layers are added. Because the \texttt{geom}etric object was not specified, we have a blank plot which is not very useful!

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ alaska\_flights, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ dep\_delay, }\AttributeTok{y =}\NormalTok{ arr\_delay))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/nolayers-1} 

}

\caption{A plot with no layers.}\label{fig:nolayers}
\end{figure}

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC2.2)} What are some practical reasons why \texttt{dep\_delay} and \texttt{arr\_delay} have a positive relationship?

\textbf{(LC2.3)} What variables in the \texttt{weather} data frame would you expect to have a negative correlation (i.e., a negative relationship) with \texttt{dep\_delay}? Why? Remember that we are focusing on numerical variables here. Hint: Explore the \texttt{weather} dataset by using the \texttt{View()} function.

\textbf{(LC2.4)} Why do you believe there is a cluster of points near (0, 0)? What does (0, 0) correspond to in terms of the Alaska Air flights?

\textbf{(LC2.5)} What are some other features of the plot that stand out to you?

\textbf{(LC2.6)} Create a new scatterplot using different variables in the \texttt{alaska\_flights} data frame by modifying the example given.

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{overplotting}{%
\subsection{Overplotting}\label{overplotting}}

The large mass of points near (0, 0) in Figure \ref{fig:noalpha} can cause some confusion since it is hard to tell the true number of points that are plotted. This is the result of a phenomenon called \index{overplotting} \emph{overplotting}. As one may guess, this corresponds to points being plotted on top of each other over and over again. When overplotting occurs, it is difficult to know the number of points being plotted. There are two methods to address the issue of overplotting. Either by

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Adjusting the transparency of the points or
\item
  Adding a little random ``jitter'', or random ``nudges'', to each of the points.
\end{enumerate}

\textbf{Method 1: Changing the transparency}

The first way of addressing overplotting is to change the transparency/opacity of the points by setting the \texttt{alpha} argument in \texttt{geom\_point()}. We can change the \texttt{alpha} argument to be any value between \texttt{0} and \texttt{1}, where \texttt{0} sets the points to be 100\% transparent and \texttt{1} sets the points to be 100\% opaque. By default, \texttt{alpha} is set to \texttt{1}. In other words, if we don't explicitly set an \texttt{alpha} value, R will use \texttt{alpha\ =\ 1}.

Note how the following code is identical to the code in Section \ref{scatterplots} that created the scatterplot with overplotting, but with \texttt{alpha\ =\ 0.2} added to the \texttt{geom\_point()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ alaska\_flights, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ dep\_delay, }\AttributeTok{y =}\NormalTok{ arr\_delay)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/alpha-1} 

}

\caption{Arrival vs. departure delays scatterplot with alpha = 0.2.}\label{fig:alpha}
\end{figure}

The key feature to note in Figure \ref{fig:alpha} is that the transparency \index{ggplot2!alpha} \index{adding transparency to plots} of the points is cumulative: areas with a high-degree of overplotting are darker, whereas areas with a lower degree are less dark. Note furthermore that there is no \texttt{aes()} surrounding \texttt{alpha\ =\ 0.2}. This is because we are not mapping a variable to an aesthetic attribute, but rather merely changing the default setting of \texttt{alpha}. In fact, you'll receive an error if you try to change the second line to read \texttt{geom\_point(aes(alpha\ =\ 0.2))}.

\textbf{Method 2: Jittering the points}

The second way of addressing overplotting is by \emph{jittering} all the points. This means giving each point a small ``nudge'' in a random direction. You can think of ``jittering'' as shaking the points around a bit on the plot. Let's illustrate using a simple example first. Say we have a data frame with 4 identical rows of x and y values: (0,0), (0,0), (0,0), and (0,0). In Figure \ref{fig:jitter-example-plot-1}, we present both the regular scatterplot of these 4 points (on the left) and its jittered counterpart (on the right).

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/jitter-example-plot-1-1} 

}

\caption{Regular and jittered scatterplot.}\label{fig:jitter-example-plot-1}
\end{figure}

In the left-hand regular scatterplot, observe that the 4 points are superimposed on top of each other. While we know there are 4 values being plotted, this fact might not be apparent to others. In the right-hand jittered scatterplot, it is now plainly evident that this plot involves four points since each point is given a random ``nudge.''

Keep in mind, however, that jittering is strictly a visualization tool; even after creating a jittered scatterplot, the original values saved in the data frame remain unchanged. \index{ggplot2!geom\_jitter()}

To create a jittered scatterplot, instead of using \texttt{geom\_point()}, we use \texttt{geom\_jitter()}. Observe how the following code is very similar to the code that created the scatterplot with overplotting in Subsection \ref{geompoint}, but with \texttt{geom\_point()} \index{ggplot2!geom\_point()} replaced with \texttt{geom\_jitter()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ alaska\_flights, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ dep\_delay, }\AttributeTok{y =}\NormalTok{ arr\_delay)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_jitter}\NormalTok{(}\AttributeTok{width =} \DecValTok{30}\NormalTok{, }\AttributeTok{height =} \DecValTok{30}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/jitter-1} 

}

\caption{Arrival versus departure delays jittered scatterplot.}\label{fig:jitter}
\end{figure}

In order to specify how much jitter to add, we adjusted the \texttt{width} and \texttt{height} arguments to \texttt{geom\_jitter()}. This corresponds to how hard you'd like to shake the plot in horizontal x-axis units and vertical y-axis units, respectively. In this case, both axes are in minutes. How much jitter should we add using the \texttt{width} and \texttt{height} arguments? On the one hand, it is important to add just enough jitter to break any overlap in points, but on the other hand, not so much that we completely alter the original pattern in points.

As can be seen in the resulting Figure \ref{fig:jitter}, in this case jittering doesn't really provide much new insight. In this particular case, it can be argued that changing the transparency of the points by setting \texttt{alpha} proved more effective. When would it be better to use a jittered scatterplot? When would it be better to alter the points' transparency? There is no single right answer that applies to all situations. You need to make a subjective choice and own that choice. At the very least when confronted with overplotting, however, we suggest you make both types of plots and see which one better emphasizes the point you are trying to make.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC2.7)} Why is setting the \texttt{alpha} argument value useful with scatterplots? What further information does it give you that a regular scatterplot cannot?

\textbf{(LC2.8)} After viewing Figure \ref{fig:alpha}, give an approximate range of arrival delays and departure delays that occur most frequently. How has that region changed compared to when you observed the same plot without \texttt{alpha\ =\ 0.2} set in Figure \ref{fig:noalpha}?

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{summary}{%
\subsection{Summary}\label{summary}}

Scatterplots display the relationship between two numerical variables. They are among the most commonly used plots because they can provide an immediate way to see the trend in one numerical variable versus another. However, if you try to create a scatterplot where either one of the two variables is not numerical, you might get strange results. Be careful!

With medium to large datasets, you may need to play around with the different modifications to scatterplots we saw such as changing the transparency/opacity of the points or by jittering the points. This tweaking is often a fun part of data visualization, since you'll have the chance to see different relationships emerge as you tinker with your plots.

\hypertarget{linegraphs}{%
\section{5NG\#2: Linegraphs}\label{linegraphs}}

The next of the five named graphs are linegraphs. Linegraphs \index{linegraphs} show the relationship between two numerical variables when the variable on the x-axis, also called the \emph{explanatory} variable\index{explanatory variable}, is of a sequential nature. In other words, there is an inherent ordering to the variable.

The most common examples of linegraphs have some notion of time on the x-axis: hours, days, weeks, years, etc. Since time is sequential, we connect consecutive observations of the variable on the y-axis with a line. Linegraphs that have some notion of time on the x-axis are also called \emph{time series} plots\index{time series plots}. Let's illustrate linegraphs using another dataset in the \texttt{nycflights13} \index{R packages!nycflights13} package: the \texttt{weather} data frame.

Let's explore the \texttt{weather} data frame by running \texttt{View(weather)} and \texttt{glimpse(weather)}. Furthermore let's read the associated help file by running \texttt{?weather} to bring up the help file.

Observe that there is a variable called \texttt{temp} of hourly temperature recordings in Fahrenheit at weather stations near all three major airports in New York City: Newark (\texttt{origin} code \texttt{EWR}), John F. Kennedy International (\texttt{JFK}), and LaGuardia (\texttt{LGA}). However, instead of considering hourly temperatures for all days in 2013 for all three airports, for simplicity let's only consider hourly temperatures at Newark airport for the first 15 days in January.

Recall in Section \ref{scatterplots}, we used the \texttt{filter()} function to only choose the subset of rows of \texttt{flights} corresponding to Alaska Airlines flights. We similarly use \texttt{filter()} \index{dplyr!filter()} here, but by using the \texttt{\&} operator we only choose the subset of rows of \texttt{weather} where the \texttt{origin} is \texttt{"EWR"}, the \texttt{month} is January, \textbf{and} the \texttt{day} is between \texttt{1} and \texttt{15}. Recall we performed a similar task in Section \ref{scatterplots} when creating the \texttt{alaska\_flights} data frame of only Alaska Airlines flights, a topic we'll explore more in Chapter \ref{wrangling} on data wrangling.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{weather }\OtherTok{\textless{}{-}}\NormalTok{ nycflights13}\SpecialCharTok{::}\NormalTok{weather}
\NormalTok{early\_january\_weather }\OtherTok{\textless{}{-}}\NormalTok{ weather }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(origin }\SpecialCharTok{==} \StringTok{"EWR"} \SpecialCharTok{\&}\NormalTok{ month }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ day }\SpecialCharTok{\textless{}=} \DecValTok{15}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC2.9)} Take a look at both the \texttt{weather} and \texttt{early\_january\_weather} data frames by running \texttt{View(weather)} and \texttt{View(early\_january\_weather)}. In what respect do these data frames differ?

\textbf{(LC2.10)} \texttt{View()} the \texttt{flights} data frame again. Why does the \texttt{time\_hour} variable uniquely identify the hour of the measurement, whereas the \texttt{hour} variable does not?

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{geomline}{%
\subsection{\texorpdfstring{Linegraphs via \texttt{geom\_line}}{Linegraphs via geom\_line}}\label{geomline}}

Let's create a time series plot of the hourly temperatures saved in the \texttt{early\_january\_weather} data frame by using \texttt{geom\_line()} to create a linegraph\index{ggplot2!geom\_line()}, instead of using \texttt{geom\_point()} like we used previously to create scatterplots:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ early\_january\_weather, }
       \AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ time\_hour, }\AttributeTok{y =}\NormalTok{ temp)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/hourlytemp-1} 

}

\caption{Hourly temperature in Newark for January 1-15, 2013.}\label{fig:hourlytemp}
\end{figure}

Much as with the \texttt{ggplot()} code that created the scatterplot of departure and arrival delays for Alaska Airlines flights in Figure \ref{fig:noalpha}, let's break down this code piece-by-piece in terms of the grammar of graphics:

Within the \texttt{ggplot()} function call, we specify two of the components of the grammar of graphics as arguments:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The \texttt{data} to be the \texttt{early\_january\_weather} data frame by setting \texttt{data\ =\ early\_january\_weather}.
\item
  The \texttt{aes}thetic \texttt{mapping} by setting \texttt{mapping\ =\ aes(x\ =\ time\_hour,\ y\ =\ temp)}. Specifically, the variable \texttt{time\_hour} maps to the \texttt{x} position aesthetic, while the variable \texttt{temp} maps to the \texttt{y} position aesthetic.
\end{enumerate}

We add a layer to the \texttt{ggplot()} function call using the \texttt{+} sign. The layer in question specifies the third component of the grammar: the \texttt{geom}etric object in question. In this case, the geometric object is a \texttt{line} set by specifying \texttt{geom\_line()}.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC2.11)} Why should linegraphs be avoided when there is not a clear ordering of the horizontal axis?

\textbf{(LC2.12)} Why are linegraphs frequently used when time is the explanatory variable on the x-axis?

\textbf{(LC2.13)} Plot a time series of a variable other than \texttt{temp} for Newark Airport in the first 15 days of January 2013.

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{summary-1}{%
\subsection{Summary}\label{summary-1}}

Linegraphs, just like scatterplots, display the relationship between two numerical variables. However, it is preferred to use linegraphs over scatterplots when the variable on the x-axis (i.e., the explanatory variable) has an inherent ordering, such as some notion of time.

\hypertarget{histograms}{%
\section{5NG\#3: Histograms}\label{histograms}}

Let's consider the \texttt{temp} variable in the \texttt{weather} data frame once again, but unlike with the linegraphs in Section \ref{linegraphs}, let's say we don't care about its relationship with time, but rather we only care about how the values of \texttt{temp} \emph{distribute}. In other words:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What are the smallest and largest values?
\item
  What is the ``center'' or ``most typical'' value?
\item
  How do the values spread out?
\item
  What are frequent and infrequent values?
\end{enumerate}

One way to visualize this \emph{distribution} \index{distribution} of this single variable \texttt{temp} is to plot them on a horizontal line as we do in Figure \ref{fig:temp-on-line}:

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/temp-on-line-1} 

}

\caption{Plot of hourly temperature recordings from NYC in 2013.}\label{fig:temp-on-line}
\end{figure}

This gives us a general idea of how the values of \texttt{temp} distribute: observe that temperatures vary from around 11°F (-11°C) up to 100°F (38°C). Furthermore, there appear to be more recorded temperatures between 40°F and 60°F than outside this range. However, because of the high degree of overplotting in the points, it's hard to get a sense of exactly how many values are between say 50°F and 55°F.

What is commonly produced instead of Figure \ref{fig:temp-on-line} is known as a \index{histograms} \emph{histogram}. A histogram is a plot that visualizes the \emph{distribution} of a numerical value as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We first cut up the x-axis into a series of \index{histograms!bins} \emph{bins}, where each bin represents a range of values.
\item
  For each bin, we count the number of observations that fall in the range corresponding to that bin.
\item
  Then for each bin, we draw a bar whose height marks the corresponding count.
\end{enumerate}

Let's drill-down on an example of a histogram, shown in Figure \ref{fig:histogramexample}.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/histogramexample-1} 

}

\caption{Example histogram.}\label{fig:histogramexample}
\end{figure}

Let's focus only on temperatures between 30°F (-1°C) and 60°F (15°C) for now. Observe that there are three bins of equal width between 30°F and 60°F. Thus we have three bins of width 10°F each: one bin for the 30-40°F range, another bin for the 40-50°F range, and another bin for the 50-60°F range. Since:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The bin for the 30-40°F range has a height of around 5000. In other words, around 5000 of the hourly temperature recordings are between 30°F and 40°F.
\item
  The bin for the 40-50°F range has a height of around 4300. In other words, around 4300 of the hourly temperature recordings are between 40°F and 50°F.
\item
  The bin for the 50-60°F range has a height of around 3500. In other words, around 3500 of the hourly temperature recordings are between 50°F and 60°F.
\end{enumerate}

All nine bins spanning 10°F to 100°F on the x-axis have this interpretation.

\hypertarget{geomhistogram}{%
\subsection{\texorpdfstring{Histograms via \texttt{geom\_histogram}}{Histograms via geom\_histogram}}\label{geomhistogram}}

Let's now present the \texttt{ggplot()} code to plot your first histogram! Unlike with scatterplots and linegraphs, there is now only one variable being mapped in \texttt{aes()}: the single numerical variable \texttt{temp}. The y-aesthetic of a histogram, the count of the observations in each bin, gets computed for you automatically. Furthermore, the geometric object layer is now a \texttt{geom\_histogram()}. \index{ggplot2!geom\_histogram()} After running the following code, you'll see the histogram in Figure \ref{fig:weather-histogram} as well as warning messages. We'll discuss the warning messages first.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ weather, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ temp)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
\end{verbatim}

\begin{verbatim}
Warning: Removed 1 rows containing non-finite values (stat_bin).
\end{verbatim}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/weather-histogram-1} 

}

\caption{Histogram of hourly temperatures at three NYC airports.}\label{fig:weather-histogram}
\end{figure}

The first message is telling us that the histogram was constructed using \texttt{bins\ =\ 30} for 30 equally spaced bins. This is known in computer programming as a default value; unless you override this default number of bins with a number you specify, R will choose 30 by default. We'll see in the next section how to change the number of bins to another value than the default.

The second message is telling us something similar to the warning message we received when we ran the code to create a scatterplot of departure and arrival delays for Alaska Airlines flights in Figure \ref{fig:noalpha}: that because one row has a missing \texttt{NA} value for \texttt{temp}, it was omitted from the histogram. R is just giving us a friendly heads up that this was the case.

Now let's unpack the resulting histogram in Figure \ref{fig:weather-histogram}. Observe that values less than 25°F as well as values above 80°F are rather rare. However, because of the large number of bins, it's hard to get a sense for which range of temperatures is spanned by each bin; everything is one giant amorphous blob. So let's add white vertical borders demarcating the bins by adding a \texttt{color\ =\ "white"} argument to \texttt{geom\_histogram()} and ignore the warning about setting the number of bins to a better value:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ weather, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ temp)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{color =} \StringTok{"white"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/weather-histogram-2-1} 

}

\caption{Histogram of hourly temperatures at three NYC airports with white borders.}\label{fig:weather-histogram-2}
\end{figure}

We now have an easier time associating ranges of temperatures to each of the bins in Figure \ref{fig:weather-histogram-2}. We can also vary the color of the bars by setting the \index{ggplot2!fill} \texttt{fill} argument. For example, you can set the bin colors to be ``blue steel'' by setting \texttt{fill\ =\ "steelblue"}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ weather, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ temp)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{color =} \StringTok{"white"}\NormalTok{, }\AttributeTok{fill =} \StringTok{"steelblue"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

If you're curious, run \index{colors()} \texttt{colors()} to see all 657 possible choice of colors in R!

\hypertarget{adjustbins}{%
\subsection{Adjusting the bins}\label{adjustbins}}

Observe in Figure \ref{fig:weather-histogram-2} that in the 50-75°F range there appear to be roughly 8 bins. Thus each bin has width 25 divided by 8, or 3.125°F, which is not a very easily interpretable range to work with. Let's improve this by adjusting the number of bins in our histogram in one of two ways:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  By adjusting the number of bins via the \index{geom\_histogram()!bins} \texttt{bins} argument to \texttt{geom\_histogram()}.
\item
  By adjusting the width of the bins via the \index{geom\_histogram()!binwidth} \texttt{binwidth} argument to \texttt{geom\_histogram()}.
\end{enumerate}

Using the first method, we have the power to specify how many bins we would like to cut the x-axis up in. As mentioned in the previous section, the default number of bins is 30. We can override this default, to say 40 bins, as follows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ weather, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ temp)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{bins =} \DecValTok{40}\NormalTok{, }\AttributeTok{color =} \StringTok{"white"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Using the second method, instead of specifying the number of bins, we specify the width of the bins by using the \texttt{binwidth} argument in the \texttt{geom\_histogram()} layer. For example, let's set the width of each bin to be 10°F.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ weather, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ temp)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{10}\NormalTok{, }\AttributeTok{color =} \StringTok{"white"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We compare both resulting histograms side-by-side in Figure \ref{fig:hist-bins}.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/hist-bins-1} 

}

\caption{Setting histogram bins in two ways.}\label{fig:hist-bins}
\end{figure}

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC2.14)} What does changing the number of bins from 30 to 40 tell us about the distribution of temperatures?

\textbf{(LC2.15)} Would you classify the distribution of temperatures as symmetric or skewed in one direction or another?

\textbf{(LC2.16)} What would you guess is the ``center'' value in this distribution? Why did you make that choice?

\textbf{(LC2.17)} Is this data spread out greatly from the center or is it close? Why?

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{summary-2}{%
\subsection{Summary}\label{summary-2}}

Histograms, unlike scatterplots and linegraphs, present information on only a single numerical variable. Specifically, they are visualizations of the distribution of the numerical variable in question.

\hypertarget{facets}{%
\section{Facets}\label{facets}}

Before continuing with the next of the 5NG, let's briefly introduce a new concept called \emph{faceting}. Faceting is used when we'd like to split a particular visualization by the values of another variable. This will create multiple copies of the same type of plot with matching x and y axes, but whose content will differ.

For example, suppose we were interested in looking at how the histogram of hourly temperature recordings at the three NYC airports we saw in Figure \ref{fig:histogramexample} differed in each month. We could ``split'' this histogram by the 12 possible months in a given year. In other words, we would plot histograms of \texttt{temp} for each \texttt{month} separately. We do this by adding \texttt{facet\_wrap(\textasciitilde{}\ month)} layer. Note the \texttt{\textasciitilde{}} is a ``tilde'' and can generally be found on the key next to the ``1'' key on US keyboards. The tilde is required and you'll receive the error \texttt{Error\ in\ as.quoted(facets)\ :\ object\ \textquotesingle{}month\textquotesingle{}\ not\ found} if you don't include it here.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ weather, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ temp)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{5}\NormalTok{, }\AttributeTok{color =} \StringTok{"white"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ month)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/facethistogram-1} 

}

\caption{Faceted histogram of hourly temperatures by month.}\label{fig:facethistogram}
\end{figure}

We can also specify the number of rows and columns in the grid by using the \texttt{nrow} and \texttt{ncol} arguments inside of \index{ggplot2!facet\_wrap()} \texttt{facet\_wrap()}. For example, say we would like our faceted histogram to have 4 rows instead of 3. We simply add an \texttt{nrow\ =\ 4} argument to \texttt{facet\_wrap(\textasciitilde{}\ month)}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ weather, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ temp)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{5}\NormalTok{, }\AttributeTok{color =} \StringTok{"white"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ month, }\AttributeTok{nrow =} \DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/facethistogram2-1} 

}

\caption{Faceted histogram with 4 instead of 3 rows.}\label{fig:facethistogram2}
\end{figure}

Observe in both Figures \ref{fig:facethistogram} and \ref{fig:facethistogram2} that as we might expect in the Northern Hemisphere, temperatures tend to be higher in the summer months, while they tend to be lower in the winter.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC2.18)} What other things do you notice about this faceted plot? How does a faceted plot help us see relationships between two variables?

\textbf{(LC2.19)} What do the numbers 1-12 correspond to in the plot? What about 25, 50, 75, 100?

\textbf{(LC2.20)} For which types of datasets would faceted plots not work well in comparing relationships between variables? Give an example describing the nature of these variables and other important characteristics.

\textbf{(LC2.21)} Does the \texttt{temp} variable in the \texttt{weather} dataset have a lot of variability? Why do you say that?

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{boxplots}{%
\section{5NG\#4: Boxplots}\label{boxplots}}

While faceted histograms are one type of visualization used to compare the distribution of a numerical variable split by the values of another variable, another type of visualization that achieves this same goal is a \emph{side-by-side boxplot}. A boxplot \index{boxplots} is constructed from the information provided in the \emph{five-number summary} of a numerical variable (see Appendix \ref{appendix-stat-terms}).

To keep things simple for now, let's only consider the 2141 hourly temperature recordings for the month of November, each represented as a jittered point in Figure \ref{fig:nov1}.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/nov1-1} 

}

\caption{November temperatures represented as jittered points.}\label{fig:nov1}
\end{figure}

These 2141 observations have the following \emph{five-number summary}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Minimum: 21°F
\item
  First quartile (25th percentile): 36°F
\item
  Median (second quartile, 50th percentile): 45°F
\item
  Third quartile (75th percentile): 52°F
\item
  Maximum: 71°F
\end{enumerate}

In the leftmost plot of Figure \ref{fig:nov2}, let's mark these 5 values with dashed horizontal lines on top of the 2141 points. In the middle plot of Figure \ref{fig:nov2} let's add the \emph{boxplot}. In the rightmost plot of Figure \ref{fig:nov2}, let's remove the points and the dashed horizontal lines for clarity's sake.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/nov2-1} 

}

\caption{Building up a boxplot of November temperatures.}\label{fig:nov2}
\end{figure}

What the boxplot does is visually summarize the 2141 points by cutting the 2141 temperature recordings into \emph{quartiles} at the dashed lines, where each quartile contains roughly 2141 \(\div\) 4 \(\approx\) 535 observations. Thus

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  25\% of points fall below the bottom edge of the box, which is the first quartile of 36°F. In other words, 25\% of observations were below 36°F.
\item
  25\% of points fall between the bottom edge of the box and the solid middle line, which is the median of 45°F. Thus, 25\% of observations were between 36°F and 45°F and 50\% of observations were below 45°F.
\item
  25\% of points fall between the solid middle line and the top edge of the box, which is the third quartile of 52°F. It follows that 25\% of observations were between 45°F and 52°F and 75\% of observations were below 52°F.
\item
  25\% of points fall above the top edge of the box. In other words, 25\% of observations were above 52°F.
\item
  The middle 50\% of points lie within the \emph{interquartile range (IQR)} \index{interquartile range (IQR)} between the first and third quartile. Thus, the IQR for this example is 52 - 36 = 16°F. The interquartile range is a measure of a numerical variable's \emph{spread}.
\end{enumerate}

Furthermore, in the rightmost plot of Figure \ref{fig:nov2}, we see the \emph{whiskers} \index{boxplots!whiskers} of the boxplot. The whiskers stick out from either end of the box all the way to the minimum and maximum observed temperatures of 21°F and 71°F, respectively. However, the whiskers don't always extend to the smallest and largest observed values as they do here. They in fact extend no more than 1.5 \(\times\) the interquartile range from either end of the box. In this case of the November temperatures, no more than 1.5 \(\times\) 16°F = 24°F from either end of the box. Any observed values outside this range get marked with points called \emph{outliers}, which we'll see in the next section.

\hypertarget{geomboxplot}{%
\subsection{\texorpdfstring{Boxplots via \texttt{geom\_boxplot}}{Boxplots via geom\_boxplot}}\label{geomboxplot}}

Let's now create a side-by-side boxplot \index{boxplots!side-by-side} of hourly temperatures split by the 12 months as we did previously with the faceted histograms. We do this by mapping the \texttt{month} variable to the x-position aesthetic, the \texttt{temp} variable to the y-position aesthetic, and by adding a \texttt{geom\_boxplot()} layer:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ weather, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ month, }\AttributeTok{y =}\NormalTok{ temp)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/badbox-1} 

}

\caption{Invalid boxplot specification.}\label{fig:badbox}
\end{figure}

\begin{verbatim}
Warning messages:
1: Continuous x aesthetic -- did you forget aes(group=...)? 
2: Removed 1 rows containing non-finite values (stat_boxplot). 
\end{verbatim}

Observe in Figure \ref{fig:badbox} that this plot does not provide information about temperature separated by month. The first warning message clues us in as to why. It is telling us that we have a ``continuous'', or numerical variable, on the x-position aesthetic. Boxplots, however, require a categorical variable to be mapped to the x-position aesthetic. The second warning message is identical to the warning message when plotting a histogram of hourly temperatures: that one of the values was recorded as \texttt{NA} missing.

We can convert the numerical variable \texttt{month} into a \texttt{factor} categorical variable by using the \texttt{factor()} \index{factors} function. So after applying \texttt{factor(month)}, month goes from having numerical values just the 1, 2, \ldots, and 12 to having an associated ordering. With this ordering, \texttt{ggplot()} now knows how to work with this variable to produce the needed plot.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ weather, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{factor}\NormalTok{(month), }\AttributeTok{y =}\NormalTok{ temp)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/monthtempbox-1} 

}

\caption{Side-by-side boxplot of temperature split by month.}\label{fig:monthtempbox}
\end{figure}

The resulting Figure \ref{fig:monthtempbox} shows 12 separate ``box and whiskers'' plots similar to the rightmost plot of Figure \ref{fig:nov2} of only November temperatures. Thus the different boxplots are shown ``side-by-side.''

\begin{itemize}
\tightlist
\item
  The ``box'' portions of the visualization represent the 1st quartile, the median (the 2nd quartile), and the 3rd quartile.
\item
  The height of each box (the value of the 3rd quartile minus the value of the 1st quartile) is the interquartile range (IQR). It is a measure of the spread of the middle 50\% of values, with longer boxes indicating more variability.
\item
  The ``whisker'' portions of these plots extend out from the bottoms and tops of the boxes and represent points less than the 25th percentile and greater than the 75th percentiles, respectively. They're set to extend out no more than \(1.5 \times IQR\) units away from either end of the boxes. We say ``no more than'' because the ends of the whiskers have to correspond to observed temperatures. The length of these whiskers show how the data outside the middle 50\% of values vary, with longer whiskers indicating more variability.
\item
  The dots representing values falling outside the whiskers are called \index{outliers} \emph{outliers}. These can be thought of as anomalous (``out-of-the-ordinary'') values.
\end{itemize}

It is important to keep in mind that the definition of an outlier is somewhat arbitrary and not absolute. In this case, they are defined by the length of the whiskers, which are no more than \(1.5 \times IQR\) units long for each boxplot. Looking at this side-by-side plot we can see, as expected, that summer months (6 through 8) have higher median temperatures as evidenced by the higher solid lines in the middle of the boxes. We can easily compare temperatures across months by drawing imaginary horizontal lines across the plot. Furthermore, the heights of the 12 boxes as quantified by the interquartile ranges are informative too; they tell us about variability, or spread, of temperatures recorded in a given month.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC2.22)} What does the dot at the bottom of the plot for May correspond to? Explain what might have occurred in May to produce this point.

\textbf{(LC2.23)} Which months have the highest variability in temperature? What reasons can you give for this?

\textbf{(LC2.24)} We looked at the distribution of the numerical variable \texttt{temp} split by the numerical variable \texttt{month} that we converted using the \texttt{factor()} function in order to make a side-by-side boxplot. Why would a boxplot of \texttt{temp} split by the numerical variable \texttt{pressure} similarly converted to a categorical variable using the \texttt{factor()} not be informative?

\textbf{(LC2.25)} Boxplots provide a simple way to identify outliers. Why may outliers be easier to identify when looking at a boxplot instead of a faceted histogram?

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{summary-3}{%
\subsection{Summary}\label{summary-3}}

Side-by-side boxplots provide us with a way to compare the distribution of a numerical variable across multiple values of another variable. One can see where the median falls across the different groups by comparing the solid lines in the center of the boxes.

To study the spread of a numerical variable within one of the boxes, look at both the length of the box and also how far the whiskers extend from either end of the box. Outliers are even more easily identified when looking at a boxplot than when looking at a histogram as they are marked with distinct points.

\hypertarget{geombar}{%
\section{5NG\#5: Barplots}\label{geombar}}

Both histograms and boxplots are tools to visualize the distribution of numerical variables. Another commonly desired task is to visualize the distribution of a categorical variable. This is a simpler task, as we are simply counting different categories within a categorical variable, also known as the \index{levels} \emph{levels} of the categorical variable. Often the best way to visualize these different counts, also known as \index{frequencies} \emph{frequencies}, is with barplots (also called barcharts).

One complication, however, is how your data is represented. Is the categorical variable of interest ``pre-counted'' or not? For example, run the following code that manually creates two data frames representing a collection of fruit: 3 apples and 2 oranges.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fruits }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{fruit =} \FunctionTok{c}\NormalTok{(}\StringTok{"apple"}\NormalTok{, }\StringTok{"apple"}\NormalTok{, }\StringTok{"orange"}\NormalTok{, }\StringTok{"apple"}\NormalTok{, }\StringTok{"orange"}\NormalTok{)}
\NormalTok{)}
\NormalTok{fruits\_counted }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{fruit =} \FunctionTok{c}\NormalTok{(}\StringTok{"apple"}\NormalTok{, }\StringTok{"orange"}\NormalTok{),}
  \AttributeTok{number =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We see both the \texttt{fruits} and \texttt{fruits\_counted} data frames represent the same collection of fruit. Whereas \texttt{fruits} just lists the fruit individually\ldots{}

\begin{verbatim}
# A tibble: 5 x 1
  fruit 
  <chr> 
1 apple 
2 apple 
3 orange
4 apple 
5 orange
\end{verbatim}

\ldots{} \texttt{fruits\_counted} has a variable \texttt{count} which represent the ``pre-counted'' values of each fruit.

\begin{verbatim}
# A tibble: 2 x 2
  fruit  number
  <chr>   <dbl>
1 apple       3
2 orange      2
\end{verbatim}

Depending on how your categorical data is represented, you'll need to add a different \texttt{geom}etric layer type to your \texttt{ggplot()} to create a barplot, as we now explore.

\hypertarget{barplots-via-geom_bar-or-geom_col}{%
\subsection{\texorpdfstring{Barplots via \texttt{geom\_bar} or \texttt{geom\_col}}{Barplots via geom\_bar or geom\_col}}\label{barplots-via-geom_bar-or-geom_col}}

Let's generate barplots using these two different representations of the same basket of fruit: 3 apples and 2 oranges. Using the \texttt{fruits} data frame where all 5 fruits are listed individually in 5 rows, we map the \texttt{fruit} variable to the x-position aesthetic and add a \index{ggplot2!geom\_bar()} \texttt{geom\_bar()} layer:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ fruits, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ fruit)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/geombar-1} 

}

\caption{Barplot when counts are not pre-counted.}\label{fig:geombar}
\end{figure}

However, using the \texttt{fruits\_counted} data frame where the fruits have been ``pre-counted'', we once again map the \texttt{fruit} variable to the x-position aesthetic, but here we also map the \texttt{count} variable to the y-position aesthetic, and add a \index{ggplot2!geom\_col()} \texttt{geom\_col()} layer instead.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ fruits\_counted, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ fruit, }\AttributeTok{y =}\NormalTok{ number)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/geomcol-1} 

}

\caption{Barplot when counts are pre-counted.}\label{fig:geomcol}
\end{figure}

Compare the barplots in Figures \ref{fig:geombar} and \ref{fig:geomcol}. They are identical because they reflect counts of the same five fruits. However, depending on how our categorical data is represented, either ``pre-counted'' or not, we must add a different \texttt{geom} layer. When the categorical variable whose distribution you want to visualize

\begin{itemize}
\tightlist
\item
  Is \emph{not} pre-counted in your data frame, we use \texttt{geom\_bar()}.
\item
  Is pre-counted in your data frame, we use \texttt{geom\_col()} with the y-position aesthetic mapped to the variable that has the counts.
\end{itemize}

Let's now go back to the \texttt{flights} data frame in the \texttt{nycflights13} package and visualize the distribution of the categorical variable \texttt{carrier}. In other words, let's visualize the number of domestic flights out of New York City each airline company flew in 2013. Recall from Subsection \ref{exploredataframes} when you first explored the \texttt{flights} data frame, you saw that each row corresponds to a flight. In other words, the \texttt{flights} data frame is more like the \texttt{fruits} data frame than the \texttt{fruits\_counted} data frame because the flights have not been pre-counted by \texttt{carrier}. Thus we should use \texttt{geom\_bar()} instead of \texttt{geom\_col()} to create a barplot. Much like a \texttt{geom\_histogram()}, there is only one variable in the \texttt{aes()} aesthetic mapping: the variable \texttt{carrier} gets mapped to the \texttt{x}-position. As a difference though, histograms have bars that touch whereas bar graphs have white space between the bars going from left to right.



\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ flights, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ carrier)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/flightsbar-1} 

}

\caption{Number of flights departing NYC in 2013 by airline using geom\_bar().}\label{fig:flightsbar}
\end{figure}

Observe in Figure \ref{fig:flightsbar} that United Airlines (UA), JetBlue Airways (B6), and ExpressJet Airlines (EV) had the most flights depart NYC in 2013. If you don't know which airlines correspond to which carrier codes, then run \texttt{View(airlines)} to see a directory of airlines. For example, B6 is JetBlue Airways. Alternatively, say you had a data frame where the number of flights for each \texttt{carrier} was pre-counted as in Table \ref{tab:flights-counted}.

\begingroup\fontsize{10}{12}\selectfont

\begin{longtable}[t]{lr}
\caption{\label{tab:flights-counted}Number of flights pre-counted for each carrier}\\
\toprule
carrier & number\\
\midrule
9E & 18460\\
AA & 32729\\
AS & 714\\
B6 & 54635\\
DL & 48110\\
EV & 54173\\
F9 & 685\\
FL & 3260\\
HA & 342\\
MQ & 26397\\
OO & 32\\
UA & 58665\\
US & 20536\\
VX & 5162\\
WN & 12275\\
YV & 601\\
\bottomrule
\end{longtable}
\endgroup{}

In order to create a barplot visualizing the distribution of the categorical variable \texttt{carrier} in this case, we would now use \texttt{geom\_col()} instead of \texttt{geom\_bar()}, with an additional \texttt{y\ =\ number} in the aesthetic mapping on top of the \texttt{x\ =\ carrier}. The resulting barplot would be identical to Figure \ref{fig:flightsbar}.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC2.26)} Why are histograms inappropriate for categorical variables?

\textbf{(LC2.27)} What is the difference between histograms and barplots?

\textbf{(LC2.28)} How many Envoy Air flights departed NYC in 2013?

\textbf{(LC2.29)} What was the 7th highest airline for departed flights from NYC in 2013? How could we better present the table to get this answer quickly?

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{must-avoid-pie-charts}{%
\subsection{Must avoid pie charts!}\label{must-avoid-pie-charts}}

One of the most common plots used to visualize the distribution of categorical data is the \index{pie charts} pie chart. While they may seem harmless enough, pie charts actually present a problem in that humans are unable to judge angles well. As Naomi Robbins describes in her book, \emph{Creating More Effective Graphs} \citep{robbins2013}, we overestimate angles greater than 90 degrees and we underestimate angles less than 90 degrees. In other words, it is difficult for us to determine the relative size of one piece of the pie compared to another.

Let's examine the same data used in our previous barplot of the number of flights departing NYC by airline in Figure \ref{fig:flightsbar}, but this time we will use a pie chart in Figure \ref{fig:carrierpie}. Try to answer the following questions:

\begin{itemize}
\tightlist
\item
  How much larger is the portion of the pie for ExpressJet Airlines (\texttt{EV}) compared to US Airways (\texttt{US})?
\item
  What is the third largest carrier in terms of departing flights?
\item
  How many carriers have fewer flights than United Airlines (\texttt{UA})?
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/carrierpie-1} 

}

\caption{The dreaded pie chart.}\label{fig:carrierpie}
\end{figure}

While it is quite difficult to answer these questions when looking at the pie chart in Figure \ref{fig:carrierpie}, we can much more easily answer these questions using the barchart in Figure \ref{fig:flightsbar}. This is true since barplots present the information in a way such that comparisons between categories can be made with single horizontal lines, whereas pie charts present the information in a way such that comparisons must be made by \index{pie charts!problems with} comparing angles.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC2.30)} Why should pie charts be avoided and replaced by barplots?

\textbf{(LC2.31)} Why do you think people continue to use pie charts?

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{two-categ-barplot}{%
\subsection{Two categorical variables}\label{two-categ-barplot}}

Barplots are a very common way to visualize the frequency of different categories, or levels, of a single categorical variable. Another use of barplots is to visualize the \emph{joint} distribution of two categorical variables at the same time. Let's examine the \emph{joint} distribution of outgoing domestic flights from NYC by \texttt{carrier} as well as \texttt{origin}. In other words, the number of flights for each \texttt{carrier} and \texttt{origin} combination.

For example, the number of WestJet flights from \texttt{JFK}, the number of WestJet flights from \texttt{LGA}, the number of WestJet flights from \texttt{EWR}, the number of American Airlines flights from \texttt{JFK}, and so on. Recall the \texttt{ggplot()} code that created the barplot of \texttt{carrier} frequency in Figure \ref{fig:flightsbar}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ flights, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ carrier)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

We can now map the additional variable \texttt{origin} by adding a \texttt{fill\ =\ origin} inside the \texttt{aes()} aesthetic mapping.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ flights, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ carrier, }\AttributeTok{fill =}\NormalTok{ origin)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/flights-stacked-bar-1} 

}

\caption{Stacked barplot of flight amount by carrier and origin.}\label{fig:flights-stacked-bar}
\end{figure}

Figure \ref{fig:flights-stacked-bar} is an example of a \index{barplot!stacked} \emph{stacked barplot}. While simple to make, in certain aspects it is not ideal. For example, it is difficult to compare the heights of the different colors between the bars, corresponding to comparing the number of flights from each \texttt{origin} airport between the carriers.

Before we continue, let's address some common points of confusion among new R users. First, the \texttt{fill} aesthetic corresponds to the color used to fill the bars, while the \texttt{color} aesthetic corresponds to the color of the outline of the bars. This is identical to how we added color to our histogram in Subsection \ref{geomhistogram}: we set the outline of the bars to white by setting \texttt{color\ =\ "white"} and the colors of the bars to blue steel by setting \texttt{fill\ =\ "steelblue"}. Observe in Figure \ref{fig:flights-stacked-bar-color} that mapping \texttt{origin} to \texttt{color} and not \texttt{fill} yields grey bars with different colored outlines.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ flights, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ carrier, }\AttributeTok{color =}\NormalTok{ origin)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/flights-stacked-bar-color-1} 

}

\caption{Stacked barplot with color aesthetic used instead of fill.}\label{fig:flights-stacked-bar-color}
\end{figure}

Second, note that \texttt{fill} is another aesthetic mapping much like \texttt{x}-position; thus we were careful to include it within the parentheses of the \texttt{aes()} mapping. The following code, where the \texttt{fill} aesthetic is specified outside the \texttt{aes()} mapping will yield an error. This is a fairly common error that new \texttt{ggplot} users make:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ flights, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ carrier), }\AttributeTok{fill =}\NormalTok{ origin) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

An alternative to stacked barplots are \index{barplot!side-by-side} \emph{side-by-side barplots}, also known as \emph{dodged barplots}, as seen in Figure \ref{fig:flights-dodged-bar-color}. The code to create a side-by-side barplot is identical to the code to create a stacked barplot, but with a \index{ggplot2!position} \texttt{position\ =\ "dodge"} argument added to \texttt{geom\_bar()}. In other words, we are overriding the default barplot type, which is a \emph{stacked} barplot, and specifying it to be a side-by-side barplot instead.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ flights, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ carrier, }\AttributeTok{fill =}\NormalTok{ origin)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/flights-dodged-bar-color-1} 

}

\caption{Side-by-side barplot comparing number of flights by carrier and origin.}\label{fig:flights-dodged-bar-color}
\end{figure}

Note the width of the bars for \texttt{AS}, \texttt{F9}, \texttt{FL}, \texttt{HA} and \texttt{YV} is different than the others. We can make one tweak to the \texttt{position} argument to get them to be the same size in terms of width as the other bars by using the more robust \texttt{position\_dodge()} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ flights, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ carrier, }\AttributeTok{fill =}\NormalTok{ origin)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{position =} \FunctionTok{position\_dodge}\NormalTok{(}\AttributeTok{preserve =} \StringTok{"single"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/flights-dodged-bar-color-tweak-1} 

}

\caption{Side-by-side barplot comparing number of flights by carrier and origin (with formatting tweak).}\label{fig:flights-dodged-bar-color-tweak}
\end{figure}

Lastly, another type of barplot is a \index{barplot!faceted} \emph{faceted barplot}. Recall in Section \ref{facets} we visualized the distribution of hourly temperatures at the 3 NYC airports \emph{split} by month using facets. We apply the same principle to our barplot visualizing the frequency of \texttt{carrier} split by \texttt{origin}: instead of mapping \texttt{origin} to \texttt{fill} we include it as the variable to create small multiples of the plot across the levels of \texttt{origin}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ flights, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ carrier)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ origin, }\AttributeTok{ncol =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/facet-bar-vert-1} 

}

\caption{Faceted barplot comparing the number of flights by carrier and origin.}\label{fig:facet-bar-vert}
\end{figure}

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC2.32)} What kinds of questions are not easily answered by looking at Figure \ref{fig:flights-stacked-bar}?

\textbf{(LC2.33)} What can you say, if anything, about the relationship between airline and airport in NYC in 2013 in regards to the number of departing flights?

\textbf{(LC2.34)} Why might the side-by-side barplot be preferable to a stacked barplot in this case?

\textbf{(LC2.35)} What are the disadvantages of using a dodged barplot, in general?

\textbf{(LC2.36)} Why is the faceted barplot preferred to the side-by-side and stacked barplots in this case?

\textbf{(LC2.37)} What information about the different carriers at different airports is more easily seen in the faceted barplot?

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{summary-4}{%
\subsection{Summary}\label{summary-4}}

Barplots are a common way of displaying the distribution of a categorical variable, or in other words the frequency with which the different categories (also called \emph{levels}) occur. They are easy to understand and make it easy to make comparisons across levels. Furthermore, when trying to visualize the relationship of two categorical variables, you have many options: stacked barplots, side-by-side barplots, and faceted barplots. Depending on what aspect of the relationship you are trying to emphasize, you will need to make a choice between these three types of barplots and own that choice.

\hypertarget{data-vis-conclusion}{%
\section{Conclusion}\label{data-vis-conclusion}}

\hypertarget{summary-table}{%
\subsection{Summary table}\label{summary-table}}

Let's recap all five of the five named graphs (5NG) \index{five named graphs} in Table \ref{tab:viz-summary-table} summarizing their differences. Using these 5NG, you'll be able to visualize the distributions and relationships of variables contained in a wide array of datasets. This will be even more the case as we start to map more variables to more of each \texttt{geom}etric object's \texttt{aes}thetic attribute options, further unlocking the awesome power of the \texttt{ggplot2} package.

\begin{table}

\caption{\label{tab:viz-summary-table}Summary of Five Named Graphs}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{rl>{\raggedright\arraybackslash}p{1.1in}>{\raggedright\arraybackslash}p{1.2in}>{\raggedright\arraybackslash}p{1.6in}}
\toprule
  & Named graph & Shows & Geometric object & Notes\\
\midrule
1 & Scatterplot & Relationship between 2 numerical variables & geom\_point() & \\
2 & Linegraph & Relationship between 2 numerical variables & geom\_line() & Used when there is a sequential order to x-variable, e.g., time\\
3 & Histogram & Distribution of 1 numerical variable & geom\_histogram() & Facetted histograms show the distribution of 1 numerical variable split by the values of another variable\\
4 & Boxplot & Distribution of 1 numerical variable split by the values of another variable & geom\_boxplot() & \\
5 & Barplot & Distribution of 1 categorical variable & geom\_bar() when counts are not pre-counted, geom\_col() when counts are pre-counted & Stacked, side-by-side, and faceted barplots show the joint distribution of 2 categorical variables\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{function-argument-specification}{%
\subsection{Function argument specification}\label{function-argument-specification}}

Let's go over some important points about specifying the arguments (i.e., inputs) to functions. Run the following two segments of code:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Segment 1:}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ flights, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ carrier)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{()}

\CommentTok{\# Segment 2:}
\FunctionTok{ggplot}\NormalTok{(flights, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ carrier)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

You'll notice that both code segments create the same barplot, even though in the second segment we omitted the \texttt{data\ =} and \texttt{mapping\ =} code argument names. This is because the \texttt{ggplot()} function by default assumes that the \texttt{data} argument comes first and the \texttt{mapping} argument comes second. \index{functions!argument order} As long as you specify the data frame in question first and the \texttt{aes()} mapping second, you can omit the explicit statement of the argument names \texttt{data\ =} and \texttt{mapping\ =}.

Going forward for the rest of this book, all \texttt{ggplot()} code will be like the second segment: with the \texttt{data\ =} and \texttt{mapping\ =} explicit naming of the argument omitted with the default ordering of arguments respected. We'll do this for brevity's sake; it's common to see this style when reviewing other R users' code.

\hypertarget{additional-resources-1}{%
\subsection{Additional resources}\label{additional-resources-1}}

Solutions to all \emph{Learning checks} can be found online in \href{https://moderndive.com/D-appendixD.html}{Appendix D}.

An R script file of all R code used in this chapter is available at \url{https://www.moderndive.com/scripts/02-visualization.R}.

If you want to further unlock the power of the \texttt{ggplot2} package for data visualization, we suggest that you check out RStudio's ``Data Visualization with ggplot2'' cheatsheet. This cheatsheet summarizes much more than what we've discussed in this chapter. In particular, it presents many more than the 5 \texttt{geom}etric objects we covered in this chapter while providing quick and easy to read visual descriptions. For all the \texttt{geom}etric objects, it also lists all the possible aesthetic attributes one can tweak. In the current version of RStudio in late 2019, you can access this cheatsheet by going to the RStudio Menu Bar -\textgreater{} Help -\textgreater{} Cheatsheets -\textgreater{} ``Data Visualization with ggplot2.''

\hypertarget{whats-to-come-3}{%
\subsection{What's to come}\label{whats-to-come-3}}

Recall in Figure \ref{fig:noalpha} in Section \ref{scatterplots} we visualized the relationship between departure delay and arrival delay for Alaska Airlines flights. This necessitated paring down the \texttt{flights} data frame to a new data frame \texttt{alaska\_flights} consisting of only \texttt{carrier\ ==\ AS} flights first:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alaska\_flights }\OtherTok{\textless{}{-}}\NormalTok{ flights }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(carrier }\SpecialCharTok{==} \StringTok{"AS"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ alaska\_flights, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ dep\_delay, }\AttributeTok{y =}\NormalTok{ arr\_delay)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Furthermore recall in Figure \ref{fig:hourlytemp} in Section \ref{linegraphs} we visualized hourly temperature recordings at Newark airport only for the first 15 days of January 2013. This necessitated paring down the \texttt{weather} data frame to a new data frame \texttt{early\_january\_weather} consisting of hourly temperature recordings only for \texttt{origin\ ==\ "EWR"}, \texttt{month\ ==\ 1}, and day less than or equal to \texttt{15} first:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{early\_january\_weather }\OtherTok{\textless{}{-}}\NormalTok{ nycflights13}\SpecialCharTok{::}\NormalTok{weather }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(origin }\SpecialCharTok{==} \StringTok{"EWR"} \SpecialCharTok{\&}\NormalTok{ month }\SpecialCharTok{==} \DecValTok{1} \SpecialCharTok{\&}\NormalTok{ day }\SpecialCharTok{\textless{}=} \DecValTok{15}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ early\_january\_weather, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ time\_hour, }\AttributeTok{y =}\NormalTok{ temp)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

These two code segments were a preview of Chapter \ref{wrangling} on data wrangling using the \texttt{dplyr} package. Data wrangling is the process of transforming and modifying existing data with the intent of making it more appropriate for analysis purposes. For example, these two code segments used the \texttt{filter()} function to create new data frames (\texttt{alaska\_flights} and \texttt{early\_january\_weather}) by choosing only a subset of rows of existing data frames (\texttt{flights} and \texttt{weather}). In the next chapter, we'll formally introduce the \texttt{filter()} and other data wrangling functions as well as the \emph{pipe operator} \texttt{\%\textgreater{}\%} which allows you to combine multiple data wrangling actions into a single sequential \emph{chain} of actions. On to Chapter \ref{wrangling} on data wrangling!

\hypertarget{wrangling}{%
\chapter{Data Wrangling}\label{wrangling}}

So far in our journey, we've seen how to look at data saved in data frames using the \texttt{glimpse()} and \texttt{View()} functions in Chapter \ref{getting-started}, and how to create data visualizations using the \texttt{ggplot2} package in Chapter \ref{viz}. In particular we studied what we term the ``five named graphs'' (5NG):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  scatterplots via \texttt{geom\_point()}
\item
  linegraphs via \texttt{geom\_line()}
\item
  boxplots via \texttt{geom\_boxplot()}
\item
  histograms via \texttt{geom\_histogram()}
\item
  barplots via \texttt{geom\_bar()} or \texttt{geom\_col()}
\end{enumerate}

We created these visualizations using the grammar of graphics, which maps variables in a data frame to the aesthetic attributes of one of the 5 \texttt{geom}etric objects. We can also control other aesthetic attributes of the geometric objects such as the size and color as seen in the Gapminder data example in Figure \ref{fig:gapminder}.

Recall however that for two of our visualizations, we first needed to transform/modify existing data frames a little. For example, recall the scatterplot in Figure \ref{fig:noalpha} of departure and arrival delays \emph{only} for Alaska Airlines flights. In order to create this visualization, we first needed to pare down the \texttt{flights} data frame to an \texttt{alaska\_flights} data frame consisting of only \texttt{carrier\ ==\ "AS"} flights. Thus, \texttt{alaska\_flights} will have fewer rows than \texttt{flights}. We did this using the \texttt{filter()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alaska\_flights }\OtherTok{\textless{}{-}}\NormalTok{ flights }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(carrier }\SpecialCharTok{==} \StringTok{"AS"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In this chapter, we'll extend this example and we'll introduce a series of functions from the \texttt{dplyr} package for data wrangling that will allow you to take a data frame and ``wrangle'' it (transform it) to suit your needs. Such functions include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{filter()} a data frame's existing rows to only pick out a subset of them. For example, the \texttt{alaska\_flights} data frame.
\item
  \texttt{summarize()} one or more of its columns/variables with a \emph{summary statistic}. Examples of summary statistics include the median and interquartile range of temperatures as we saw in Section \ref{boxplots} on boxplots.
\item
  \texttt{group\_by()} its rows. In other words, assign different rows to be part of the same \emph{group}. We can then combine \texttt{group\_by()} with \texttt{summarize()} to report summary statistics for each group \emph{separately}. For example, say you don't want a single overall average departure delay \texttt{dep\_delay} for all three \texttt{origin} airports combined, but rather three separate average departure delays, one computed for each of the three \texttt{origin} airports.
\item
  \texttt{mutate()} its existing columns/variables to create new ones. For example, convert hourly temperature recordings from degrees Fahrenheit to degrees Celsius.
\item
  \texttt{arrange()} its rows. For example, sort the rows of \texttt{weather} in ascending or descending order of \texttt{temp}.
\item
  \texttt{join()} it with another data frame by matching along a ``key'' variable. In other words, merge these two data frames together.
\end{enumerate}

Notice how we used \texttt{computer\_code} font to describe the actions we want to take on our data frames. This is because the \texttt{dplyr} package for data wrangling has intuitively verb-named functions that are easy to remember.

There is a further benefit to learning to use the \texttt{dplyr} package for data wrangling: its similarity to the database querying language \href{https://en.wikipedia.org/wiki/SQL}{SQL} (pronounced ``sequel'' or spelled out as ``S'', ``Q'', ``L''). SQL (which stands for ``Structured Query Language'') is used to manage large databases quickly and efficiently and is widely used by many institutions with a lot of data. While SQL is a topic left for a book or a course on database management, keep in mind that once you learn \texttt{dplyr}, you can learn SQL easily. We'll talk more about their similarities in Subsection \ref{normal-forms}.

\hypertarget{wrangling-packages}{%
\subsection*{Needed packages}\label{wrangling-packages}}


Let's load all the packages needed for this chapter (this assumes you've already installed them). If needed, read Section \ref{packages} for information on how to install and load R packages.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(nycflights13)}
\end{Highlighting}
\end{Shaded}

\hypertarget{piping}{%
\section{\texorpdfstring{The pipe operator: \texttt{\%\textgreater{}\%}}{The pipe operator: \%\textgreater\%}}\label{piping}}

Before we start data wrangling, let's first introduce a nifty tool that gets loaded with the \texttt{dplyr} package: the \index{operators!pipe} pipe operator \texttt{\%\textgreater{}\%}. The pipe operator allows us to combine multiple operations in R into a single sequential \emph{chain} of actions.

Let's start with a hypothetical example. Say you would like to perform a hypothetical sequence of operations on a hypothetical data frame \texttt{x} using hypothetical functions \texttt{f()}, \texttt{g()}, and \texttt{h()}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Take \texttt{x} \emph{then}
\item
  Use \texttt{x} as an input to a function \texttt{f()} \emph{then}
\item
  Use the output of \texttt{f(x)} as an input to a function \texttt{g()} \emph{then}
\item
  Use the output of \texttt{g(f(x))} as an input to a function \texttt{h()}
\end{enumerate}

One way to achieve this sequence of operations is by using nesting parentheses as follows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{h}\NormalTok{(}\FunctionTok{g}\NormalTok{(}\FunctionTok{f}\NormalTok{(x)))}
\end{Highlighting}
\end{Shaded}

This code isn't so hard to read since we are applying only three functions: \texttt{f()}, then \texttt{g()}, then \texttt{h()} and each of the functions is short in its name. Further, each of these functions also only has one argument. However, you can imagine that this will get progressively harder to read as the number of functions applied in your sequence increases and the arguments in each function increase as well. This is where the pipe operator \texttt{\%\textgreater{}\%} comes in handy. \texttt{\%\textgreater{}\%} takes the output of one function and then ``pipes'' it to be the input of the next function. Furthermore, a helpful trick is to read \texttt{\%\textgreater{}\%} as ``then'' or ``and then.'' For example, you can obtain the same output as the hypothetical sequence of functions as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{f}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{g}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{h}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

You would read this sequence as:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Take \texttt{x} \emph{then}
\item
  Use this output as the input to the next function \texttt{f()} \emph{then}
\item
  Use this output as the input to the next function \texttt{g()} \emph{then}
\item
  Use this output as the input to the next function \texttt{h()}
\end{enumerate}

So while both approaches achieve the same goal, the latter is much more human-readable because you can clearly read the sequence of operations line-by-line. But what are the hypothetical \texttt{x}, \texttt{f()}, \texttt{g()}, and \texttt{h()}? Throughout this chapter on data wrangling:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The starting value \texttt{x} will be a data frame. For example, the \index{R packages!nycflights13} \texttt{flights} data frame we explored in Section \ref{nycflights13}.
\item
  The sequence of functions, here \texttt{f()}, \texttt{g()}, and \texttt{h()}, will mostly be a sequence of any number of the six data wrangling verb-named functions we listed in the introduction to this chapter. For example, the \texttt{filter(carrier\ ==\ "AS")} function and argument specified we previewed earlier.
\item
  The result will be the transformed/modified data frame that you want. In our example, we'll save the result in a new data frame by using the \texttt{\textless{}-} assignment operator with the name \texttt{alaska\_flights} via \texttt{alaska\_flights\ \textless{}-}.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alaska\_flights }\OtherTok{\textless{}{-}}\NormalTok{ flights }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(carrier }\SpecialCharTok{==} \StringTok{"AS"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Much like when adding layers to a \texttt{ggplot()} using the \texttt{+} sign, you form a single \emph{chain} of data wrangling operations by combining verb-named functions into a single sequence using the pipe operator \texttt{\%\textgreater{}\%}. Furthermore, much like how the \texttt{+} sign has to come at the end of lines when constructing plots, the pipe operator \texttt{\%\textgreater{}\%} has to come at the end of lines as well.

Keep in mind, there are many more advanced data wrangling functions than just the six listed in the introduction to this chapter; you'll see some examples of these in Section \ref{other-verbs}. However, just with these six verb-named functions you'll be able to perform a broad array of data wrangling tasks for the rest of this book.

\hypertarget{filter}{%
\section{\texorpdfstring{\texttt{filter} rows}{filter rows}}\label{filter}}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{images/cheatsheets/filter} 

}

\caption{Diagram of filter() rows operation.}\label{fig:filter}
\end{figure}

The \index{dplyr!filter} \texttt{filter()} function here works much like the ``Filter'' option in Microsoft Excel; it allows you to specify criteria about the values of a variable in your dataset and then filters out only the rows that match that criteria.

We begin by focusing only on flights from New York City to Portland, Oregon. The \texttt{dest} destination code (or airport code) for Portland, Oregon is \texttt{"PDX"}. Run the following and look at the results in RStudio's spreadsheet viewer to ensure that only flights heading to Portland are chosen here:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{portland\_flights }\OtherTok{\textless{}{-}}\NormalTok{ flights }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(dest }\SpecialCharTok{==} \StringTok{"PDX"}\NormalTok{)}
\FunctionTok{View}\NormalTok{(portland\_flights)}
\end{Highlighting}
\end{Shaded}

Note the order of the code. First, take the \texttt{flights} data frame \texttt{flights} \emph{then} \texttt{filter()} the data frame so that only those where the \texttt{dest} equals \texttt{"PDX"} are included. We test for equality using the double equal sign \index{operators!==} \texttt{==} and not a single equal sign \texttt{=}. In other words \texttt{filter(dest\ =\ "PDX")} will yield an error. This is a convention across many programming languages. If you are new to coding, you'll probably forget to use the double equal sign \texttt{==} a few times before you get the hang of it.

You can use other operators \index{operators} beyond just the \texttt{==} operator that tests for equality:

\begin{itemize}
\tightlist
\item
  \texttt{\textgreater{}} corresponds to ``greater than''
\item
  \texttt{\textless{}} corresponds to ``less than''
\item
  \texttt{\textgreater{}=} corresponds to ``greater than or equal to''
\item
  \texttt{\textless{}=} corresponds to ``less than or equal to''
\item
  \texttt{!=} corresponds to ``not equal to.'' The \texttt{!} is used in many programming languages to indicate ``not.''
\end{itemize}

Furthermore, you can combine multiple criteria using operators that make comparisons:

\begin{itemize}
\tightlist
\item
  \texttt{\textbar{}} corresponds to ``or''
\item
  \texttt{\&} corresponds to ``and''
\end{itemize}

To see many of these in action, let's filter \texttt{flights} for all rows that departed from JFK \emph{and} were heading to Burlington, Vermont (\texttt{"BTV"}) or Seattle, Washington (\texttt{"SEA"}) \emph{and} departed in the months of October, November, or December. Run the following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{btv\_sea\_flights\_fall }\OtherTok{\textless{}{-}}\NormalTok{ flights }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(origin }\SpecialCharTok{==} \StringTok{"JFK"} \SpecialCharTok{\&}\NormalTok{ (dest }\SpecialCharTok{==} \StringTok{"BTV"} \SpecialCharTok{|}\NormalTok{ dest }\SpecialCharTok{==} \StringTok{"SEA"}\NormalTok{) }\SpecialCharTok{\&}\NormalTok{ month }\SpecialCharTok{\textgreater{}=} \DecValTok{10}\NormalTok{)}
\FunctionTok{View}\NormalTok{(btv\_sea\_flights\_fall)}
\end{Highlighting}
\end{Shaded}

Note that even though colloquially speaking one might say ``all flights leaving Burlington, Vermont \emph{and} Seattle, Washington,'' in terms of computer operations, we really mean ``all flights leaving Burlington, Vermont \emph{or} leaving Seattle, Washington.'' For a given row in the data, \texttt{dest} can be \texttt{"BTV"}, or \texttt{"SEA"}, or something else, but not both \texttt{"BTV"} and \texttt{"SEA"} at the same time. Furthermore, note the careful use of parentheses around \texttt{dest\ ==\ "BTV"\ \textbar{}\ dest\ ==\ "SEA"}.

We can often skip the use of \texttt{\&} and just separate our conditions with a comma. The previous code will return the identical output \texttt{btv\_sea\_flights\_fall} as the following code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{btv\_sea\_flights\_fall }\OtherTok{\textless{}{-}}\NormalTok{ flights }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(origin }\SpecialCharTok{==} \StringTok{"JFK"}\NormalTok{, (dest }\SpecialCharTok{==} \StringTok{"BTV"} \SpecialCharTok{|}\NormalTok{ dest }\SpecialCharTok{==} \StringTok{"SEA"}\NormalTok{), month }\SpecialCharTok{\textgreater{}=} \DecValTok{10}\NormalTok{)}
\FunctionTok{View}\NormalTok{(btv\_sea\_flights\_fall)}
\end{Highlighting}
\end{Shaded}

Let's present another example that uses the \index{operators!not} \texttt{!} ``not'' operator to pick rows that \emph{don't} match a criteria. As mentioned earlier, the \texttt{!} can be read as ``not.'' Here we are filtering rows corresponding to flights that didn't go to Burlington, VT or Seattle, WA.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{not\_BTV\_SEA }\OtherTok{\textless{}{-}}\NormalTok{ flights }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\NormalTok{(dest }\SpecialCharTok{==} \StringTok{"BTV"} \SpecialCharTok{|}\NormalTok{ dest }\SpecialCharTok{==} \StringTok{"SEA"}\NormalTok{))}
\FunctionTok{View}\NormalTok{(not\_BTV\_SEA)}
\end{Highlighting}
\end{Shaded}

Again, note the careful use of parentheses around the \texttt{(dest\ ==\ "BTV"\ \textbar{}\ dest\ ==\ "SEA")}. If we didn't use parentheses as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\NormalTok{dest }\SpecialCharTok{==} \StringTok{"BTV"} \SpecialCharTok{|}\NormalTok{ dest }\SpecialCharTok{==} \StringTok{"SEA"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We would be returning all flights not headed to \texttt{"BTV"} \emph{or} those headed to \texttt{"SEA"}, which is an entirely different resulting data frame.

Now say we have a larger number of airports we want to filter for, say \texttt{"SEA"}, \texttt{"SFO"}, \texttt{"PDX"}, \texttt{"BTV"}, and \texttt{"BDL"}. We could continue to use the \texttt{\textbar{}} (\emph{or}) \index{operators!or} operator:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{many\_airports }\OtherTok{\textless{}{-}}\NormalTok{ flights }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(dest }\SpecialCharTok{==} \StringTok{"SEA"} \SpecialCharTok{|}\NormalTok{ dest }\SpecialCharTok{==} \StringTok{"SFO"} \SpecialCharTok{|}\NormalTok{ dest }\SpecialCharTok{==} \StringTok{"PDX"} \SpecialCharTok{|} 
\NormalTok{         dest }\SpecialCharTok{==} \StringTok{"BTV"} \SpecialCharTok{|}\NormalTok{ dest }\SpecialCharTok{==} \StringTok{"BDL"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

but as we progressively include more airports, this will get unwieldy to write. A slightly shorter approach uses the \texttt{\%in\%} \index{operators!in} operator along with the \texttt{c()} function. Recall from Subsection \ref{programming-concepts} that the \texttt{c()} function ``combines'' or ``concatenates'' values into a single \emph{vector} of values. \index{vectors}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{many\_airports }\OtherTok{\textless{}{-}}\NormalTok{ flights }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(dest }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"SEA"}\NormalTok{, }\StringTok{"SFO"}\NormalTok{, }\StringTok{"PDX"}\NormalTok{, }\StringTok{"BTV"}\NormalTok{, }\StringTok{"BDL"}\NormalTok{))}
\FunctionTok{View}\NormalTok{(many\_airports)}
\end{Highlighting}
\end{Shaded}

What this code is doing is filtering \texttt{flights} for all flights where \texttt{dest} is in the vector of airports \texttt{c("BTV",\ "SEA",\ "PDX",\ "SFO",\ "BDL")}. Both outputs of \texttt{many\_airports} are the same, but as you can see the latter takes much less energy to code. The \texttt{\%in\%} operator is useful for looking for matches commonly in one vector/variable compared to another.

As a final note, we recommend that \texttt{filter()} should often be among the first verbs you consider applying to your data. This cleans your dataset to only those rows you care about, or put differently, it narrows down the scope of your data frame to just the observations you care about.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC3.1)} What's another way of using the ``not'' operator \texttt{!} to filter only the rows that are not going to Burlington, VT nor Seattle, WA in the \texttt{flights} data frame? Test this out using the previous code.

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{summarize}{%
\section{\texorpdfstring{\texttt{summarize} variables}{summarize variables}}\label{summarize}}

The next common task when working with data frames is to compute \emph{summary statistics}. \index{summary statistics}Summary statistics are single numerical values that summarize a large number of values. Commonly known examples of summary statistics include the mean (also called the average) and the median (the middle value). Other examples of summary statistics that might not immediately come to mind include the \emph{sum}, the smallest value also called the \emph{minimum}, the largest value also called the \emph{maximum}, and the \emph{standard deviation}. See Appendix \ref{appendix-stat-terms} for a glossary of such summary statistics.

Let's calculate two summary statistics of the \texttt{temp} temperature variable in the \texttt{weather} data frame: the mean and standard deviation (recall from Section \ref{nycflights13} that the \texttt{weather} data frame is included in the \texttt{nycflights13} package). To compute these summary statistics, we need the \texttt{mean()} and \texttt{sd()} \emph{summary functions} in R. Summary functions in R take in many values and return a single value, as illustrated in Figure \ref{fig:summary-function}.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{images/cheatsheets/summary} 

}

\caption{Diagram illustrating a summary function in R.}\label{fig:summary-function}
\end{figure}

More precisely, we'll use the \texttt{mean()} and \texttt{sd()} summary functions within the \texttt{summarize()} \index{dplyr!summarize()} function from the \texttt{dplyr} package. Note you can also use the British English spelling of \texttt{summarise()}. As shown in Figure \ref{fig:sum1}, the \texttt{summarize()} function takes in a data frame and returns a data frame with only one row corresponding to the summary statistics.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth,height=0.8\textheight]{images/cheatsheets/summarize1} 

}

\caption{Diagram of summarize() rows.}\label{fig:sum1}
\end{figure}

We'll save the results in a new data frame called \texttt{summary\_temp} that will have two columns/variables: the \texttt{mean} and the \texttt{std\_dev}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{summary\_temp }\OtherTok{\textless{}{-}}\NormalTok{ weather }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(temp), }\AttributeTok{std\_dev =} \FunctionTok{sd}\NormalTok{(temp))}
\NormalTok{summary\_temp}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 2
   mean std_dev
  <dbl>   <dbl>
1    NA      NA
\end{verbatim}

Why are the values returned \texttt{NA}? As we saw in Subsection \ref{geompoint} when creating the scatterplot of departure and arrival delays for \texttt{alaska\_flights}, \texttt{NA} is how R encodes \emph{missing values} \index{missing values} where \texttt{NA} indicates ``not available'' or ``not applicable.'' If a value for a particular row and a particular column does not exist, \texttt{NA} is stored instead. Values can be missing for many reasons. Perhaps the data was collected but someone forgot to enter it? Perhaps the data was not collected at all because it was too difficult to do so? Perhaps there was an erroneous value that someone entered that has been corrected to read as missing? You'll often encounter issues with missing values when working with real data.

Going back to our \texttt{summary\_temp} output, by default any time you try to calculate a summary statistic of a variable that has one or more \texttt{NA} missing values in R, \texttt{NA} is returned. To work around this fact, you can set the \texttt{na.rm} argument to \texttt{TRUE}, where \texttt{rm} is short for ``remove''; this will ignore any \texttt{NA} missing values and only return the summary value for all non-missing values.

The code that follows computes the mean and standard deviation of all non-missing values of \texttt{temp}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{summary\_temp }\OtherTok{\textless{}{-}}\NormalTok{ weather }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(temp, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{), }
            \AttributeTok{std\_dev =} \FunctionTok{sd}\NormalTok{(temp, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{))}
\NormalTok{summary\_temp}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 2
     mean std_dev
    <dbl>   <dbl>
1 55.2604 17.7879
\end{verbatim}

Notice how the \texttt{na.rm\ =\ TRUE} \index{functions!na.rm argument} are used as arguments to the \texttt{mean()} \index{mean()} and \texttt{sd()} \index{sd()} summary functions individually, and not to the \texttt{summarize()} function.

However, one needs to be cautious whenever ignoring missing values as we've just done. In the upcoming \emph{Learning checks} questions, we'll consider the possible ramifications of blindly sweeping rows with missing values ``under the rug.'' This is in fact why the \texttt{na.rm} argument to any summary statistic function in R is set to \texttt{FALSE} by default. In other words, R does not ignore rows with missing values by default. R is alerting you to the presence of missing data and you should be mindful of this missingness and any potential causes of this missingness throughout your analysis.

What are other summary functions we can use inside the \texttt{summarize()} verb to compute summary statistics? As seen in the diagram in Figure \ref{fig:summary-function}, you can use any function in R that takes many values and returns just one. Here are just a few:

\begin{itemize}
\tightlist
\item
  \texttt{mean()}: the average
\item
  \texttt{sd()}: the standard deviation, which is a measure of spread
\item
  \texttt{min()} and \texttt{max()}: the minimum and maximum values, respectively
\item
  \texttt{IQR()}: interquartile range
\item
  \texttt{sum()}: the total amount when adding multiple numbers
\item
  \texttt{n()}: a count of the number of rows in each group. This particular summary function will make more sense when \texttt{group\_by()} is covered in Section \ref{groupby}.
\end{itemize}

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC3.2)} Say a doctor is studying the effect of smoking on lung cancer for a large number of patients who have records measured at five-year intervals. She notices that a large number of patients have missing data points because the patient has died, so she chooses to ignore these patients in her analysis. What is wrong with this doctor's approach?

\textbf{(LC3.3)} Modify the earlier \texttt{summarize()} function code that creates the \texttt{summary\_temp} data frame to also use the \texttt{n()} summary function: \texttt{summarize(...\ ,\ count\ =\ n())}. What does the returned value correspond to?

\textbf{(LC3.4)} Why doesn't the following code work? Run the code line-by-line instead of all at once, and then look at the data. In other words, run \texttt{summary\_temp\ \textless{}-\ weather\ \%\textgreater{}\%\ summarize(mean\ =\ mean(temp,\ na.rm\ =\ TRUE))} first.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{summary\_temp }\OtherTok{\textless{}{-}}\NormalTok{ weather }\SpecialCharTok{\%\textgreater{}\%}   
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(temp, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{std\_dev =} \FunctionTok{sd}\NormalTok{(temp, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{groupby}{%
\section{\texorpdfstring{\texttt{group\_by} rows}{group\_by rows}}\label{groupby}}



\begin{figure}

{\centering \includegraphics[width=\textwidth]{images/cheatsheets/group_summary} 

}

\caption{Diagram of group\_by() and summarize().}\label{fig:groupsummarize}
\end{figure}

Say instead of a single mean temperature for the whole year, you would like 12 mean temperatures, one for each of the 12 months separately. In other words, we would like to compute the mean temperature split by month. We can do this by ``grouping'' temperature observations by the values of another variable, in this case by the 12 values of the variable \texttt{month}. Run the following code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{summary\_monthly\_temp }\OtherTok{\textless{}{-}}\NormalTok{ weather }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(month) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(temp, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{), }
            \AttributeTok{std\_dev =} \FunctionTok{sd}\NormalTok{(temp, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{))}
\NormalTok{summary\_monthly\_temp}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 12 x 3
   month    mean  std_dev
   <int>   <dbl>    <dbl>
 1     1 35.6357 10.2246 
 2     2 34.2706  6.98238
 3     3 39.8801  6.24928
 4     4 51.7456  8.78617
 5     5 61.795   9.68164
 6     6 72.184   7.54637
 7     7 80.0662  7.11990
 8     8 74.4685  5.19161
 9     9 67.3713  8.46590
10    10 60.0711  8.84604
11    11 44.9904 10.4438 
12    12 38.4418  9.98243
\end{verbatim}

This code is identical to the previous code that created \texttt{summary\_temp}, but with an extra \texttt{group\_by(month)} added before the \texttt{summarize()}. Grouping the \texttt{weather} dataset by \texttt{month} and then applying the \texttt{summarize()} functions yields a data frame that displays the mean and standard deviation temperature split by the 12 months of the year.

It is important to note that the \index{dplyr!group\_by()} \texttt{group\_by()} function doesn't change data frames by itself. Rather it changes the \emph{meta-data}\index{meta-data}, or data about the data, specifically the grouping structure. It is only after we apply the \texttt{summarize()} function that the data frame changes.

For example, let's consider the \index{ggplot2!diamonds} \texttt{diamonds} data frame included in the \texttt{ggplot2} package. Run this code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diamonds}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 53,940 x 10
      carat cut       color clarity   depth table price     x     y       z
      <dbl> <ord>     <ord> <ord>     <dbl> <dbl> <int> <dbl> <dbl>   <dbl>
 1 0.23     Ideal     E     SI2     61.5       55   326  3.95 3.98  2.43   
 2 0.21     Premium   E     SI1     59.8       61   326  3.89 3.84  2.31   
 3 0.23     Good      E     VS1     56.9       65   327  4.05 4.07  2.31   
 4 0.290000 Premium   I     VS2     62.4       58   334  4.2  4.23  2.63   
 5 0.31     Good      J     SI2     63.3       58   335  4.34 4.350 2.75   
 6 0.24     Very Good J     VVS2    62.8       57   336  3.94 3.96  2.48   
 7 0.24     Very Good I     VVS1    62.3       57   336  3.95 3.98  2.47   
 8 0.26     Very Good H     SI1     61.9       55   337  4.07 4.11  2.53000
 9 0.22     Fair      E     VS2     65.1000    61   337  3.87 3.78  2.49   
10 0.23     Very Good H     VS1     59.4       61   338  4    4.05  2.39   
# ... with 53,930 more rows
\end{verbatim}

Observe that the first line of the output reads \texttt{\#\ A\ tibble:\ 53,940\ x\ 10}. This is an example of meta-data, in this case the number of observations/rows and variables/columns in \texttt{diamonds}. The actual data itself are the subsequent table of values. Now let's pipe the \texttt{diamonds} data frame into \texttt{group\_by(cut)}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diamonds }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(cut)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 53,940 x 10
# Groups:   cut [5]
      carat cut       color clarity   depth table price     x     y       z
      <dbl> <ord>     <ord> <ord>     <dbl> <dbl> <int> <dbl> <dbl>   <dbl>
 1 0.23     Ideal     E     SI2     61.5       55   326  3.95 3.98  2.43   
 2 0.21     Premium   E     SI1     59.8       61   326  3.89 3.84  2.31   
 3 0.23     Good      E     VS1     56.9       65   327  4.05 4.07  2.31   
 4 0.290000 Premium   I     VS2     62.4       58   334  4.2  4.23  2.63   
 5 0.31     Good      J     SI2     63.3       58   335  4.34 4.350 2.75   
 6 0.24     Very Good J     VVS2    62.8       57   336  3.94 3.96  2.48   
 7 0.24     Very Good I     VVS1    62.3       57   336  3.95 3.98  2.47   
 8 0.26     Very Good H     SI1     61.9       55   337  4.07 4.11  2.53000
 9 0.22     Fair      E     VS2     65.1000    61   337  3.87 3.78  2.49   
10 0.23     Very Good H     VS1     59.4       61   338  4    4.05  2.39   
# ... with 53,930 more rows
\end{verbatim}

Observe that now there is additional meta-data: \texttt{\#\ Groups:\ cut\ {[}5{]}} indicating that the grouping structure meta-data has been set based on the 5 possible levels of the categorical variable \texttt{cut}: \texttt{"Fair"}, \texttt{"Good"}, \texttt{"Very\ Good"}, \texttt{"Premium"}, and \texttt{"Ideal"}. On the other hand, observe that the data has not changed: it is still a table of 53,940 \(\times\) 10 values.

Only by combining a \texttt{group\_by()} with another data wrangling operation, in this case \texttt{summarize()}, will the data actually be transformed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diamonds }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(cut) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{avg\_price =} \FunctionTok{mean}\NormalTok{(price))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 5 x 2
  cut       avg_price
  <ord>         <dbl>
1 Fair        4358.76
2 Good        3928.86
3 Very Good   3981.76
4 Premium     4584.26
5 Ideal       3457.54
\end{verbatim}

If you would like to remove this grouping structure meta-data, we can pipe the resulting data frame into the \index{dplyr!ungroup()} \texttt{ungroup()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{diamonds }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(cut) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ungroup}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 53,940 x 10
      carat cut       color clarity   depth table price     x     y       z
      <dbl> <ord>     <ord> <ord>     <dbl> <dbl> <int> <dbl> <dbl>   <dbl>
 1 0.23     Ideal     E     SI2     61.5       55   326  3.95 3.98  2.43   
 2 0.21     Premium   E     SI1     59.8       61   326  3.89 3.84  2.31   
 3 0.23     Good      E     VS1     56.9       65   327  4.05 4.07  2.31   
 4 0.290000 Premium   I     VS2     62.4       58   334  4.2  4.23  2.63   
 5 0.31     Good      J     SI2     63.3       58   335  4.34 4.350 2.75   
 6 0.24     Very Good J     VVS2    62.8       57   336  3.94 3.96  2.48   
 7 0.24     Very Good I     VVS1    62.3       57   336  3.95 3.98  2.47   
 8 0.26     Very Good H     SI1     61.9       55   337  4.07 4.11  2.53000
 9 0.22     Fair      E     VS2     65.1000    61   337  3.87 3.78  2.49   
10 0.23     Very Good H     VS1     59.4       61   338  4    4.05  2.39   
# ... with 53,930 more rows
\end{verbatim}

Observe how the \texttt{\#\ Groups:\ cut\ {[}5{]}} meta-data is no longer present.

Let's now revisit the \texttt{n()} \index{dplyr!n()} counting summary function we briefly introduced previously. Recall that the \texttt{n()} function counts rows. This is opposed to the \texttt{sum()} summary function that returns the sum of a numerical variable. For example, suppose we'd like to count how many flights departed each of the three airports in New York City:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{by\_origin }\OtherTok{\textless{}{-}}\NormalTok{ flights }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(origin) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{count =} \FunctionTok{n}\NormalTok{())}
\NormalTok{by\_origin}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 2
  origin  count
  <chr>   <int>
1 EWR    120835
2 JFK    111279
3 LGA    104662
\end{verbatim}

We see that Newark (\texttt{"EWR"}) had the most flights departing in 2013 followed by \texttt{"JFK"} and lastly by LaGuardia (\texttt{"LGA"}). Note there is a subtle but important difference between \texttt{sum()} and \texttt{n()}; while \texttt{sum()} returns the sum of a numerical variable, \texttt{n()} returns a count of the number of rows/observations.

\hypertarget{grouping-by-more-than-one-variable}{%
\subsection{Grouping by more than one variable}\label{grouping-by-more-than-one-variable}}

You are not limited to grouping by one variable. Say you want to know the number of flights leaving each of the three New York City airports \emph{for each month}. We can also group by a second variable \texttt{month} using \texttt{group\_by(origin,\ month)}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{by\_origin\_monthly }\OtherTok{\textless{}{-}}\NormalTok{ flights }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(origin, month) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{count =} \FunctionTok{n}\NormalTok{())}
\NormalTok{by\_origin\_monthly}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 36 x 3
# Groups:   origin [3]
   origin month count
   <chr>  <int> <int>
 1 EWR        1  9893
 2 EWR        2  9107
 3 EWR        3 10420
 4 EWR        4 10531
 5 EWR        5 10592
 6 EWR        6 10175
 7 EWR        7 10475
 8 EWR        8 10359
 9 EWR        9  9550
10 EWR       10 10104
# ... with 26 more rows
\end{verbatim}

Observe that there are 36 rows to \texttt{by\_origin\_monthly} because there are 12 months for 3 airports (\texttt{EWR}, \texttt{JFK}, and \texttt{LGA}).

Why do we \texttt{group\_by(origin,\ month)} and not \texttt{group\_by(origin)} and then \texttt{group\_by(month)}? Let's investigate:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{by\_origin\_monthly\_incorrect }\OtherTok{\textless{}{-}}\NormalTok{ flights }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(origin) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(month) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{count =} \FunctionTok{n}\NormalTok{())}
\NormalTok{by\_origin\_monthly\_incorrect}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 12 x 2
   month count
   <int> <int>
 1     1 27004
 2     2 24951
 3     3 28834
 4     4 28330
 5     5 28796
 6     6 28243
 7     7 29425
 8     8 29327
 9     9 27574
10    10 28889
11    11 27268
12    12 28135
\end{verbatim}

What happened here is that the second \texttt{group\_by(month)} overwrote the grouping structure meta-data of the earlier \texttt{group\_by(origin)}, so that in the end we are only grouping by \texttt{month}. The lesson here is if you want to \texttt{group\_by()} two or more variables, you should include all the variables at the same time in the same \texttt{group\_by()} adding a comma between the variable names.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC3.5)} Recall from Chapter \ref{viz} when we looked at temperatures by months in NYC. What does the standard deviation column in the \texttt{summary\_monthly\_temp} data frame tell us about temperatures in NYC throughout the year?

\textbf{(LC3.6)} What code would be required to get the mean and standard deviation temperature for each day in 2013 for NYC?

\textbf{(LC3.7)} Recreate \texttt{by\_monthly\_origin}, but instead of grouping via \texttt{group\_by(origin,\ month)}, group variables in a different order \texttt{group\_by(month,\ origin)}. What differs in the resulting dataset?

\textbf{(LC3.8)} How could we identify how many flights left each of the three airports for each \texttt{carrier}?

\textbf{(LC3.9)} How does the \texttt{filter()} operation differ from a \texttt{group\_by()} followed by a \texttt{summarize()}?

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{mutate}{%
\section{\texorpdfstring{\texttt{mutate} existing variables}{mutate existing variables}}\label{mutate}}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth,height=0.8\textheight]{images/cheatsheets/mutate} 

}

\caption{Diagram of mutate() columns.}\label{fig:select}
\end{figure}

Another common transformation of data is to create/compute new variables based on existing ones. For example, say you are more comfortable thinking of temperature in degrees Celsius (°C) instead of degrees Fahrenheit (°F). The formula to convert temperatures from °F to °C is

\[
\text{temp in C} = \frac{\text{temp in F} - 32}{1.8}
\]

We can apply this formula to the \texttt{temp} variable using the \texttt{mutate()} \index{dplyr!mutate()} function from the \texttt{dplyr} package, which takes existing variables and mutates them to create new ones.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{weather }\OtherTok{\textless{}{-}}\NormalTok{ weather }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{temp\_in\_C =}\NormalTok{ (temp }\SpecialCharTok{{-}} \DecValTok{32}\NormalTok{) }\SpecialCharTok{/} \FloatTok{1.8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In this code, we \texttt{mutate()} the \texttt{weather} data frame by creating a new variable \texttt{temp\_in\_C\ =\ (temp\ -\ 32)\ /\ 1.8} and then \emph{overwrite} the original \texttt{weather} data frame. Why did we overwrite the data frame \texttt{weather}, instead of assigning the result to a new data frame like \texttt{weather\_new}? As a rough rule of thumb, as long as you are not losing original information that you might need later, it's acceptable practice to overwrite existing data frames with updated ones, as we did here. On the other hand, why did we not overwrite the variable \texttt{temp}, but instead created a new variable called \texttt{temp\_in\_C}? Because if we did this, we would have erased the original information contained in \texttt{temp} of temperatures in Fahrenheit that may still be valuable to us.

Let's now compute monthly average temperatures in both °F and °C using the \texttt{group\_by()} and \texttt{summarize()} code we saw in Section \ref{groupby}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{summary\_monthly\_temp }\OtherTok{\textless{}{-}}\NormalTok{ weather }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(month) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{mean\_temp\_in\_F =} \FunctionTok{mean}\NormalTok{(temp, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{), }
            \AttributeTok{mean\_temp\_in\_C =} \FunctionTok{mean}\NormalTok{(temp\_in\_C, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{))}
\NormalTok{summary\_monthly\_temp}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 12 x 3
   month mean_temp_in_F mean_temp_in_C
   <int>          <dbl>          <dbl>
 1     1        35.6357        2.01981
 2     2        34.2706        1.26144
 3     3        39.8801        4.37782
 4     4        51.7456       10.9698 
 5     5        61.795        16.5528 
 6     6        72.184        22.3244 
 7     7        80.0662       26.7035 
 8     8        74.4685       23.5936 
 9     9        67.3713       19.6507 
10    10        60.0711       15.5951 
11    11        44.9904        7.21691
12    12        38.4418        3.57878
\end{verbatim}

Let's consider another example. Passengers are often frustrated when their flight departs late, but aren't as annoyed if, in the end, pilots can make up some time during the flight. This is known in the airline industry as \emph{gain}, and we will create this variable using the \texttt{mutate()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights }\OtherTok{\textless{}{-}}\NormalTok{ flights }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{gain =}\NormalTok{ dep\_delay }\SpecialCharTok{{-}}\NormalTok{ arr\_delay)}
\end{Highlighting}
\end{Shaded}

Let's take a look at only the \texttt{dep\_delay}, \texttt{arr\_delay}, and the resulting \texttt{gain} variables for the first 5 rows in our updated \texttt{flights} data frame in Table \ref{tab:first-five-flights}.

\begin{table}[!h]

\caption{\label{tab:first-five-flights}First five rows of departure/arrival delay and gain variables}
\centering
\begin{tabular}[t]{r|r|r}
\hline
dep\_delay & arr\_delay & gain\\
\hline
2 & 11 & -9\\
\hline
4 & 20 & -16\\
\hline
2 & 33 & -31\\
\hline
-1 & -18 & 17\\
\hline
-6 & -25 & 19\\
\hline
\end{tabular}
\end{table}

The flight in the first row departed 2 minutes late but arrived 11 minutes late, so its ``gained time in the air'' is a loss of 9 minutes, hence its \texttt{gain} is 2 - 11 = -9. On the other hand, the flight in the fourth row departed a minute early (\texttt{dep\_delay} of -1) but arrived 18 minutes early (\texttt{arr\_delay} of -18), so its ``gained time in the air'' is \(-1 - (-18) = -1 + 18 = 17\) minutes, hence its \texttt{gain} is +17.

Let's look at some summary statistics of the \texttt{gain} variable by considering multiple summary functions at once in the same \texttt{summarize()} code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gain\_summary }\OtherTok{\textless{}{-}}\NormalTok{ flights }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}
    \AttributeTok{min =} \FunctionTok{min}\NormalTok{(gain, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q1 =} \FunctionTok{quantile}\NormalTok{(gain, }\FloatTok{0.25}\NormalTok{, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{median =} \FunctionTok{quantile}\NormalTok{(gain, }\FloatTok{0.5}\NormalTok{, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q3 =} \FunctionTok{quantile}\NormalTok{(gain, }\FloatTok{0.75}\NormalTok{, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{max =} \FunctionTok{max}\NormalTok{(gain, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(gain, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{sd =} \FunctionTok{sd}\NormalTok{(gain, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{missing =} \FunctionTok{sum}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(gain))}
\NormalTok{  )}
\NormalTok{gain\_summary}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 8
    min    q1 median    q3   max    mean      sd missing
  <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>   <dbl>   <int>
1  -196    -3      7    17   109 5.65978 18.0436    9430
\end{verbatim}

We see for example that the average gain is +5 minutes, while the largest is +109 minutes! However, this code would take some time to type out in practice. We'll see later on in Subsection \ref{model1EDA} that there is a much more succinct way to compute a variety of common summary statistics: using the \texttt{skim()} function from the \texttt{skimr} package.

Recall from Section \ref{histograms} that since \texttt{gain} is a numerical variable, we can visualize its distribution using a histogram.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ flights, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gain)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{color =} \StringTok{"white"}\NormalTok{, }\AttributeTok{bins =} \DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/gain-hist-1} 

}

\caption{Histogram of gain variable.}\label{fig:gain-hist}
\end{figure}

The resulting histogram in Figure \ref{fig:gain-hist} provides a different perspective on the \texttt{gain} variable than the summary statistics we computed earlier. For example, note that most values of \texttt{gain} are right around 0.

To close out our discussion on the \texttt{mutate()} function to create new variables, note that we can create multiple new variables at once in the same \texttt{mutate()} code. Furthermore, within the same \texttt{mutate()} code we can refer to new variables we just created. As an example, consider the \texttt{mutate()} code Hadley Wickham \index{Wickham, Hadley} and Garrett Grolemund \index{Grolemund, Garrett} show in Chapter 5 of \emph{R for Data Science} \citep{rds2016}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights }\OtherTok{\textless{}{-}}\NormalTok{ flights }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{gain =}\NormalTok{ dep\_delay }\SpecialCharTok{{-}}\NormalTok{ arr\_delay,}
    \AttributeTok{hours =}\NormalTok{ air\_time }\SpecialCharTok{/} \DecValTok{60}\NormalTok{,}
    \AttributeTok{gain\_per\_hour =}\NormalTok{ gain }\SpecialCharTok{/}\NormalTok{ hours}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC3.10)} What do positive values of the \texttt{gain} variable in \texttt{flights} correspond to? What about negative values? And what about a zero value?

\textbf{(LC3.11)} Could we create the \texttt{dep\_delay} and \texttt{arr\_delay} columns by simply subtracting \texttt{dep\_time} from \texttt{sched\_dep\_time} and similarly for arrivals? Try the code out and explain any differences between the result and what actually appears in \texttt{flights}.

\textbf{(LC3.12)} What can we say about the distribution of \texttt{gain}? Describe it in a few sentences using the plot and the \texttt{gain\_summary} data frame values.

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{arrange}{%
\section{\texorpdfstring{\texttt{arrange} and sort rows}{arrange and sort rows}}\label{arrange}}

One of the most commonly performed data wrangling tasks is to sort a data frame's rows in the alphanumeric order of one of the variables. The \texttt{dplyr} package's \texttt{arrange()} function \index{dplyr!arrange()} allows us to sort/reorder a data frame's rows according to the values of the specified variable.

Suppose we are interested in determining the most frequent destination airports for all domestic flights departing from New York City in 2013:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{freq\_dest }\OtherTok{\textless{}{-}}\NormalTok{ flights }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(dest) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{num\_flights =} \FunctionTok{n}\NormalTok{())}
\NormalTok{freq\_dest}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 105 x 2
   dest  num_flights
   <chr>       <int>
 1 ABQ           254
 2 ACK           265
 3 ALB           439
 4 ANC             8
 5 ATL         17215
 6 AUS          2439
 7 AVL           275
 8 BDL           443
 9 BGR           375
10 BHM           297
# ... with 95 more rows
\end{verbatim}

Observe that by default the rows of the resulting \texttt{freq\_dest} data frame are sorted in alphabetical order of \texttt{dest}ination. Say instead we would like to see the same data, but sorted from the most to the least number of flights (\texttt{num\_flights}) instead:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{freq\_dest }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(num\_flights)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 105 x 2
   dest  num_flights
   <chr>       <int>
 1 LEX             1
 2 LGA             1
 3 ANC             8
 4 SBN            10
 5 HDN            15
 6 MTJ            15
 7 EYW            17
 8 PSP            19
 9 JAC            25
10 BZN            36
# ... with 95 more rows
\end{verbatim}

This is, however, the opposite of what we want. The rows are sorted with the least frequent destination airports displayed first. This is because \texttt{arrange()} always returns rows sorted in ascending order by default. To switch the ordering to be in ``descending'' order instead, we use the \texttt{desc()} \index{dplyr!desc()} function as so:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{freq\_dest }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(num\_flights))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 105 x 2
   dest  num_flights
   <chr>       <int>
 1 ORD         17283
 2 ATL         17215
 3 LAX         16174
 4 BOS         15508
 5 MCO         14082
 6 CLT         14064
 7 SFO         13331
 8 FLL         12055
 9 MIA         11728
10 DCA          9705
# ... with 95 more rows
\end{verbatim}

\hypertarget{joins}{%
\section{\texorpdfstring{\texttt{join} data frames}{join data frames}}\label{joins}}

Another common data transformation task is ``joining'' or ``merging'' two different datasets. For example, in the \texttt{flights} data frame, the variable \texttt{carrier} lists the carrier code for the different flights. While the corresponding airline names for \texttt{"UA"} and \texttt{"AA"} might be somewhat easy to guess (United and American Airlines), what airlines have codes \texttt{"VX"}, \texttt{"HA"}, and \texttt{"B6"}? This information is provided in a separate data frame \texttt{airlines}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{View}\NormalTok{(airlines)}
\end{Highlighting}
\end{Shaded}

We see that in \texttt{airlines}, \texttt{carrier} is the carrier code, while \texttt{name} is the full name of the airline company. Using this table, we can see that \texttt{"VX"}, \texttt{"HA"}, and \texttt{"B6"} correspond to Virgin America, Hawaiian Airlines, and JetBlue, respectively. However, wouldn't it be nice to have all this information in a single data frame instead of two separate data frames? We can do this by ``joining'' the \texttt{flights} and \texttt{airlines} data frames.

Note that the values in the variable \texttt{carrier} in the \texttt{flights} data frame match the values in the variable \texttt{carrier} in the \texttt{airlines} data frame. In this case, we can use the variable \texttt{carrier} as a \index{joining data!key variable} \emph{key variable} to match the rows of the two data frames. Key variables are almost always \emph{identification variables} that uniquely identify the observational units as we saw in Subsection \ref{identification-vs-measurement-variables}. This ensures that rows in both data frames are appropriately matched during the join. Hadley and Garrett \citep{rds2016} created the diagram shown in Figure \ref{fig:reldiagram} to help us understand how the different data frames in the \texttt{nycflights13} package are linked by various key variables:



\begin{figure}

{\centering \includegraphics[width=\textwidth,height=1.2\textheight]{images/r4ds/relational-nycflights} 

}

\caption{Data relationships in nycflights13 from \emph{R for Data Science}.}\label{fig:reldiagram}
\end{figure}

\hypertarget{matching-key-variable-names}{%
\subsection{Matching ``key'' variable names}\label{matching-key-variable-names}}

In both the \texttt{flights} and \texttt{airlines} data frames, the key variable we want to join/merge/match the rows by has the same name: \texttt{carrier}. Let's use the \texttt{inner\_join()} \index{dplyr!inner\_join()} function to join the two data frames, where the rows will be matched by the variable \texttt{carrier}, and then compare the resulting data frames:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights\_joined }\OtherTok{\textless{}{-}}\NormalTok{ flights }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{inner\_join}\NormalTok{(airlines, }\AttributeTok{by =} \StringTok{"carrier"}\NormalTok{)}
\FunctionTok{View}\NormalTok{(flights)}
\FunctionTok{View}\NormalTok{(flights\_joined)}
\end{Highlighting}
\end{Shaded}

Observe that the \texttt{flights} and \texttt{flights\_joined} data frames are identical except that \texttt{flights\_joined} has an additional variable \texttt{name}. The values of \texttt{name} correspond to the airline companies' names as indicated in the \texttt{airlines} data frame.

A visual representation of the \texttt{inner\_join()} is shown in Figure \ref{fig:ijdiagram} \citep{rds2016}. There are other types of joins available (such as \texttt{left\_join()}, \texttt{right\_join()}, \texttt{outer\_join()}, and \texttt{anti\_join()}), but the \texttt{inner\_join()} will solve nearly all of the problems you'll encounter in this book.



\begin{figure}

{\centering \includegraphics[width=\textwidth,height=1.2\textheight]{images/r4ds/join-inner} 

}

\caption{Diagram of inner join from \emph{R for Data Science}.}\label{fig:ijdiagram}
\end{figure}

\hypertarget{diff-key}{%
\subsection{Different ``key'' variable names}\label{diff-key}}

Say instead you are interested in the destinations of all domestic flights departing NYC in 2013, and you ask yourself questions like: ``What cities are these airports in?'', or ``Is \texttt{"ORD"} Orlando?'', or ``Where is \texttt{"FLL"}?''.

The \texttt{airports} data frame contains the airport codes for each airport:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{View}\NormalTok{(airports)}
\end{Highlighting}
\end{Shaded}

However, if you look at both the \texttt{airports} and \texttt{flights} data frames, you'll find that the airport codes are in variables that have different names. In \texttt{airports} the airport code is in \texttt{faa}, whereas in \texttt{flights} the airport codes are in \texttt{origin} and \texttt{dest}. This fact is further highlighted in the visual representation of the relationships between these data frames in Figure \ref{fig:reldiagram}.

In order to join these two data frames by airport code, our \texttt{inner\_join()} operation will use the \texttt{by\ =\ c("dest"\ =\ "faa")} \index{dplyr!inner\_join()!by} argument with modified code syntax allowing us to join two data frames where the key variable has a different name:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights\_with\_airport\_names }\OtherTok{\textless{}{-}}\NormalTok{ flights }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{inner\_join}\NormalTok{(airports, }\AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"dest"} \OtherTok{=} \StringTok{"faa"}\NormalTok{))}
\FunctionTok{View}\NormalTok{(flights\_with\_airport\_names)}
\end{Highlighting}
\end{Shaded}

Let's construct the chain of pipe operators \texttt{\%\textgreater{}\%} that computes the number of flights from NYC to each destination, but also includes information about each destination airport:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{named\_dests }\OtherTok{\textless{}{-}}\NormalTok{ flights }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(dest) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{num\_flights =} \FunctionTok{n}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(num\_flights)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{inner\_join}\NormalTok{(airports, }\AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"dest"} \OtherTok{=} \StringTok{"faa"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{airport\_name =}\NormalTok{ name)}
\NormalTok{named\_dests}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 101 x 9
   dest  num_flights airport_name     lat       lon   alt    tz dst   tzone
   <chr>       <int> <chr>          <dbl>     <dbl> <dbl> <dbl> <chr> <chr>
 1 ORD         17283 Chicago Oha~ 41.9786  -87.9048   668    -6 A     Amer~
 2 ATL         17215 Hartsfield ~ 33.6367  -84.4281  1026    -5 A     Amer~
 3 LAX         16174 Los Angeles~ 33.9425 -118.408    126    -8 A     Amer~
 4 BOS         15508 General Edw~ 42.3643  -71.0052    19    -5 A     Amer~
 5 MCO         14082 Orlando Intl 28.4294  -81.3090    96    -5 A     Amer~
 6 CLT         14064 Charlotte D~ 35.214   -80.9431   748    -5 A     Amer~
 7 SFO         13331 San Francis~ 37.6190 -122.375     13    -8 A     Amer~
 8 FLL         12055 Fort Lauder~ 26.0726  -80.1528     9    -5 A     Amer~
 9 MIA         11728 Miami Intl   25.7932  -80.2906     8    -5 A     Amer~
10 DCA          9705 Ronald Reag~ 38.8521  -77.0377    15    -5 A     Amer~
# ... with 91 more rows
\end{verbatim}

In case you didn't know, \texttt{"ORD"} is the airport code of Chicago O'Hare airport and \texttt{"FLL"} is the main airport in Fort Lauderdale, Florida, which can be seen in the \texttt{airport\_name} variable.

\hypertarget{multiple-key-variables}{%
\subsection{Multiple ``key'' variables}\label{multiple-key-variables}}

Say instead we want to join two data frames by \emph{multiple key variables}. For example, in Figure \ref{fig:reldiagram}, we see that in order to join the \texttt{flights} and \texttt{weather} data frames, we need more than one key variable: \texttt{year}, \texttt{month}, \texttt{day}, \texttt{hour}, and \texttt{origin}. This is because the combination of these 5 variables act to uniquely identify each observational unit in the \texttt{weather} data frame: hourly weather recordings at each of the 3 NYC airports.

We achieve this by specifying a \emph{vector} of key variables to join by using the \texttt{c()} function. Recall from Subsection \ref{programming-concepts} that \texttt{c()} is short for ``combine'' or ``concatenate.'' \index{vectors}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights\_weather\_joined }\OtherTok{\textless{}{-}}\NormalTok{ flights }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{inner\_join}\NormalTok{(weather, }\AttributeTok{by =} \FunctionTok{c}\NormalTok{(}\StringTok{"year"}\NormalTok{, }\StringTok{"month"}\NormalTok{, }\StringTok{"day"}\NormalTok{, }\StringTok{"hour"}\NormalTok{, }\StringTok{"origin"}\NormalTok{))}
\FunctionTok{View}\NormalTok{(flights\_weather\_joined)}
\end{Highlighting}
\end{Shaded}

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC3.13)} Looking at Figure \ref{fig:reldiagram}, when joining \texttt{flights} and \texttt{weather} (or, in other words, matching the hourly weather values with each flight), why do we need to join by all of \texttt{year}, \texttt{month}, \texttt{day}, \texttt{hour}, and \texttt{origin}, and not just \texttt{hour}?

\textbf{(LC3.14)} What surprises you about the top 10 destinations from NYC in 2013?

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{normal-forms}{%
\subsection{Normal forms}\label{normal-forms}}

The data frames included in the \texttt{nycflights13} package are in a form that minimizes redundancy of data. For example, the \texttt{flights} data frame only saves the \texttt{carrier} code of the airline company; it does not include the actual name of the airline. For example, the first row of \texttt{flights} has \texttt{carrier} equal to \texttt{UA}, but it does not include the airline name of ``United Air Lines Inc.''

The names of the airline companies are included in the \texttt{name} variable of the \texttt{airlines} data frame. In order to have the airline company name included in \texttt{flights}, we could join these two data frames as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{joined\_flights }\OtherTok{\textless{}{-}}\NormalTok{ flights }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{inner\_join}\NormalTok{(airlines, }\AttributeTok{by =} \StringTok{"carrier"}\NormalTok{)}
\FunctionTok{View}\NormalTok{(joined\_flights)}
\end{Highlighting}
\end{Shaded}

We are capable of performing this join because each of the data frames have \emph{keys} in common to relate one to another: the \texttt{carrier} variable in both the \texttt{flights} and \texttt{airlines} data frames. The \emph{key} variable(s) that we base our joins on are often \emph{identification variables} as we mentioned previously.

This is an important property of what's known as \emph{normal forms} of data. The process of decomposing data frames into less redundant tables without losing information is called \emph{normalization}. More information is available on \href{https://en.wikipedia.org/wiki/Database_normalization}{Wikipedia}.

Both \texttt{dplyr} and \href{https://en.wikipedia.org/wiki/SQL}{SQL} we mentioned in the introduction of this chapter use such \emph{normal forms}. Given that they share such commonalities, once you learn either of these two tools, you can learn the other very easily.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC3.15)} What are some advantages of data in normal forms? What are some disadvantages?

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{other-verbs}{%
\section{Other verbs}\label{other-verbs}}

Here are some other useful data wrangling verbs:

\begin{itemize}
\tightlist
\item
  \texttt{select()} only a subset of variables/columns.
\item
  \texttt{rename()} variables/columns to have new names.
\item
  Return only the \texttt{top\_n()} values of a variable.
\end{itemize}

\hypertarget{select}{%
\subsection{\texorpdfstring{\texttt{select} variables}{select variables}}\label{select}}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{images/cheatsheets/select} 

}

\caption{Diagram of select() columns.}\label{fig:selectfig}
\end{figure}

We've seen that the \texttt{flights} data frame in the \texttt{nycflights13} package contains 19 different variables. You can identify the names of these 19 variables by running the \texttt{glimpse()} function from the \texttt{dplyr} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(flights)}
\end{Highlighting}
\end{Shaded}

However, say you only need two of these 19 variables, say \texttt{carrier} and \texttt{flight}. You can \texttt{select()} \index{dplyr!select()} these two variables:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(carrier, flight)}
\end{Highlighting}
\end{Shaded}

This function makes it easier to explore large datasets since it allows us to limit the scope to only those variables we care most about. For example, if we \texttt{select()} only a smaller number of variables as is shown in Figure \ref{fig:selectfig}, it will make viewing the dataset in RStudio's spreadsheet viewer more digestible.

Let's say instead you want to drop, or de-select, certain variables. For example, consider the variable \texttt{year} in the \texttt{flights} data frame. This variable isn't quite a ``variable'' because it is always \texttt{2013} and hence doesn't change. Say you want to remove this variable from the data frame. We can deselect \texttt{year} by using the \texttt{-} sign:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights\_no\_year }\OtherTok{\textless{}{-}}\NormalTok{ flights }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{year)}
\end{Highlighting}
\end{Shaded}

Another way of selecting columns/variables is by specifying a range of columns:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flight\_arr\_times }\OtherTok{\textless{}{-}}\NormalTok{ flights }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(month}\SpecialCharTok{:}\NormalTok{day, arr\_time}\SpecialCharTok{:}\NormalTok{sched\_arr\_time)}
\NormalTok{flight\_arr\_times}
\end{Highlighting}
\end{Shaded}

This will \texttt{select()} all columns between \texttt{month} and \texttt{day}, as well as between \texttt{arr\_time} and \texttt{sched\_arr\_time}, and drop the rest.

The \texttt{select()} function can also be used to reorder columns when used with the \texttt{everything()} helper function. For example, suppose we want the \texttt{hour}, \texttt{minute}, and \texttt{time\_hour} variables to appear immediately after the \texttt{year}, \texttt{month}, and \texttt{day} variables, while not discarding the rest of the variables. In the following code, \texttt{everything()} will pick up all remaining variables:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights\_reorder }\OtherTok{\textless{}{-}}\NormalTok{ flights }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(year, month, day, hour, minute, time\_hour, }\FunctionTok{everything}\NormalTok{())}
\FunctionTok{glimpse}\NormalTok{(flights\_reorder)}
\end{Highlighting}
\end{Shaded}

Lastly, the helper functions \texttt{starts\_with()}, \texttt{ends\_with()}, and \texttt{contains()} can be used to select variables/columns that match those conditions. As examples,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(}\FunctionTok{starts\_with}\NormalTok{(}\StringTok{"a"}\NormalTok{))}
\NormalTok{flights }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(}\FunctionTok{ends\_with}\NormalTok{(}\StringTok{"delay"}\NormalTok{))}
\NormalTok{flights }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(}\FunctionTok{contains}\NormalTok{(}\StringTok{"time"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{rename}{%
\subsection{\texorpdfstring{\texttt{rename} variables}{rename variables}}\label{rename}}

Another useful function is \index{dplyr!rename()} \texttt{rename()}, which as you may have guessed changes the name of variables. Suppose we want to only focus on \texttt{dep\_time} and \texttt{arr\_time} and change \texttt{dep\_time} and \texttt{arr\_time} to be \texttt{departure\_time} and \texttt{arrival\_time} instead in the \texttt{flights\_time} data frame:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights\_time\_new }\OtherTok{\textless{}{-}}\NormalTok{ flights }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(dep\_time, arr\_time) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{departure\_time =}\NormalTok{ dep\_time, }\AttributeTok{arrival\_time =}\NormalTok{ arr\_time)}
\FunctionTok{glimpse}\NormalTok{(flights\_time\_new)}
\end{Highlighting}
\end{Shaded}

Note that in this case we used a single \texttt{=} sign within the \texttt{rename()}. For example, \texttt{departure\_time\ =\ dep\_time} renames the \texttt{dep\_time} variable to have the new name \texttt{departure\_time}. This is because we are not testing for equality like we would using \texttt{==}. Instead we want to assign a new variable \texttt{departure\_time} to have the same values as \texttt{dep\_time} and then delete the variable \texttt{dep\_time}. Note that new \texttt{dplyr} users often forget that the new variable name comes before the equal sign.

\hypertarget{top_n-values-of-a-variable}{%
\subsection{\texorpdfstring{\texttt{top\_n} values of a variable}{top\_n values of a variable}}\label{top_n-values-of-a-variable}}

We can also return the top \texttt{n} values of a variable using the \texttt{top\_n()} \index{dplyr!top\_n()} function. For example, we can return a data frame of the top 10 destination airports using the example from Subsection \ref{diff-key}. Observe that we set the number of values to return to \texttt{n\ =\ 10} and \texttt{wt\ =\ num\_flights} to indicate that we want the rows corresponding to the top 10 values of \texttt{num\_flights}. See the help file for \texttt{top\_n()} by running \texttt{?top\_n} for more information.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{named\_dests }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{top\_n}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{, }\AttributeTok{wt =}\NormalTok{ num\_flights)}
\end{Highlighting}
\end{Shaded}

Let's further \texttt{arrange()} these results in descending order of \texttt{num\_flights}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{named\_dests  }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{top\_n}\NormalTok{(}\AttributeTok{n =} \DecValTok{10}\NormalTok{, }\AttributeTok{wt =}\NormalTok{ num\_flights) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(num\_flights))}
\end{Highlighting}
\end{Shaded}

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC3.16)} What are some ways to select all three of the \texttt{dest}, \texttt{air\_time}, and \texttt{distance} variables from \texttt{flights}? Give the code showing how to do this in at least three different ways.

\textbf{(LC3.17)} How could one use \texttt{starts\_with()}, \texttt{ends\_with()}, and \texttt{contains()} to select columns from the \texttt{flights} data frame? Provide three different examples in total: one for \texttt{starts\_with()}, one for \texttt{ends\_with()}, and one for \texttt{contains()}.

\textbf{(LC3.18)} Why might we want to use the \texttt{select} function on a data frame?

\textbf{(LC3.19)} Create a new data frame that shows the top 5 airports with the largest arrival delays from NYC in 2013.

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{wrangling-conclusion}{%
\section{Conclusion}\label{wrangling-conclusion}}

\hypertarget{summary-table-1}{%
\subsection{Summary table}\label{summary-table-1}}

Let's recap our data wrangling verbs in Table \ref{tab:wrangle-summary-table}. Using these verbs and the pipe \texttt{\%\textgreater{}\%} operator from Section \ref{piping}, you'll be able to write easily legible code to perform almost all the data wrangling and data transformation necessary for the rest of this book.

\begin{table}[!h]

\caption{\label{tab:wrangle-summary-table}Summary of data wrangling verbs}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{>{\raggedright\arraybackslash}p{0.9in}>{\raggedright\arraybackslash}p{4in}}
\toprule
Verb & Data wrangling operation\\
\midrule
filter() & Pick out a subset of rows\\
summarize() & Summarize many values to one using a summary statistic function like mean(), median(), etc.\\
group\_by() & Add grouping structure to rows in data frame. Note this does not change values in data frame, rather only the meta-data\\
mutate() & Create new variables by mutating existing ones\\
arrange() & Arrange rows of a data variable in ascending (default) or descending order\\
inner\_join() & Join/merge two data frames, matching rows by a key variable\\
\bottomrule
\end{tabular}
\end{table}

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC3.20)} Let's now put your newly acquired data wrangling skills to the test!

An airline industry measure of a passenger airline's capacity is the \href{https://en.wikipedia.org/wiki/Available_seat_miles}{available seat miles}, which is equal to the number of seats available multiplied by the number of miles or kilometers flown summed over all flights.

For example, let's consider the scenario in Figure \ref{fig:available-seat-miles}. Since the airplane has 4 seats and it travels 200 miles, the available seat miles are \(4 \times 200 = 800\).

\begin{figure}

{\centering \includegraphics[width=\textwidth,height=0.4\textheight]{images/flowcharts/flowchart/flowchart.012} 

}

\caption{Example of available seat miles for one flight.}\label{fig:available-seat-miles}
\end{figure}

Extending this idea, let's say an airline had 2 flights using a plane with 10 seats that flew 500 miles and 3 flights using a plane with 20 seats that flew 1000 miles, the available seat miles would be \(2 \times 10 \times 500 + 3 \times 20 \times 1000 = 70,000\) seat miles.

Using the datasets included in the \texttt{nycflights13} package, compute the available seat miles for each airline sorted in descending order. After completing all the necessary data wrangling steps, the resulting data frame should have 16 rows (one for each airline) and 2 columns (airline name and available seat miles). Here are some hints:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Crucial}: Unless you are very confident in what you are doing, it is worthwhile not starting to code right away. Rather, first sketch out on paper all the necessary data wrangling steps not using exact code, but rather high-level \emph{pseudocode} that is informal yet detailed enough to articulate what you are doing. This way you won't confuse \emph{what} you are trying to do (the algorithm) with \emph{how} you are going to do it (writing \texttt{dplyr} code).
\item
  Take a close look at all the datasets using the \texttt{View()} function: \texttt{flights}, \texttt{weather}, \texttt{planes}, \texttt{airports}, and \texttt{airlines} to identify which variables are necessary to compute available seat miles.
\item
  Figure \ref{fig:reldiagram} showing how the various datasets can be joined will also be useful.
\item
  Consider the data wrangling verbs in Table \ref{tab:wrangle-summary-table} as your toolbox!
\end{enumerate}

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{additional-resources-2}{%
\subsection{Additional resources}\label{additional-resources-2}}

Solutions to all \emph{Learning checks} can be found online in \href{https://moderndive.com/D-appendixD.html}{Appendix D}.

An R script file of all R code used in this chapter is available at \url{https://www.moderndive.com/scripts/03-wrangling.R}.

If you want to further unlock the power of the \texttt{dplyr} package for data wrangling, we suggest that you check out RStudio's ``Data Transformation with dplyr'' cheatsheet. This cheatsheet summarizes much more than what we've discussed in this chapter, in particular more intermediate level and advanced data wrangling functions, while providing quick and easy-to-read visual descriptions. In fact, many of the diagrams illustrating data wrangling operations in this chapter, such as Figure \ref{fig:filter} on \texttt{filter()}, originate from this cheatsheet.

In the current version of RStudio in late 2019, you can access this cheatsheet by going to the RStudio Menu Bar -\textgreater{} Help -\textgreater{} Cheatsheets -\textgreater{} ``Data Transformation with dplyr.''

On top of the data wrangling verbs and examples we presented in this section, if you'd like to see more examples of using the \texttt{dplyr} package for data wrangling, check out \href{http://r4ds.had.co.nz/transform.html}{Chapter 5} of \emph{R for Data Science} \citep{rds2016}.

\hypertarget{whats-to-come-1}{%
\subsection{What's to come?}\label{whats-to-come-1}}

So far in this book, we've explored, visualized, and wrangled data saved in data frames. These data frames were saved in a spreadsheet-like format: in a rectangular shape with a certain number of rows corresponding to observations and a certain number of columns corresponding to variables describing these observations.

We'll see in the upcoming Chapter \ref{tidy} that there are actually two ways to represent data in spreadsheet-type rectangular format: (1) ``wide'' format and (2) ``tall/narrow'' format. The tall/narrow format is also known as \emph{``tidy''} format in R user circles. While the distinction between ``tidy'' and non-``tidy'' formatted data is subtle, it has immense implications for our data science work. This is because almost all the packages used in this book, including the \texttt{ggplot2} package for data visualization and the \texttt{dplyr} package for data wrangling, all assume that all data frames are in ``tidy'' format.

Furthermore, up until now we've only explored, visualized, and wrangled data saved within R packages. But what if you want to analyze data that you have saved in a Microsoft Excel, a Google Sheets, or a ``Comma-Separated Values'' (CSV) file? In Section \ref{csv}, we'll show you how to import this data into R using the \texttt{readr} package.

\hypertarget{tidy}{%
\chapter{Data Importing and ``Tidy'' Data}\label{tidy}}

In Subsection \ref{programming-concepts}, we introduced the concept of a \index{data frame} data frame in R: a rectangular spreadsheet-like representation of data where the rows correspond to observations and the columns correspond to variables describing each observation. In Section \ref{nycflights13}, we started exploring our first data frame: the \texttt{flights} data frame included in the \texttt{nycflights13} package. In Chapter \ref{viz}, we created visualizations based on the data included in \texttt{flights} and other data frames such as \texttt{weather}. In Chapter \ref{wrangling}, we learned how to take existing data frames and transform/modify them to suit our ends.

In this final chapter of the ``Data Science with \texttt{tidyverse}'' portion of the book, we extend some of these ideas by discussing a type of data formatting called ``tidy'' data. You will see that having data stored in ``tidy'' format is about more than just what the everyday definition of the term ``tidy'' might suggest: having your data ``neatly organized.'' Instead, we define the term ``tidy'' as it's used by data scientists who use R, outlining a set of rules by which data is saved.

Knowledge of this type of data formatting was not necessary for our treatment of data visualization in Chapter \ref{viz} and data wrangling in Chapter \ref{wrangling}. This is because all the data used were already in ``tidy'' format. In this chapter, we'll now see that this format is essential to using the tools we covered up until now. Furthermore, it will also be useful for all subsequent chapters in this book when we cover regression and statistical inference. First, however, we'll show you how to import spreadsheet data in R.

\hypertarget{tidy-packages}{%
\subsection*{Needed packages}\label{tidy-packages}}


Let's load all the packages needed for this chapter (this assumes you've already installed them). If needed, read Section \ref{packages} for information on how to install and load R packages.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(readr)}
\FunctionTok{library}\NormalTok{(tidyr)}
\FunctionTok{library}\NormalTok{(nycflights13)}
\FunctionTok{library}\NormalTok{(fivethirtyeight)}
\end{Highlighting}
\end{Shaded}

\hypertarget{csv}{%
\section{Importing data}\label{csv}}

Up to this point, we've almost entirely used data stored inside of an R package. Say instead you have your own data saved on your computer or somewhere online. How can you analyze this data in R? Spreadsheet data is often saved in one of the following three formats:

First, a \emph{Comma Separated Values} \texttt{.csv} \index{CSV file} file. You can think of a \texttt{.csv} file as a bare-bones spreadsheet where:

\begin{itemize}
\tightlist
\item
  Each line in the file corresponds to one row of data/one observation.
\item
  Values for each line are separated with commas. In other words, the values of different variables are separated by commas in each row.
\item
  The first line is often, but not always, a \emph{header} row indicating the names of the columns/variables.
\end{itemize}

Second, an Excel \texttt{.xlsx} spreadsheet file. This format is based on Microsoft's proprietary Excel software. As opposed to bare-bones \texttt{.csv} files, \texttt{.xlsx} Excel files contain a lot of meta-data\index{meta-data} (data about data). Recall we saw a previous example of meta-data in Section \ref{groupby} when adding ``group structure'' meta-data to a data frame by using the \texttt{group\_by()} verb. Some examples of Excel spreadsheet meta-data include the use of bold and italic fonts, colored cells, different column widths, and formula macros.

Third, a \href{https://www.google.com/sheets/about/}{Google Sheets} file, which is a ``cloud'' or online-based way to work with a spreadsheet. Google Sheets allows you to download your data in both comma separated values \texttt{.csv} and Excel \texttt{.xlsx} formats. One way to import Google Sheets data in R is to go to the Google Sheets menu bar -\textgreater{} File -\textgreater{} Download as -\textgreater{} Select ``Microsoft Excel'' or ``Comma-separated values'' and then load that data into R. A more advanced way to import Google Sheets data in R is by using the \href{https://cran.r-project.org/web/packages/googlesheets/vignettes/basic-usage.html}{\texttt{googlesheets}} package, a method we leave to a more advanced data science book.

We'll cover two methods for importing \texttt{.csv} and \texttt{.xlsx} spreadsheet data in R: one using the console and the other using RStudio's graphical user interface, abbreviated as ``GUI.''

\hypertarget{using-the-console}{%
\subsection{Using the console}\label{using-the-console}}

First, let's import a Comma Separated Values \texttt{.csv} file that exists on the internet. The \texttt{.csv} file \texttt{dem\_score.csv} contains ratings of the level of democracy in different countries spanning 1952 to 1992 and is accessible at \url{https://moderndive.com/data/dem_score.csv}. Let's use the \texttt{read\_csv()} function from the \texttt{readr} \index{R packages!readr!read\_csv()} \citep{R-readr} package to read it off the web, import it into R, and save it in a data frame called \texttt{dem\_score}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(readr)}
\NormalTok{dem\_score }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"https://moderndive.com/data/dem\_score.csv"}\NormalTok{)}
\NormalTok{dem\_score}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 96 x 10
   country   `1952` `1957` `1962` `1967` `1972` `1977` `1982` `1987` `1992`
   <chr>      <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>
 1 Albania       -9     -9     -9     -9     -9     -9     -9     -9      5
 2 Argentina     -9     -1     -1     -9     -9     -9     -8      8      7
 3 Armenia       -9     -7     -7     -7     -7     -7     -7     -7      7
 4 Australia     10     10     10     10     10     10     10     10     10
 5 Austria       10     10     10     10     10     10     10     10     10
 6 Azerbaij~     -9     -7     -7     -7     -7     -7     -7     -7      1
 7 Belarus       -9     -7     -7     -7     -7     -7     -7     -7      7
 8 Belgium       10     10     10     10     10     10     10     10     10
 9 Bhutan       -10    -10    -10    -10    -10    -10    -10    -10    -10
10 Bolivia       -4     -3     -3     -4     -7     -7      8      9      9
# ... with 86 more rows
\end{verbatim}

In this \texttt{dem\_score} data frame, the minimum value of \texttt{-10} corresponds to a highly autocratic nation, whereas a value of \texttt{10} corresponds to a highly democratic nation. Note also that backticks surround the different variable names. Variable names in R by default are not allowed to start with a number nor include spaces, but we can get around this fact by surrounding the column name with backticks. We'll revisit the \texttt{dem\_score} data frame in a case study in the upcoming Section \ref{case-study-tidy}.

Note that the \texttt{read\_csv()} function included in the \texttt{readr} package is different than the \texttt{read.csv()} function that comes installed with R. While the difference in the names might seem trivial (an \texttt{\_} instead of a \texttt{.}), the \texttt{read\_csv()} function is, in our opinion, easier to use since it can more easily read data off the web and generally imports data at a much faster speed. Furthermore, the \texttt{read\_csv()} function included in the \texttt{readr} saves data frames as \texttt{tibbles} by default.

\hypertarget{using-rstudios-interface}{%
\subsection{Using RStudio's interface}\label{using-rstudios-interface}}

Let's read in the exact same data, but this time from an Excel file saved on your computer. Furthermore, we'll do this using RStudio's graphical interface instead of running \texttt{read\_csv()} in the console. First, download the Excel file \texttt{dem\_score.xlsx} by going to https://moderndive.com/data/dem\_score.xlsx, then

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Go to the Files pane of RStudio.
\item
  Navigate to the directory (i.e., folder on your computer) where the downloaded \texttt{dem\_score.xlsx} Excel file is saved. For example, this might be in your Downloads folder.
\item
  Click on \texttt{dem\_score.xlsx}.
\item
  Click ``Import Dataset\ldots{}''
\end{enumerate}

At this point, you should see a screen pop-up like in Figure \ref{fig:read-excel}. After clicking on the ``Import'' \index{RStudio!import data} button on the bottom right of Figure \ref{fig:read-excel}, RStudio will save this spreadsheet's data in a data frame called \texttt{dem\_score} and display its contents in the spreadsheet viewer.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{images/rstudio_screenshots/read_excel} 

}

\caption{Importing an Excel file to R.}\label{fig:read-excel}
\end{figure}

Furthermore, note the ``Code Preview'' block in the bottom right of Figure \ref{fig:read-excel}. You can copy and paste this code to reload your data again later programmatically, instead of repeating this manual point-and-click process.

\hypertarget{tidy-data-ex}{%
\section{``Tidy'' data}\label{tidy-data-ex}}

Let's now switch gears and learn about the concept of ``tidy'' data format with a motivating example from the \texttt{fivethirtyeight} package. The \texttt{fivethirtyeight} package \citep{R-fivethirtyeight} provides access to the datasets used in many articles published by the data journalism website, \href{https://fivethirtyeight.com/}{FiveThirtyEight.com}. For a complete list of all 129 datasets included in the \texttt{fivethirtyeight} package, check out the package webpage by going to: \url{https://fivethirtyeight-r.netlify.app/articles/fivethirtyeight.html}.\index{R packages!fivethirtyeight}

Let's focus our attention on the \texttt{drinks} data frame and look at its first 5 rows:

\begin{verbatim}
# A tibble: 5 x 5
  country   beer_servings spirit_servings wine_servings total_litres_of_pu~
  <chr>             <int>           <int>         <int>               <dbl>
1 Afghanis~             0               0             0                 0  
2 Albania              89             132            54                 4.9
3 Algeria              25               0            14                 0.7
4 Andorra             245             138           312                12.4
5 Angola              217              57            45                 5.9
\end{verbatim}

After reading the help file by running \texttt{?drinks}, you'll see that \texttt{drinks} is a data frame containing results from a survey of the average number of servings of beer, spirits, and wine consumed in 193 countries. This data was originally reported on FiveThirtyEight.com in Mona Chalabi's article: \href{https://fivethirtyeight.com/features/dear-mona-followup-where-do-people-drink-the-most-beer-wine-and-spirits/}{``Dear Mona Followup: Where Do People Drink The Most Beer, Wine And Spirits?''}.

Let's apply some of the data wrangling verbs we learned in Chapter \ref{wrangling} on the \texttt{drinks} data frame:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{filter()} the \texttt{drinks} data frame to only consider 4 countries: the United States, China, Italy, and Saudi Arabia, \emph{then}
\item
  \texttt{select()} all columns except \texttt{total\_litres\_of\_pure\_alcohol} by using the \texttt{-} sign, \emph{then}
\item
  \texttt{rename()} the variables \texttt{beer\_servings}, \texttt{spirit\_servings}, and \texttt{wine\_servings} to \texttt{beer}, \texttt{spirit}, and \texttt{wine}, respectively.
\end{enumerate}

and save the resulting data frame in \texttt{drinks\_smaller}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{drinks\_smaller }\OtherTok{\textless{}{-}}\NormalTok{ fivethirtyeight}\SpecialCharTok{::}\NormalTok{drinks }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(country }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"USA"}\NormalTok{, }\StringTok{"China"}\NormalTok{, }\StringTok{"Italy"}\NormalTok{, }\StringTok{"Saudi Arabia"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{total\_litres\_of\_pure\_alcohol) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{beer =}\NormalTok{ beer\_servings, }\AttributeTok{spirit =}\NormalTok{ spirit\_servings, }\AttributeTok{wine =}\NormalTok{ wine\_servings)}
\NormalTok{drinks\_smaller}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 4
  country       beer spirit  wine
  <chr>        <int>  <int> <int>
1 China           79    192     8
2 Italy           85     42   237
3 Saudi Arabia     0      5     0
4 USA            249    158    84
\end{verbatim}

Let's now ask ourselves a question: ``Using the \texttt{drinks\_smaller} data frame, how would we create the side-by-side barplot in Figure \ref{fig:drinks-smaller}?''. Recall we saw barplots displaying two categorical variables in Subsection \ref{two-categ-barplot}.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/drinks-smaller-1} 

}

\caption{Comparing alcohol consumption in 4 countries.}\label{fig:drinks-smaller}
\end{figure}

Let's break down the grammar of graphics we introduced in Section \ref{grammarofgraphics}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The categorical variable \texttt{country} with four levels (China, Italy, Saudi Arabia, USA) would have to be mapped to the \texttt{x}-position of the bars.
\item
  The numerical variable \texttt{servings} would have to be mapped to the \texttt{y}-position of the bars (the height of the bars).
\item
  The categorical variable \texttt{type} with three levels (beer, spirit, wine) would have to be mapped to the \texttt{fill} color of the bars.
\end{enumerate}

Observe that \texttt{drinks\_smaller} has three separate variables \texttt{beer}, \texttt{spirit}, and \texttt{wine}. In order to use the \texttt{ggplot()} function to recreate the barplot in Figure \ref{fig:drinks-smaller} however, we need a \emph{single variable} \texttt{type} with three possible values: \texttt{beer}, \texttt{spirit}, and \texttt{wine}. We could then map this \texttt{type} variable to the \texttt{fill} aesthetic of our plot. In other words, to recreate the barplot in Figure \ref{fig:drinks-smaller}, our data frame would have to look like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{drinks\_smaller\_tidy}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 12 x 3
   country      type   servings
   <chr>        <chr>     <int>
 1 China        beer         79
 2 Italy        beer         85
 3 Saudi Arabia beer          0
 4 USA          beer        249
 5 China        spirit      192
 6 Italy        spirit       42
 7 Saudi Arabia spirit        5
 8 USA          spirit      158
 9 China        wine          8
10 Italy        wine        237
11 Saudi Arabia wine          0
12 USA          wine         84
\end{verbatim}

Observe that while \texttt{drinks\_smaller} and \texttt{drinks\_smaller\_tidy} are both rectangular in shape and contain the same 12 numerical values (3 alcohol types by 4 countries), they are formatted differently. \texttt{drinks\_smaller} is formatted in what's known as \index{wide data format} \href{https://en.wikipedia.org/wiki/Wide_and_narrow_data}{``wide''} format, whereas \texttt{drinks\_smaller\_tidy} is formatted in what's known as \href{https://en.wikipedia.org/wiki/Wide_and_narrow_data\#Narrow}{``long/narrow''} format.

In the context of doing data science in R, long/narrow format \index{long data format} is also known as ``tidy'' format. In order to use the \texttt{ggplot2} and \texttt{dplyr} packages for data visualization and data wrangling, your input data frames \emph{must} be in ``tidy'' format. Thus, all non-``tidy'' data must be converted to ``tidy'' format first. Before we convert non-``tidy'' data frames like \texttt{drinks\_smaller} to ``tidy'' data frames like \texttt{drinks\_smaller\_tidy}, let's define ``tidy'' data.

\hypertarget{tidy-definition}{%
\subsection{Definition of ``tidy'' data}\label{tidy-definition}}

You have surely heard the word ``tidy'' in your life:

\begin{itemize}
\tightlist
\item
  ``Tidy up your room!''
\item
  ``Write your homework in a tidy way so it is easier to provide feedback.''
\item
  Marie Kondo's best-selling book, \href{https://www.powells.com/book/-9781607747307}{\emph{The Life-Changing Magic of Tidying Up: The Japanese Art of Decluttering and Organizing}}, and Netflix TV series \href{https://www.netflix.com/title/80209379}{\emph{Tidying Up with Marie Kondo}}.
\item
  ``I am not by any stretch of the imagination a tidy person, and the piles of unread books on the coffee table and by my bed have a plaintive, pleading quality to me - `Read me, please!'\,'' - Linda Grant
\end{itemize}

What does it mean for your data to be ``tidy''? While ``tidy'' has a clear English meaning of ``organized,'' the word ``tidy'' in data science using R means that your data follows a standardized format. We will follow Hadley Wickham's \index{Wickham, Hadley} definition of \emph{``tidy'' data} \index{tidy data} \citep{tidy} shown also in Figure \ref{fig:tidyfig}:

\begin{quote}
A \emph{dataset} is a collection of values, usually either numbers (if quantitative) or strings AKA text data (if qualitative/categorical). Values are organised in two ways. Every value belongs to a variable and an observation. A variable contains all values that measure the same underlying attribute (like height, temperature, duration) across units. An observation contains all values measured on the same unit (like a person, or a day, or a city) across attributes.

``Tidy'' data is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. In \emph{tidy data}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Each variable forms a column.
\item
  Each observation forms a row.
\item
  Each type of observational unit forms a table.
\end{enumerate}
\end{quote}



\begin{figure}

{\centering \includegraphics[width=0.8\linewidth,height=0.8\textheight]{images/r4ds/tidy-1} 

}

\caption{Tidy data graphic from \emph{R for Data Science}.}\label{fig:tidyfig}
\end{figure}

For example, say you have the following table of stock prices in Table \ref{tab:non-tidy-stocks}:

\begin{table}[!h]

\caption{\label{tab:non-tidy-stocks}Stock prices (non-tidy format)}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{llll}
\toprule
Date & Boeing stock price & Amazon stock price & Google stock price\\
\midrule
2009-01-01 & \$173.55 & \$174.90 & \$174.34\\
2009-01-02 & \$172.61 & \$171.42 & \$170.04\\
\bottomrule
\end{tabular}
\end{table}

Although the data are neatly organized in a rectangular spreadsheet-type format, they do not follow the definition of data in ``tidy'' format. While there are three variables corresponding to three unique pieces of information (date, stock name, and stock price), there are not three columns. In ``tidy'' data format, each variable should be its own column, as shown in Table \ref{tab:tidy-stocks}. Notice that both tables present the same information, but in different formats.

\begin{table}[!h]

\caption{\label{tab:tidy-stocks}Stock prices (tidy format)}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{lll}
\toprule
Date & Stock Name & Stock Price\\
\midrule
2009-01-01 & Boeing & \$173.55\\
2009-01-01 & Amazon & \$174.90\\
2009-01-01 & Google & \$174.34\\
2009-01-02 & Boeing & \$172.61\\
2009-01-02 & Amazon & \$171.42\\
2009-01-02 & Google & \$170.04\\
\bottomrule
\end{tabular}
\end{table}

Now we have the requisite three columns Date, Stock Name, and Stock Price. On the other hand, consider the data in Table \ref{tab:tidy-stocks-2}.

\begin{table}[!h]

\caption{\label{tab:tidy-stocks-2}Example of tidy data}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{l|l|l}
\hline
Date & Boeing Price & Weather\\
\hline
2009-01-01 & \$173.55 & Sunny\\
\hline
2009-01-02 & \$172.61 & Overcast\\
\hline
\end{tabular}
\end{table}

In this case, even though the variable ``Boeing Price'' occurs just like in our non-``tidy'' data in Table \ref{tab:non-tidy-stocks}, the data \emph{is} ``tidy'' since there are three variables corresponding to three unique pieces of information: Date, Boeing price, and the Weather that particular day.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC4.1)} What are common characteristics of ``tidy'' data frames?

\textbf{(LC4.2)} What makes ``tidy'' data frames useful for organizing data?

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{converting-to-tidy-data}{%
\subsection{Converting to ``tidy'' data}\label{converting-to-tidy-data}}

In this book so far, you've only seen data frames that were already in ``tidy'' format. Furthermore, for the rest of this book, you'll mostly only see data frames that are already in ``tidy'' format as well. This is not always the case however with all datasets in the world. If your original data frame is in wide (non-``tidy'') format and you would like to use the \texttt{ggplot2} or \texttt{dplyr} packages, you will first have to convert it to ``tidy'' format. To do so, we recommend using the \index{tidyr!pivot\_longer()} \texttt{pivot\_longer()} function in the \texttt{tidyr} \index{R packages!tidyr} package \citep{R-tidyr}.

Going back to our \texttt{drinks\_smaller} data frame from earlier:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{drinks\_smaller}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 4
  country       beer spirit  wine
  <chr>        <int>  <int> <int>
1 China           79    192     8
2 Italy           85     42   237
3 Saudi Arabia     0      5     0
4 USA            249    158    84
\end{verbatim}

We convert it to ``tidy'' format by using the \texttt{pivot\_longer()} function from the \texttt{tidyr} package as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{drinks\_smaller\_tidy }\OtherTok{\textless{}{-}}\NormalTok{ drinks\_smaller }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{names\_to =} \StringTok{"type"}\NormalTok{, }
               \AttributeTok{values\_to =} \StringTok{"servings"}\NormalTok{, }
               \AttributeTok{cols =} \SpecialCharTok{{-}}\NormalTok{country)}
\NormalTok{drinks\_smaller\_tidy}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 12 x 3
   country      type   servings
   <chr>        <chr>     <int>
 1 China        beer         79
 2 China        spirit      192
 3 China        wine          8
 4 Italy        beer         85
 5 Italy        spirit       42
 6 Italy        wine        237
 7 Saudi Arabia beer          0
 8 Saudi Arabia spirit        5
 9 Saudi Arabia wine          0
10 USA          beer        249
11 USA          spirit      158
12 USA          wine         84
\end{verbatim}

We set the arguments to \texttt{pivot\_longer()} as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{names\_to} here corresponds to the name of the variable in the new ``tidy''/long data frame that will contain the \emph{column names} of the original data. Observe how we set \texttt{names\_to\ =\ "type"}. In the resulting \texttt{drinks\_smaller\_tidy}, the column \texttt{type} contains the three types of alcohol \texttt{beer}, \texttt{spirit}, and \texttt{wine}. Since \texttt{type} is a variable name that doesn't appear in \texttt{drinks\_smaller}, we use quotation marks around it. You'll receive an error if you just use \texttt{names\_to\ =\ type} here.
\item
  \texttt{values\_to} here is the name of the variable in the new ``tidy'' data frame that will contain the \emph{values} of the original data. Observe how we set \texttt{values\_to\ =\ "servings"} since each of the numeric values in each of the \texttt{beer}, \texttt{wine}, and \texttt{spirit} columns of the \texttt{drinks\_smaller} data corresponds to a value of \texttt{servings}. In the resulting \texttt{drinks\_smaller\_tidy}, the column \texttt{servings} contains the 4 \(\times\) 3 = 12 numerical values. Note again that \texttt{servings} doesn't appear as a variable in \texttt{drinks\_smaller} so it again needs quotation marks around it for the \texttt{values\_to} argument.
\item
  The third argument \texttt{cols} is the columns in the \texttt{drinks\_smaller} data frame you either want to or don't want to ``tidy.'' Observe how we set this to \texttt{-country} indicating that we don't want to ``tidy'' the \texttt{country} variable in \texttt{drinks\_smaller} and rather only \texttt{beer}, \texttt{spirit}, and \texttt{wine}. Since \texttt{country} is a column that appears in \texttt{drinks\_smaller} we don't put quotation marks around it.
\end{enumerate}

The third argument here of \texttt{cols} is a little nuanced, so let's consider code that's written slightly differently but that produces the same output:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{drinks\_smaller }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{names\_to =} \StringTok{"type"}\NormalTok{, }
               \AttributeTok{values\_to =} \StringTok{"servings"}\NormalTok{, }
               \AttributeTok{cols =} \FunctionTok{c}\NormalTok{(beer, spirit, wine))}
\end{Highlighting}
\end{Shaded}

Note that the third argument now specifies which columns we want to ``tidy'' with \texttt{c(beer,\ spirit,\ wine)}, instead of the columns we don't want to ``tidy'' using \texttt{-country}. We use the \texttt{c()} function to create a vector of the columns in \texttt{drinks\_smaller} that we'd like to ``tidy.'' Note that since these three columns appear one after another in the \texttt{drinks\_smaller} data frame, we could also do the following for the \texttt{cols} argument:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{drinks\_smaller }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{names\_to =} \StringTok{"type"}\NormalTok{, }
               \AttributeTok{values\_to =} \StringTok{"servings"}\NormalTok{, }
               \AttributeTok{cols =}\NormalTok{ beer}\SpecialCharTok{:}\NormalTok{wine)}
\end{Highlighting}
\end{Shaded}

With our \texttt{drinks\_smaller\_tidy} ``tidy'' formatted data frame, we can now produce the barplot you saw in Figure \ref{fig:drinks-smaller} using \texttt{geom\_col()}. This is done in Figure \ref{fig:drinks-smaller-tidy-barplot}. Recall from Section \ref{geombar} on barplots that we use \texttt{geom\_col()} and not \texttt{geom\_bar()}, since we would like to map the ``pre-counted'' \texttt{servings} variable to the \texttt{y}-aesthetic of the bars.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(drinks\_smaller\_tidy, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ country, }\AttributeTok{y =}\NormalTok{ servings, }\AttributeTok{fill =}\NormalTok{ type)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{(}\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}



\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/drinks-smaller-tidy-barplot-1} 

}

\caption{Comparing alcohol consumption in 4 countries using geom\_col().}\label{fig:drinks-smaller-tidy-barplot}
\end{figure}

Converting ``wide'' format data to ``tidy'' format often confuses new R users. The only way to learn to get comfortable with the \texttt{pivot\_longer()} function is with practice, practice, and more practice using different datasets. For example, run \texttt{?pivot\_longer} and look at the examples in the bottom of the help file. We'll show another example of using \texttt{pivot\_longer()} to convert a ``wide'' formatted data frame to ``tidy'' format in Section \ref{case-study-tidy}.

If however you want to convert a ``tidy'' data frame to ``wide'' format, you will need to use the \texttt{pivot\_wider()}\index{tidyr!pivot\_wider()} function instead. Run \texttt{?pivot\_wider} and look at the examples in the bottom of the help file for examples.

You can also view examples of both \texttt{pivot\_longer()} and \texttt{pivot\_wider()} on the \href{https://tidyr.tidyverse.org/dev/articles/pivot.html\#pew}{tidyverse.org} webpage. There's a nice example to check out the different functions available for data tidying and a case study using data from the World Health Organization on that webpage. Furthermore, each week the R4DS Online Learning Community posts a dataset in the weekly \href{https://github.com/rfordatascience/tidytuesday}{\texttt{\#}TidyTuesday event} that might serve as a nice place for you to find other data to explore and transform.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC4.3)} Take a look at the \texttt{airline\_safety} data frame included in the \texttt{fivethirtyeight} data package. Run the following:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airline\_safety}
\end{Highlighting}
\end{Shaded}

After reading the help file by running \texttt{?airline\_safety}, we see that \texttt{airline\_safety} is a data frame containing information on different airline companies' safety records. This data was originally reported on the data journalism website, FiveThirtyEight.com, in Nate Silver's article, \href{https://fivethirtyeight.com/features/should-travelers-avoid-flying-airlines-that-have-had-crashes-in-the-past/}{``Should Travelers Avoid Flying Airlines That Have Had Crashes in the Past?''}. Let's only consider the variables \texttt{airlines} and those relating to fatalities for simplicity:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airline\_safety\_smaller }\OtherTok{\textless{}{-}}\NormalTok{ airline\_safety }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(airline, }\FunctionTok{starts\_with}\NormalTok{(}\StringTok{"fatalities"}\NormalTok{))}
\NormalTok{airline\_safety\_smaller}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 56 x 3
   airline               fatalities_85_99 fatalities_00_14
   <chr>                            <int>            <int>
 1 Aer Lingus                           0                0
 2 Aeroflot                           128               88
 3 Aerolineas Argentinas                0                0
 4 Aeromexico                          64                0
 5 Air Canada                           0                0
 6 Air France                          79              337
 7 Air India                          329              158
 8 Air New Zealand                      0                7
 9 Alaska Airlines                      0               88
10 Alitalia                            50                0
# ... with 46 more rows
\end{verbatim}

This data frame is not in ``tidy'' format. How would you convert this data frame to be in ``tidy'' format, in particular so that it has a variable \texttt{fatalities\_years} indicating the incident year and a variable \texttt{count} of the fatality counts?

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{nycflights13-package-1}{%
\subsection{\texorpdfstring{\texttt{nycflights13} package}{nycflights13 package}}\label{nycflights13-package-1}}

Recall the \texttt{nycflights13} package we introduced in Section \ref{nycflights13} with data about all domestic flights departing from New York City in 2013. Let's revisit the \texttt{flights} data frame by running \texttt{View(flights)}. We saw that \texttt{flights} has a rectangular shape, with each of its 336,776 rows corresponding to a flight and each of its 22 columns corresponding to different characteristics/measurements of each flight. This satisfied the first two criteria of the definition of ``tidy'' data from Subsection \ref{tidy-definition}: that ``Each variable forms a column'' and ``Each observation forms a row.'' But what about the third property of ``tidy'' data that ``Each type of observational unit forms a table''?

Recall that we saw in Subsection \ref{exploredataframes} that the observational unit for the \texttt{flights} data frame is an individual flight. In other words, the rows of the \texttt{flights} data frame refer to characteristics/measurements of individual flights. Also included in the \texttt{nycflights13} package are other data frames with their rows representing different observational units \citep{R-nycflights13}:

\begin{itemize}
\tightlist
\item
  \texttt{airlines}: translation between two letter IATA carrier codes and airline company names (16 in total). The observational unit is an airline company.
\item
  \texttt{planes}: aircraft information about each of 3,322 planes used, i.e., the observational unit is an aircraft.
\item
  \texttt{weather}: hourly meteorological data (about 8,705 observations) for each of the three NYC airports, i.e., the observational unit is an hourly measurement of weather at one of the three airports.
\item
  \texttt{airports}: airport names and locations. The observational unit is an airport.
\end{itemize}

The organization of the information into these five data frames follows the third ``tidy'' data property: observations corresponding to the same observational unit should be saved in the same table, i.e., data frame. You could think of this property as the old English expression: ``birds of a feather flock together.''

\hypertarget{case-study-tidy}{%
\section{Case study: Democracy in Guatemala}\label{case-study-tidy}}

In this section, we'll show you another example of how to convert a data frame that isn't in ``tidy'' format (``wide'' format) to a data frame that is in ``tidy'' format (``long/narrow'' format). We'll do this using the \texttt{pivot\_longer()} function from the \texttt{tidyr} package again.

Furthermore, we'll make use of functions from the \texttt{ggplot2} and \texttt{dplyr} packages to produce a \emph{time-series plot} showing how the democracy scores have changed over the 40 years from 1952 to 1992 for Guatemala. Recall that we saw time-series plots in Section \ref{linegraphs} on creating linegraphs using \texttt{geom\_line()}.

Let's use the \texttt{dem\_score} data frame we imported in Section \ref{csv}, but focus on only data corresponding to Guatemala.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{guat\_dem }\OtherTok{\textless{}{-}}\NormalTok{ dem\_score }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(country }\SpecialCharTok{==} \StringTok{"Guatemala"}\NormalTok{)}
\NormalTok{guat\_dem}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 10
  country   `1952` `1957` `1962` `1967` `1972` `1977` `1982` `1987` `1992`
  <chr>      <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>
1 Guatemala      2     -6     -5      3      1     -3     -7      3      3
\end{verbatim}

Let's lay out the grammar of graphics we saw in Section \ref{grammarofgraphics}.

First we know we need to set \texttt{data\ =\ guat\_dem} and use a \texttt{geom\_line()} layer, but what is the aesthetic mapping of variables? We'd like to see how the democracy score has changed over the years, so we need to map:

\begin{itemize}
\tightlist
\item
  \texttt{year} to the x-position aesthetic and
\item
  \texttt{democracy\_score} to the y-position aesthetic
\end{itemize}

Now we are stuck in a predicament, much like with our \texttt{drinks\_smaller} example in Section \ref{tidy-data-ex}. We see that we have a variable named \texttt{country}, but its only value is \texttt{"Guatemala"}. We have other variables denoted by different year values. Unfortunately, the \texttt{guat\_dem} data frame is not ``tidy'' and hence is not in the appropriate format to apply the grammar of graphics, and thus we cannot use the \texttt{ggplot2} package just yet.

We need to take the values of the columns corresponding to years in \texttt{guat\_dem} and convert them into a new ``names'' variable called \texttt{year}. Furthermore, we need to take the democracy score values in the inside of the data frame and turn them into a new ``values'' variable called \texttt{democracy\_score}. Our resulting data frame will have three columns: \texttt{country}, \texttt{year}, and \texttt{democracy\_score}. Recall that the \texttt{pivot\_longer()} function in the \texttt{tidyr} package does this for us:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{guat\_dem\_tidy }\OtherTok{\textless{}{-}}\NormalTok{ guat\_dem }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{names\_to =} \StringTok{"year"}\NormalTok{, }
               \AttributeTok{values\_to =} \StringTok{"democracy\_score"}\NormalTok{, }
               \AttributeTok{cols =} \SpecialCharTok{{-}}\NormalTok{country) }
\NormalTok{guat\_dem\_tidy}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 9 x 3
  country   year  democracy_score
  <chr>     <chr>           <dbl>
1 Guatemala 1952                2
2 Guatemala 1957               -6
3 Guatemala 1962               -5
4 Guatemala 1967                3
5 Guatemala 1972                1
6 Guatemala 1977               -3
7 Guatemala 1982               -7
8 Guatemala 1987                3
9 Guatemala 1992                3
\end{verbatim}

(Note this code differs slightly from our print edition due to an update of the \texttt{tidyr} package to version 1.1.0.) We set the arguments to \texttt{pivot\_longer()} as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{names\_to} is the name of the variable in the new ``tidy'' data frame that will contain the \emph{column names} of the original data. Observe how we set \texttt{names\_to\ =\ "year"}. In the resulting \texttt{guat\_dem\_tidy}, the column \texttt{year} contains the years where Guatemala's democracy scores were measured.
\item
  \texttt{values\_to} is the name of the variable in the new ``tidy'' data frame that will contain the \emph{values} of the original data. Observe how we set \texttt{values\_to\ =\ "democracy\_score"}. In the resulting \texttt{guat\_dem\_tidy} the column \texttt{democracy\_score} contains the 1 \(\times\) 9 = 9 democracy scores as numeric values.
\item
  The third argument is the columns you either want to or don't want to ``tidy.'' Observe how we set this to \texttt{cols\ =\ -country} indicating that we don't want to ``tidy'' the \texttt{country} variable in \texttt{guat\_dem} and rather only variables \texttt{1952} through \texttt{1992}.
\item
  The last argument of \texttt{names\_transform} tells R what type of variable \texttt{year} should be set to. Without specifying that it is an \texttt{integer} as we've done here, \texttt{pivot\_longer()} will set it to be a character value by default.
\end{enumerate}

We can now create the time-series plot in Figure \ref{fig:guat-dem-tidy} to visualize how democracy scores in Guatemala have changed from 1952 to 1992 using a \index{ggplot2!geom\_line()} \texttt{geom\_line()}. Furthermore, we'll use the \texttt{labs()} function in the \texttt{ggplot2} package to add informative labels to all the \texttt{aes()}thetic attributes of our plot, in this case the \texttt{x} and \texttt{y} positions.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(guat\_dem\_tidy, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ year, }\AttributeTok{y =}\NormalTok{ democracy\_score)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Year"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Democracy Score"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
geom_path: Each group consists of only one observation. Do you need to
adjust the group aesthetic?
\end{verbatim}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/guat-dem-tidy-1} 

}

\caption{Democracy scores in Guatemala 1952-1992.}\label{fig:guat-dem-tidy}
\end{figure}

Note that if we forgot to include the \texttt{names\_transform} argument specifying that \texttt{year} was not of character format, we would have gotten an error here since \texttt{geom\_line()} wouldn't have known how to sort the character values in \texttt{year} in the right order.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC4.4)} Convert the \texttt{dem\_score} data frame into
a ``tidy'' data frame and assign the name of \texttt{dem\_score\_tidy} to the resulting long-formatted data frame.

\textbf{(LC4.5)} Read in the life expectancy data stored at \url{https://moderndive.com/data/le_mess.csv} and convert it to a ``tidy'' data frame.

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{tidyverse-package}{%
\section{\texorpdfstring{\texttt{tidyverse} package}{tidyverse package}}\label{tidyverse-package}}

Notice at the beginning of the chapter we loaded the following four packages, which are among four of the most frequently used R packages for data science:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(dplyr)}
\FunctionTok{library}\NormalTok{(readr)}
\FunctionTok{library}\NormalTok{(tidyr)}
\end{Highlighting}
\end{Shaded}

Recall that \texttt{ggplot2} is for data visualization, \texttt{dplyr} is for data wrangling, \texttt{readr} is for importing spreadsheet data into R, and \texttt{tidyr} is for converting data to ``tidy'' format. There is a much quicker way to load these packages than by individually loading them: by installing and loading the \texttt{tidyverse} package. The \texttt{tidyverse} package acts as an ``umbrella'' package whereby installing/loading it will install/load multiple packages at once for you.

After installing the \texttt{tidyverse} package as you would a normal package as seen in Section \ref{packages}, running:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

would be the same as running:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(dplyr)}
\FunctionTok{library}\NormalTok{(readr)}
\FunctionTok{library}\NormalTok{(tidyr)}
\FunctionTok{library}\NormalTok{(purrr)}
\FunctionTok{library}\NormalTok{(tibble)}
\FunctionTok{library}\NormalTok{(stringr)}
\FunctionTok{library}\NormalTok{(forcats)}
\end{Highlighting}
\end{Shaded}

The \texttt{purrr}, \texttt{tibble}, \texttt{stringr}, and \texttt{forcats} are left for a more advanced book; check out \href{http://r4ds.had.co.nz/}{\emph{R for Data Science}} to learn about these packages.

For the remainder of this book, we'll start every chapter by running \texttt{library(tidyverse)}, instead of loading the various component packages individually. The \texttt{tidyverse} ``umbrella'' package gets its name from the fact that all the functions in all its packages are designed to have common inputs and outputs: data frames are in ``tidy'' format. This standardization of input and output data frames makes transitions between different functions in the different packages as seamless as possible. For more information, check out the \href{https://www.tidyverse.org/}{tidyverse.org} webpage for the package.

\hypertarget{tidy-data-conclusion}{%
\section{Conclusion}\label{tidy-data-conclusion}}

\hypertarget{additional-resources-3}{%
\subsection{Additional resources}\label{additional-resources-3}}

Solutions to all \emph{Learning checks} can be found online in \href{https://moderndive.com/D-appendixD.html}{Appendix D}.

An R script file of all R code used in this chapter is available at \url{https://www.moderndive.com/scripts/04-tidy.R}.

If you want to learn more about using the \texttt{readr} and \texttt{tidyr} package, we suggest that you check out RStudio's ``Data Import Cheat Sheet.'' In the current version of RStudio in late 2019, you can access this cheatsheet by going to the RStudio Menu Bar -\textgreater{} Help -\textgreater{} Cheatsheets -\textgreater{} ``Browse Cheatsheets'' -\textgreater{} Scroll down the page to the ``Data Import Cheat Sheet.'' The first page of this cheatsheet has information on using the \texttt{readr} package to import data, while the second page has information on using the \texttt{tidyr} package to ``tidy'' data.

\hypertarget{whats-to-come-2}{%
\subsection{What's to come?}\label{whats-to-come-2}}

Congratulations! You've completed the ``Data Science with \texttt{tidyverse}'' portion of this book. We'll now move to the ``Data modeling with moderndive'' portion of this book in Chapters \ref{regression} and \ref{multiple-regression}, where you'll leverage your data visualization and wrangling skills to model relationships between different variables in data frames.

However, we're going to leave Chapter \ref{inference-for-regression} on ``Inference for Regression'' until after we've covered statistical inference in Chapters \ref{sampling}, \ref{confidence-intervals}, and \ref{hypothesis-testing}. Onwards and upwards into Data Modeling as shown in Figure \ref{fig:part2}!



\begin{figure}

{\centering \includegraphics[width=\textwidth]{images/flowcharts/flowchart/flowchart.005} 

}

\caption{\emph{ModernDive} flowchart - on to Part II!}\label{fig:part2}
\end{figure}



\hypertarget{part-refmoderndivepart}{%
\part{Data Modeling with \texttt{moderndive}}\label{part-refmoderndivepart}}

\hypertarget{regression}{%
\chapter{Basic Regression}\label{regression}}

Now that we are equipped with data visualization skills from Chapter \ref{viz}, data wrangling skills from Chapter \ref{wrangling}, and an understanding of how to import data and the concept of a ``tidy'' data format from Chapter \ref{tidy}, let's now proceed with data modeling. The fundamental premise of data modeling is to make explicit the relationship between:

\begin{itemize}
\tightlist
\item
  an \emph{outcome variable} \(y\), also called a \emph{dependent variable} or response variable, \index{variables!response / outcome / dependent} and
\item
  an \emph{explanatory/predictor variable} \(x\), also called an \emph{independent variable} or \index{variables!explanatory / predictor / independent} covariate.
\end{itemize}

Another way to state this is using mathematical terminology: we will model the outcome variable \(y\) ``as a function'' of the explanatory/predictor variable \(x\). When we say ``function'' here, we aren't referring to functions in R like the \texttt{ggplot()} function, but rather as a mathematical function. But, why do we have two different labels, explanatory and predictor, for the variable \(x\)? That's because even though the two terms are often used interchangeably, roughly speaking data modeling serves one of two purposes:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Modeling for explanation}: When you want to explicitly describe and quantify the relationship between the outcome variable \(y\) and a set of explanatory variables \(x\), determine the significance of any relationships, have measures summarizing these relationships, and possibly identify any \emph{causal} relationships between the variables.
\item
  \textbf{Modeling for prediction}: When you want to predict an outcome variable \(y\) based on the information contained in a set of predictor variables \(x\). Unlike modeling for explanation, however, you don't care so much about understanding how all the variables relate and interact with one another, but rather only whether you can make good predictions about \(y\) using the information in \(x\).
\end{enumerate}

For example, say you are interested in an outcome variable \(y\) of whether patients develop lung cancer and information \(x\) on their risk factors, such as smoking habits, age, and socioeconomic status. If we are modeling for explanation, we would be interested in both describing and quantifying the effects of the different risk factors. One reason could be that you want to design an intervention to reduce lung cancer incidence in a population, such as targeting smokers of a specific age group with advertising for smoking cessation programs. If we are modeling for prediction, however, we wouldn't care so much about understanding how all the individual risk factors contribute to lung cancer, but rather only whether we can make good predictions of which people will contract lung cancer.

In this book, we'll focus on modeling for explanation and hence refer to \(x\) as \emph{explanatory variables}. If you are interested in learning about modeling for prediction, we suggest you check out books and courses on the field of \emph{machine learning} such as \href{http://www-bcf.usc.edu/~gareth/ISL/}{\emph{An Introduction to Statistical Learning with Applications in R (ISLR)}} \citep{islr2017}. Furthermore, while there exist many techniques for modeling, such as tree-based models and neural networks, in this book we'll focus on one particular technique: \emph{linear regression}. \index{regression!linear} Linear regression is one of the most commonly-used and easy-to-understand approaches to modeling.

Linear regression involves a \emph{numerical} outcome variable \(y\) and explanatory variables \(x\) that are either \emph{numerical} or \emph{categorical}. Furthermore, the relationship between \(y\) and \(x\) is assumed to be linear, or in other words, a line. However, we'll see that what constitutes a ``line'' will vary depending on the nature of your explanatory variables \(x\).

In Chapter \ref{regression} on basic regression, we'll only consider models with a single explanatory variable \(x\). In Section \ref{model1}, the explanatory variable will be numerical. This scenario is known as \emph{simple linear regression}. In Section \ref{model2}, the explanatory variable will be categorical.

In Chapter \ref{multiple-regression} on multiple regression, we'll extend the ideas behind basic regression and consider models with two explanatory variables \(x_1\) and \(x_2\). In Section \ref{model4}, we'll have two numerical explanatory variables. In Section \ref{model3}, we'll have one numerical and one categorical explanatory variable. In particular, we'll consider two such models: \emph{interaction} and \emph{parallel slopes} models.

In Chapter \ref{inference-for-regression} on inference for regression, we'll revisit our regression models and analyze the results using the tools for \emph{statistical inference} you'll develop in Chapters \ref{sampling}, \ref{confidence-intervals}, and \ref{hypothesis-testing} on sampling, bootstrapping and confidence intervals, and hypothesis testing and \(p\)-values, respectively.

Let's now begin with basic regression, \index{regression!basic} which refers to linear regression models with a single explanatory variable \(x\). We'll also discuss important statistical concepts like the \emph{correlation coefficient}, that ``correlation isn't necessarily causation,'' and what it means for a line to be ``best-fitting.''

\hypertarget{reg-packages}{%
\subsection*{Needed packages}\label{reg-packages}}


Let's now load all the packages needed for this chapter (this assumes you've already installed them). In this chapter, we introduce some new packages:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The \texttt{tidyverse} ``umbrella'' \citep{R-tidyverse} package. Recall from our discussion in Section \ref{tidyverse-package} that loading the \texttt{tidyverse} package by running \texttt{library(tidyverse)} loads the following commonly used data science packages all at once:

  \begin{itemize}
  \tightlist
  \item
    \texttt{ggplot2} for data visualization
  \item
    \texttt{dplyr} for data wrangling
  \item
    \texttt{tidyr} for converting data to ``tidy'' format
  \item
    \texttt{readr} for importing spreadsheet data into R
  \item
    As well as the more advanced \texttt{purrr}, \texttt{tibble}, \texttt{stringr}, and \texttt{forcats} packages
  \end{itemize}
\item
  The \texttt{moderndive} \index{R packages!moderndive} package of datasets and functions for tidyverse-friendly introductory linear regression.
\item
  The \texttt{skimr} \citep{R-skimr} package, which provides a simple-to-use function to quickly compute a wide array of commonly used summary statistics. \index{summary statistics}
\end{enumerate}

If needed, read Section \ref{packages} for information on how to install and load R packages.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(moderndive)}
\FunctionTok{library}\NormalTok{(skimr)}
\FunctionTok{library}\NormalTok{(gapminder)}
\end{Highlighting}
\end{Shaded}

\hypertarget{model1}{%
\section{One numerical explanatory variable}\label{model1}}

Why do some professors and instructors at universities and colleges receive high teaching evaluations scores from students while others receive lower ones? Are there differences in teaching evaluations between instructors of different demographic groups? Could there be an impact due to student biases? These are all questions that are of interest to university/college administrators, as teaching evaluations are among the many criteria considered in determining which instructors and professors get promoted.

Researchers at the University of Texas in Austin, Texas (UT Austin) tried to answer the following research question: what factors explain differences in instructor teaching evaluation scores? To this end, they collected instructor and course information on 463 courses. A full description of the study can be found at \href{https://www.openintro.org/stat/data/?data=evals}{openintro.org}.

In this section, we'll keep things simple for now and try to explain differences in instructor teaching scores as a function of one numerical variable: the instructor's ``beauty'' score (we'll describe how this score was determined shortly). Could it be that instructors with higher ``beauty'' scores also have higher teaching evaluations? Could it be instead that instructors with higher ``beauty'' scores tend to have lower teaching evaluations? Or could it be that there is no relationship between ``beauty'' score and teaching evaluations? We'll answer these questions by modeling the relationship between teaching scores and ``beauty'' scores using \emph{simple linear regression} \index{regression!simple linear} where we have:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A numerical outcome variable \(y\) (the instructor's teaching score) and
\item
  A single numerical explanatory variable \(x\) (the instructor's ``beauty'' score).
\end{enumerate}

\hypertarget{model1EDA}{%
\subsection{Exploratory data analysis}\label{model1EDA}}

The data on the 463 courses at UT Austin can be found in the \texttt{evals} data frame included in the \texttt{moderndive} package. However, to keep things simple, let's \texttt{select()} only the subset of the variables we'll consider in this chapter, and save this data in a new data frame called \texttt{evals\_ch5}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{evals\_ch5 }\OtherTok{\textless{}{-}}\NormalTok{ evals }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(ID, score, bty\_avg, age)}
\end{Highlighting}
\end{Shaded}

A crucial step before doing any kind of analysis or modeling is performing an \emph{exploratory data analysis}, \index{data analysis!exploratory} or EDA for short. EDA gives you a sense of the distributions of the individual variables in your data, whether any potential relationships exist between variables, whether there are outliers and/or missing values, and (most importantly) how to build your model. Here are three common steps in an EDA:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Most crucially, looking at the raw data values.
\item
  Computing summary statistics, such as means, medians, and interquartile ranges.
\item
  Creating data visualizations.
\end{enumerate}

Let's perform the first common step in an exploratory data analysis: looking at the raw data values. Because this step seems so trivial, unfortunately many data analysts ignore it. However, getting an early sense of what your raw data looks like can often prevent many larger issues down the road.

You can do this by using RStudio's spreadsheet viewer or by using the \texttt{glimpse()} function as introduced in Subsection \ref{exploredataframes} on exploring data frames:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(evals\_ch5)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 463
Columns: 4
$ ID      <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,...
$ score   <dbl> 4.7, 4.1, 3.9, 4.8, 4.6, 4.3, 2.8, 4.1, 3.4, 4.5, 3.8,...
$ bty_avg <dbl> 5.00, 5.00, 5.00, 5.00, 3.00, 3.00, 3.00, 3.33, 3.33, ...
$ age     <int> 36, 36, 36, 36, 59, 59, 59, 51, 51, 40, 40, 40, 40, 40...
\end{verbatim}

Observe that \texttt{Observations:\ 463} indicates that there are 463 rows/observations in \texttt{evals\_ch5}, where each row corresponds to one observed course at UT Austin. It is important to note that the \emph{observational unit} \index{observational unit} is an individual course and not an individual instructor. Recall from Subsection \ref{exploredataframes} that the observational unit is the ``type of thing'' that is being measured by our variables. Since instructors teach more than one course in an academic year, the same instructor will appear more than once in the data. Hence there are fewer than 463 unique instructors being represented in \texttt{evals\_ch5}. We'll revisit this idea in Section \ref{regression-conditions}, when we talk about the ``independence assumption'' for inference for regression.

A full description of all the variables included in \texttt{evals} can be found at \href{https://www.openintro.org/stat/data/?data=evals}{openintro.org} or by reading the associated help file (run \texttt{?evals} in the console). However, let's fully describe only the 4 variables we selected in \texttt{evals\_ch5}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{ID}: An identification variable used to distinguish between the 1 through 463 courses in the dataset.
\item
  \texttt{score}: A numerical variable of the course instructor's average teaching score, where the average is computed from the evaluation scores from all students in that course. Teaching scores of 1 are lowest and 5 are highest. This is the outcome variable \(y\) of interest.
\item
  \texttt{bty\_avg}: A numerical variable of the course instructor's average ``beauty'' score, where the average is computed from a separate panel of six students. ``Beauty'' scores of 1 are lowest and 10 are highest. This is the explanatory variable \(x\) of interest.
\item
  \texttt{age}: A numerical variable of the course instructor's age. This will be another explanatory variable \(x\) that we'll use in the \emph{Learning check} at the end of this subsection.
\end{enumerate}

An alternative way to look at the raw data values is by choosing a random sample of the rows in \texttt{evals\_ch5} by piping it into the \texttt{sample\_n()} \index{dplyr!sample\_n()} function from the \texttt{dplyr} package. Here we set the \texttt{size} argument to be \texttt{5}, indicating that we want a random sample of 5 rows. We display the results in Table \ref{tab:five-random-courses}. Note that due to the random nature of the sampling, you will likely end up with a different subset of 5 rows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{evals\_ch5 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{sample\_n}\NormalTok{(}\AttributeTok{size =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]

\caption{\label{tab:five-random-courses}A random sample of 5 out of the 463 courses at UT Austin}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{rrrr}
\toprule
ID & score & bty\_avg & age\\
\midrule
129 & 3.7 & 3.00 & 62\\
109 & 4.7 & 4.33 & 46\\
28 & 4.8 & 5.50 & 62\\
434 & 2.8 & 2.00 & 62\\
330 & 4.0 & 2.33 & 64\\
\bottomrule
\end{tabular}
\end{table}

Now that we've looked at the raw values in our \texttt{evals\_ch5} data frame and got a preliminary sense of the data, let's move on to the next common step in an exploratory data analysis: computing summary statistics. Let's start by computing the mean and median of our numerical outcome variable \texttt{score} and our numerical explanatory variable ``beauty'' score denoted as \texttt{bty\_avg}. We'll do this by using the \texttt{summarize()} function from \texttt{dplyr} along with the \texttt{mean()} and \texttt{median()} summary functions we saw in Section \ref{summarize}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{evals\_ch5 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{mean\_bty\_avg =} \FunctionTok{mean}\NormalTok{(bty\_avg), }\AttributeTok{mean\_score =} \FunctionTok{mean}\NormalTok{(score),}
            \AttributeTok{median\_bty\_avg =} \FunctionTok{median}\NormalTok{(bty\_avg), }\AttributeTok{median\_score =} \FunctionTok{median}\NormalTok{(score))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 4
  mean_bty_avg mean_score median_bty_avg median_score
         <dbl>      <dbl>          <dbl>        <dbl>
1      4.41784    4.17473          4.333          4.3
\end{verbatim}

However, what if we want other summary statistics as well, such as the standard deviation (a measure of spread), the minimum and maximum values, and various percentiles?

Typing out all these summary statistic functions in \texttt{summarize()} would be long and tedious. Instead, let's use the convenient \texttt{skim()} function from the \texttt{skimr} \index{R packages!skimr!skim()} package. This function takes in a data frame, ``skims'' it, and returns commonly used summary statistics. Let's take our \texttt{evals\_ch5} data frame, \texttt{select()} only the outcome and explanatory variables teaching \texttt{score} and \texttt{bty\_avg}, and pipe them into the \texttt{skim()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{evals\_ch5 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(score, bty\_avg) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{skim}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Skim summary statistics
 n obs: 463 
 n variables: 2 

── Variable type:numeric
 variable missing complete   n mean   sd   p0  p25  p50 p75 p100
  bty_avg       0      463 463 4.42 1.53 1.67 3.17 4.33 5.5 8.17
    score       0      463 463 4.17 0.54 2.3  3.8  4.3  4.6 5   
\end{verbatim}

(For formatting purposes in this book, the inline histogram that is usually printed with \texttt{skim()} has been removed. This can be done by using \texttt{skim\_with(numeric\ =\ list(hist\ =\ NULL))} prior to using the \texttt{skim()} function for version 1.0.6 of \texttt{skimr}.)

For the numerical variables teaching \texttt{score} and \texttt{bty\_avg} it returns:

\begin{itemize}
\tightlist
\item
  \texttt{missing}: the number of missing values
\item
  \texttt{complete}: the number of non-missing or complete values
\item
  \texttt{n}: the total number of values
\item
  \texttt{mean}: the average
\item
  \texttt{sd}: the standard deviation
\item
  \texttt{p0}: the 0th percentile: the value at which 0\% of observations are smaller than it (the \emph{minimum} value)
\item
  \texttt{p25}: the 25th percentile: the value at which 25\% of observations are smaller than it (the \emph{1st quartile})
\item
  \texttt{p50}: the 50th percentile: the value at which 50\% of observations are smaller than it (the \emph{2nd} quartile and more commonly called the \emph{median})
\item
  \texttt{p75}: the 75th percentile: the value at which 75\% of observations are smaller than it (the \emph{3rd quartile})
\item
  \texttt{p100}: the 100th percentile: the value at which 100\% of observations are smaller than it (the \emph{maximum} value)
\end{itemize}

Looking at this output, we can see how the values of both variables distribute. For example, the mean teaching score was 4.17 out of 5, whereas the mean ``beauty'' score was 4.42 out of 10. Furthermore, the middle 50\% of teaching scores was between 3.80 and 4.6 (the first and third quartiles), whereas the middle 50\% of ``beauty'' scores falls within 3.17 to 5.5 out of 10.

The \texttt{skim()} function only returns what are known as \emph{univariate} \index{univariate} summary statistics: functions that take a single variable and return some numerical summary of that variable. However, there also exist \emph{bivariate} \index{bivariate} summary statistics: functions that take in two variables and return some summary of those two variables. In particular, when the two variables are numerical, we can compute the \index{correlation (coefficient)} \emph{correlation coefficient}. Generally speaking, \emph{coefficients} are quantitative expressions of a specific phenomenon. A \emph{correlation coefficient} is a quantitative expression of the \emph{strength of the linear relationship between two numerical variables}. Its value ranges between -1 and 1 where:

\begin{itemize}
\tightlist
\item
  -1 indicates a perfect \emph{negative relationship}: As one variable increases, the value of the other variable tends to go down, following a straight line.
\item
  0 indicates no relationship: The values of both variables go up/down independently of each other.
\item
  +1 indicates a perfect \emph{positive relationship}: As the value of one variable goes up, the value of the other variable tends to go up as well in a linear fashion.
\end{itemize}

Figure \ref{fig:correlation1} gives examples of 9 different correlation coefficient values for hypothetical numerical variables \(x\) and \(y\). For example, observe in the top right plot that for a correlation coefficient of -0.75 there is a negative linear relationship between \(x\) and \(y\), but it is not as strong as the negative linear relationship between \(x\) and \(y\) when the correlation coefficient is -0.9 or -1.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/correlation1-1} 

}

\caption{Nine different correlation coefficients.}\label{fig:correlation1}
\end{figure}

The correlation coefficient can be computed using the \texttt{get\_correlation()} \index{moderndive!get\_correlation()} function in the \texttt{moderndive} package. In this case, the inputs to the function are the two numerical variables for which we want to calculate the correlation coefficient.

We put the name of the outcome variable on the left-hand side of the \texttt{\textasciitilde{}} ``tilde'' sign, while putting the name of the explanatory variable on the right-hand side. This is known as R's \index{R!formula notation} \emph{formula notation}. We will use this same ``formula'' syntax with regression later in this chapter.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{evals\_ch5 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{get\_correlation}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ score }\SpecialCharTok{\textasciitilde{}}\NormalTok{ bty\_avg)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 1
       cor
     <dbl>
1 0.187142
\end{verbatim}

An alternative way to compute correlation is to use the \texttt{cor()} summary function within a \texttt{summarize()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{evals\_ch5 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{correlation =} \FunctionTok{cor}\NormalTok{(score, bty\_avg))}
\end{Highlighting}
\end{Shaded}

In our case, the correlation coefficient of 0.187 indicates that the relationship between teaching evaluation score and ``beauty'' average is ``weakly positive.'' There is a certain amount of subjectivity in interpreting correlation coefficients, especially those that aren't close to the extreme values of -1, 0, and 1. To develop your intuition about correlation coefficients, play the ``Guess the Correlation'' 1980's style video game mentioned in Subsection \ref{additional-resources-basic-regression}.

Let's now perform the last of the steps in an exploratory data analysis: creating data visualizations. Since both the \texttt{score} and \texttt{bty\_avg} variables are numerical, a scatterplot is an appropriate graph to visualize this data. Let's do this using \texttt{geom\_point()} and display the result in Figure \ref{fig:numxplot1}. Furthermore, let's highlight the six points in the top right of the visualization in a box.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(evals\_ch5, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ bty\_avg, }\AttributeTok{y =}\NormalTok{ score)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Beauty Score"}\NormalTok{, }
       \AttributeTok{y =} \StringTok{"Teaching Score"}\NormalTok{,}
       \AttributeTok{title =} \StringTok{"Scatterplot of relationship of teaching and beauty scores"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/numxplot1-1} 

}

\caption{Instructor evaluation scores at UT Austin.}\label{fig:numxplot1}
\end{figure}

Observe that most ``beauty'' scores lie between 2 and 8, while most teaching scores lie between 3 and 5. Furthermore, while opinions may vary, it is our opinion that the relationship between teaching score and ``beauty'' score is ``weakly positive.'' This is consistent with our earlier computed correlation coefficient of 0.187.

Furthermore, there appear to be six points in the top-right of this plot highlighted in the box. However, this is not actually the case, as this plot suffers from \emph{overplotting}. Recall from Subsection \ref{overplotting} that overplotting occurs when several points are stacked directly on top of each other, making it difficult to distinguish them. So while it may appear that there are only six points in the box, there are actually more. This fact is only apparent when using \texttt{geom\_jitter()} in place of \texttt{geom\_point()}. We display the resulting plot in Figure \ref{fig:numxplot2} along with the same small box as in Figure \ref{fig:numxplot1}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(evals\_ch5, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ bty\_avg, }\AttributeTok{y =}\NormalTok{ score)) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Beauty Score"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Teaching Score"}\NormalTok{,}
       \AttributeTok{title =} \StringTok{"Scatterplot of relationship of teaching and beauty scores"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/numxplot2-1} 

}

\caption{Instructor evaluation scores at UT Austin.}\label{fig:numxplot2}
\end{figure}

It is now apparent that there are 12 points in the area highlighted in the box and not six as originally suggested in Figure \ref{fig:numxplot1}. Recall from Subsection \ref{overplotting} on overplotting that jittering adds a little random ``nudge'' to each of the points to break up these ties. Furthermore, recall that jittering is strictly a visualization tool; it does not alter the original values in the data frame \texttt{evals\_ch5}. To keep things simple going forward, however, we'll only present regular scatterplots rather than their jittered counterparts.

Let's build on the unjittered scatterplot in Figure \ref{fig:numxplot1} by adding a ``best-fitting'' line: of all possible lines we can draw on this scatterplot, it is the line that ``best'' fits through the cloud of points. We do this by adding a new \texttt{geom\_smooth(method\ =\ "lm",\ se\ =\ FALSE)} layer to the \texttt{ggplot()} code that created the scatterplot in Figure \ref{fig:numxplot1}. The \texttt{method\ =\ "lm"} argument sets the line to be a ``\texttt{l}inear \texttt{m}odel.'' The \texttt{se\ =\ FALSE} \index{ggplot2!geom\_smooth()} argument suppresses \emph{standard error} uncertainty bars. (We'll define the concept of \emph{standard error} later in Subsection \ref{sampling-definitions}.)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(evals\_ch5, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ bty\_avg, }\AttributeTok{y =}\NormalTok{ score)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Beauty Score"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Teaching Score"}\NormalTok{,}
       \AttributeTok{title =} \StringTok{"Relationship between teaching and beauty scores"}\NormalTok{) }\SpecialCharTok{+}  
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/numxplot3-1} 

}

\caption{Regression line.}\label{fig:numxplot3}
\end{figure}

The line in the resulting Figure \ref{fig:numxplot3} is called a ``regression line.'' The regression line \index{regression!line} is a visual summary of the relationship between two numerical variables, in our case the outcome variable \texttt{score} and the explanatory variable \texttt{bty\_avg}. The positive slope of the blue line is consistent with our earlier observed correlation coefficient of 0.187 suggesting that there is a positive relationship between these two variables: as instructors have higher ``beauty'' scores, so also do they receive higher teaching evaluations. We'll see later, however, that while the correlation coefficient and the slope of a regression line always have the same sign (positive or negative), they typically do not have the same value.

Furthermore, a regression line is ``best-fitting'' in that it minimizes some mathematical criteria. We present these mathematical criteria in Subsection \ref{leastsquares}, but we suggest you read this subsection only after first reading the rest of this section on regression with one numerical explanatory variable.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC5.1)} Conduct a new exploratory data analysis with the same outcome variable \(y\) being \texttt{score} but with \texttt{age} as the new explanatory variable \(x\). Remember, this involves three things:

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\tightlist
\item
  Looking at the raw data values.
\item
  Computing summary statistics.
\item
  Creating data visualizations.
\end{enumerate}

What can you say about the relationship between age and teaching scores based on this exploration?

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{model1table}{%
\subsection{Simple linear regression}\label{model1table}}

You may recall from secondary/high school algebra that the equation of a line is \(y = a + b\cdot x\). (Note that the \(\cdot\) symbol is equivalent to the \(\times\) ``multiply by'' mathematical symbol. We'll use the \(\cdot\) symbol in the rest of this book as it is more succinct.) It is defined by two coefficients \(a\) and \(b\). The intercept coefficient \(a\) is the value of \(y\) when \(x = 0\). The slope coefficient \(b\) for \(x\) is the increase in \(y\) for every increase of one in \(x\). This is also called the ``rise over run.''

However, when defining a regression line like the regression line in Figure \ref{fig:numxplot3}, we use slightly different notation: the equation of the regression line is \(\widehat{y} = b_0 + b_1 \cdot x\) \index{regression!equation of a line}. The intercept coefficient is \(b_0\), so \(b_0\) is the value of \(\widehat{y}\) when \(x = 0\). The slope coefficient for \(x\) is \(b_1\), i.e., the increase in \(\widehat{y}\) for every increase of one in \(x\). Why do we put a ``hat'' on top of the \(y\)? It's a form of notation commonly used in regression to indicate that we have a \index{regression!fitted value} ``fitted value,'' or the value of \(y\) on the regression line for a given \(x\) value. We'll discuss this more in the upcoming Subsection \ref{model1points}.

We know that the regression line in Figure \ref{fig:numxplot3} has a positive slope \(b_1\) corresponding to our explanatory \(x\) variable \texttt{bty\_avg}. Why? Because as instructors tend to have higher \texttt{bty\_avg} scores, so also do they tend to have higher teaching evaluation \texttt{scores}. However, what is the numerical value of the slope \(b_1\)? What about the intercept \(b_0\)? Let's not compute these two values by hand, but rather let's use a computer!

We can obtain the values of the intercept \(b_0\) and the slope for \texttt{bty\_avg} \(b_1\) by outputting a \emph{linear regression table}. This is done in two steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We first ``fit'' the linear regression model using the \texttt{lm()} function and save it in \texttt{score\_model}.
\item
  We get the regression table by applying the \texttt{get\_regression\_table()} \index{moderndive!get\_regression\_table()} function from the \texttt{moderndive} package to \texttt{score\_model}.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit regression model:}
\NormalTok{score\_model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(score }\SpecialCharTok{\textasciitilde{}}\NormalTok{ bty\_avg, }\AttributeTok{data =}\NormalTok{ evals\_ch5)}
\CommentTok{\# Get regression table:}
\FunctionTok{get\_regression\_table}\NormalTok{(score\_model)}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]

\caption{\label{tab:regtable}Linear regression table}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{lrrrrrr}
\toprule
term & estimate & std\_error & statistic & p\_value & lower\_ci & upper\_ci\\
\midrule
intercept & 3.880 & 0.076 & 50.96 & 0 & 3.731 & 4.030\\
bty\_avg & 0.067 & 0.016 & 4.09 & 0 & 0.035 & 0.099\\
\bottomrule
\end{tabular}
\end{table}

Let's first focus on interpreting the regression table output in Table \ref{tab:regtable}, and then we'll later revisit the code that produced it. In the \texttt{estimate} column of Table \ref{tab:regtable} are the intercept \(b_0\) = 3.88 and the slope \(b_1\) = 0.067 for \texttt{bty\_avg}. Thus the equation of the regression line in Figure \ref{fig:numxplot3} follows:

\[
\begin{aligned}
\widehat{y} &= b_0 + b_1 \cdot x\\
\widehat{\text{score}} &= b_0 + b_{\text{bty}\_\text{avg}} \cdot\text{bty}\_\text{avg}\\
&= 3.88 + 0.067\cdot\text{bty}\_\text{avg}
\end{aligned}
\]

The intercept \(b_0\) = 3.88 is the average teaching score \(\widehat{y}\) = \(\widehat{\text{score}}\) for those courses where the instructor had a ``beauty'' score \texttt{bty\_avg} of 0. Or in graphical terms, it's where the line intersects the \(y\) axis when \(x\) = 0. Note, however, that while the \index{regression!equation of a line!intercept} intercept of the regression line has a mathematical interpretation, it has no \emph{practical} interpretation here, since observing a \texttt{bty\_avg} of 0 is impossible; it is the average of six panelists' ``beauty'' scores ranging from 1 to 10. Furthermore, looking at the scatterplot with the regression line in Figure \ref{fig:numxplot3}, no instructors had a ``beauty'' score anywhere near 0.

Of greater interest is the \index{regression!equation of a line!slope} slope \(b_1\) = \(b_{\text{bty}\_\text{avg}}\) for \texttt{bty\_avg} of 0.067, as this summarizes the relationship between the teaching and ``beauty'' score variables. Note that the sign is positive, suggesting a positive relationship between these two variables, meaning teachers with higher ``beauty'' scores also tend to have higher teaching scores. Recall from earlier that the correlation coefficient is 0.187. They both have the same positive sign, but have a different value. Recall further that the correlation's interpretation is the ``strength of linear association''. The \index{regression!interpretation of the slope} slope's interpretation is a little different:

\begin{quote}
For every increase of 1 unit in \texttt{bty\_avg}, there is an \emph{associated} increase of, \emph{on average}, 0.067 units of \texttt{score}.
\end{quote}

We only state that there is an \emph{associated} increase and not necessarily a \emph{causal} increase. For example, perhaps it's not that higher ``beauty'' scores directly cause higher teaching scores per se. Instead, the following could hold true: individuals from wealthier backgrounds tend to have stronger educational backgrounds and hence have higher teaching scores, while at the same time these wealthy individuals also tend to have higher ``beauty'' scores. In other words, just because two variables are strongly associated, it doesn't necessarily mean that one causes the other. This is summed up in the often quoted phrase, ``correlation is not necessarily causation.'' We discuss this idea further in Subsection \ref{correlation-is-not-causation}.

Furthermore, we say that this associated increase is \emph{on average} 0.067 units of teaching \texttt{score}, because you might have two instructors whose \texttt{bty\_avg} scores differ by 1 unit, but their difference in teaching scores won't necessarily be exactly 0.067. What the slope of 0.067 is saying is that across all possible courses, the \emph{average} difference in teaching score between two instructors whose ``beauty'' scores differ by one is 0.067.

Now that we've learned how to compute the equation for the regression line in Figure \ref{fig:numxplot3} using the values in the \texttt{estimate} column of Table \ref{tab:regtable}, and how to interpret the resulting intercept and slope, let's revisit the code that generated this table:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit regression model:}
\NormalTok{score\_model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(score }\SpecialCharTok{\textasciitilde{}}\NormalTok{ bty\_avg, }\AttributeTok{data =}\NormalTok{ evals\_ch5)}
\CommentTok{\# Get regression table:}
\FunctionTok{get\_regression\_table}\NormalTok{(score\_model)}
\end{Highlighting}
\end{Shaded}

First, we ``fit'' the linear regression model to the \texttt{data} using the \texttt{lm()} \index{lm()} function and save this as \texttt{score\_model}. When we say ``fit'', we mean ``find the best fitting line to this data.'' \texttt{lm()} stands for ``linear model'' and is used as follows: \texttt{lm(y\ \textasciitilde{}\ x,\ data\ =\ data\_frame\_name)} where:

\begin{itemize}
\tightlist
\item
  \texttt{y} is the outcome variable, followed by a tilde \texttt{\textasciitilde{}}. In our case, \texttt{y} is set to \texttt{score}.
\item
  \texttt{x} is the explanatory variable. In our case, \texttt{x} is set to \texttt{bty\_avg}.
\item
  The combination of \texttt{y\ \textasciitilde{}\ x} is called a \emph{model formula}. (Note the order of \texttt{y} and \texttt{x}.) In our case, the model formula is \texttt{score\ \textasciitilde{}\ bty\_avg}. We saw such model formulas earlier when we computed the correlation coefficient using the \texttt{get\_correlation()} function in Subsection \ref{model1EDA}.
\item
  \texttt{data\_frame\_name} is the name of the data frame that contains the variables \texttt{y} and \texttt{x}. In our case, \texttt{data\_frame\_name} is the \texttt{evals\_ch5} data frame.
\end{itemize}

Second, we take the saved model in \texttt{score\_model} and apply the \texttt{get\_regression\_table()} function from the \texttt{moderndive} package to it to obtain the regression table in Table \ref{tab:regtable}. This function is an example of what's known in computer programming as a \emph{wrapper function}. \index{functions!wrapper} They take other pre-existing functions and ``wrap'' them into a single function that hides its inner workings. This concept is illustrated in Figure \ref{fig:moderndive-figure-wrapper}.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth,height=0.6\textheight]{images/shutterstock/wrapper_function} 

}

\caption{The concept of a wrapper function.}\label{fig:moderndive-figure-wrapper}
\end{figure}

So all you need to worry about is what the inputs look like and what the outputs look like; you leave all the other details ``under the hood of the car.'' In our regression modeling example, the \texttt{get\_regression\_table()} function takes a saved \texttt{lm()} linear regression model as input and returns a data frame of the regression table as output. If you're interested in learning more about the \texttt{get\_regression\_table()} function's inner workings, check out Subsection \ref{underthehood}.

Lastly, you might be wondering what the remaining five columns in Table \ref{tab:regtable} are: \texttt{std\_error}, \texttt{statistic}, \texttt{p\_value}, \texttt{lower\_ci} and \texttt{upper\_ci}. They are the \emph{standard error}, \emph{test statistic}, \emph{p-value}, \emph{lower 95\% confidence interval bound}, and \emph{upper 95\% confidence interval bound}. They tell us about both the \emph{statistical significance} and \emph{practical significance} of our results. This is loosely the ``meaningfulness'' of our results from a statistical perspective. Let's put aside these ideas for now and revisit them in Chapter \ref{inference-for-regression} on (statistical) inference for regression. We'll do this after we've had a chance to cover standard errors in Chapter \ref{sampling}, confidence intervals in Chapter \ref{confidence-intervals}, and hypothesis testing and \(p\)-values in Chapter \ref{hypothesis-testing}.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC5.2)} Fit a new simple linear regression using \texttt{lm(score\ \textasciitilde{}\ age,\ data\ =\ evals\_ch5)} where \texttt{age} is the new explanatory variable \(x\). Get information about the ``best-fitting'' line from the regression table by applying the \texttt{get\_regression\_table()} function. How do the regression results match up with the results from your earlier exploratory data analysis?

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{model1points}{%
\subsection{Observed/fitted values and residuals}\label{model1points}}

We just saw how to get the value of the intercept and the slope of a regression line from the \texttt{estimate} column of a regression table generated by the \texttt{get\_regression\_table()} function. Now instead say we want information on individual observations. For example, let's focus on the 21st of the 463 courses in the \texttt{evals\_ch5} data frame in Table \ref{tab:instructor-21}:

\begin{table}[!h]

\caption{\label{tab:instructor-21}Data for the 21st course out of 463}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{rrrr}
\toprule
ID & score & bty\_avg & age\\
\midrule
21 & 4.9 & 7.33 & 31\\
\bottomrule
\end{tabular}
\end{table}

What is the value \(\widehat{y}\) on the regression line corresponding to this instructor's \texttt{bty\_avg} ``beauty'' score of 7.333? In Figure \ref{fig:numxplot4} we mark three values corresponding to the instructor for this 21st course and give their statistical names:

\begin{itemize}
\tightlist
\item
  Circle: The \emph{observed value} \(y\) = 4.9 is this course's instructor's actual teaching score.
\item
  Square: The \emph{fitted value} \(\widehat{y}\) is the value on the regression line for \(x\) = \texttt{bty\_avg} = 7.333. This value is computed using the intercept and slope in the previous regression table:
\end{itemize}

\[\widehat{y} = b_0 + b_1 \cdot x = 3.88 + 0.067 \cdot 7.333 = 4.369\]

\begin{itemize}
\tightlist
\item
  Arrow: The length of this arrow is the \emph{residual} \index{regression!residual} and is computed by subtracting the fitted value \(\widehat{y}\) from the observed value \(y\). The residual can be thought of as a model's error or ``lack of fit'' for a particular observation. In the case of this course's instructor, it is \(y - \widehat{y}\) = 4.9 - 4.369 = 0.531.
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/numxplot4-1} 

}

\caption{Example of observed value, fitted value, and residual.}\label{fig:numxplot4}
\end{figure}

Now say we want to compute both the fitted value \(\widehat{y} = b_0 + b_1 \cdot x\) and the residual \(y - \widehat{y}\) for \emph{all} 463 courses in the study. Recall that each course corresponds to one of the 463 rows in the \texttt{evals\_ch5} data frame and also one of the 463 points in the regression plot in Figure \ref{fig:numxplot4}.

We could repeat the previous calculations we performed by hand 463 times, but that would be tedious and time consuming. Instead, let's do this using a computer with the \texttt{get\_regression\_points()} function. Just like the \texttt{get\_regression\_table()} function, the \texttt{get\_regression\_points()} function is a ``wrapper'' function. However, this function returns a different output. Let's apply the \texttt{get\_regression\_points()} function to \texttt{score\_model}, which is where we saved our \texttt{lm()} model in the previous section. In Table \ref{tab:regression-points-1} we present the results of only the 21st through 24th courses for brevity's sake.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{regression\_points }\OtherTok{\textless{}{-}} \FunctionTok{get\_regression\_points}\NormalTok{(score\_model)}
\NormalTok{regression\_points}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:regression-points-1}Regression points (for only the 21st through 24th courses)}
\centering
\begin{tabular}[t]{rrrrr}
\toprule
ID & score & bty\_avg & score\_hat & residual\\
\midrule
21 & 4.9 & 7.33 & 4.37 & 0.531\\
22 & 4.6 & 7.33 & 4.37 & 0.231\\
23 & 4.5 & 7.33 & 4.37 & 0.131\\
24 & 4.4 & 5.50 & 4.25 & 0.153\\
\bottomrule
\end{tabular}
\end{table}

Let's inspect the individual columns and match them with the elements of Figure \ref{fig:numxplot4}:

\begin{itemize}
\tightlist
\item
  The \texttt{score} column represents the observed outcome variable \(y\). This is the y-position of the 463 black points.
\item
  The \texttt{bty\_avg} column represents the values of the explanatory variable \(x\). This is the x-position of the 463 black points.
\item
  The \texttt{score\_hat} column represents the fitted values \(\widehat{y}\). This is the corresponding value on the regression line for the 463 \(x\) values.
\item
  The \texttt{residual} column represents the residuals \(y - \widehat{y}\). This is the 463 vertical distances between the 463 black points and the regression line.
\end{itemize}

Just as we did for the instructor of the 21st course in the \texttt{evals\_ch5} dataset (in the first row of the table), let's repeat the calculations for the instructor of the 24th course (in the fourth row of Table \ref{tab:regression-points-1}):

\begin{itemize}
\tightlist
\item
  \texttt{score} = 4.4 is the observed teaching \texttt{score} \(y\) for this course's instructor.
\item
  \texttt{bty\_avg} = 5.50 is the value of the explanatory variable \texttt{bty\_avg} \(x\) for this course's instructor.
\item
  \texttt{score\_hat} = 4.25 = 3.88 + 0.067 \(\cdot\) 5.50 is the fitted value \(\widehat{y}\) on the regression line for this course's instructor.
\item
  \texttt{residual} = 0.153 = 4.4 - 4.25 is the value of the residual for this instructor. In other words, the model's fitted value was off by 0.153 teaching score units for this course's instructor.
\end{itemize}

At this point, you can skip ahead if you like to Subsection \ref{leastsquares} to learn about the processes behind what makes ``best-fitting'' regression lines. As a primer, a ``best-fitting'' line refers to the line that minimizes the \emph{sum of squared residuals} out of all possible lines we can draw through the points. In Section \ref{model2}, we'll discuss another common scenario of having a categorical explanatory variable and a numerical outcome variable.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC5.3)} Generate a data frame of the residuals of the model where you used \texttt{age} as the explanatory \(x\) variable.

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{model2}{%
\section{One categorical explanatory variable}\label{model2}}

It's an unfortunate truth that life expectancy is not the same across all countries in the world. International development agencies are interested in studying these differences in life expectancy in the hopes of identifying where governments should allocate resources to address this problem. In this section, we'll explore differences in life expectancy in two ways:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Differences between continents: Are there significant differences in average life expectancy between the five populated continents of the world: Africa, the Americas, Asia, Europe, and Oceania?
\item
  Differences within continents: How does life expectancy vary within the world's five continents? For example, is the spread of life expectancy among the countries of Africa larger than the spread of life expectancy among the countries of Asia?
\end{enumerate}

To answer such questions, we'll use the \texttt{gapminder} data frame included in the \texttt{gapminder} \index{R packages!gapminder!gapminder data frame} package. This dataset has international development statistics such as life expectancy, GDP per capita, and population for 142 countries for 5-year intervals between 1952 and 2007. Recall we visualized some of this data in Figure \ref{fig:gapminder} in Subsection \ref{gapminder} on the grammar of graphics.

We'll use this data for basic regression again, but now using an explanatory variable \(x\) that is categorical, as opposed to the numerical explanatory variable model we used in the previous Section \ref{model1}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A numerical outcome variable \(y\) (a country's life expectancy) and
\item
  A single categorical explanatory variable \(x\) (the continent that the country is a part of).
\end{enumerate}

When the explanatory variable \(x\) is categorical, the concept of a ``best-fitting'' regression line is a little different than the one we saw previously in Section \ref{model1} where the explanatory variable \(x\) was numerical. We'll study these differences shortly in Subsection \ref{model2table}, but first we conduct an exploratory data analysis.

\hypertarget{model2EDA}{%
\subsection{Exploratory data analysis}\label{model2EDA}}

The data on the 142 countries can be found in the \texttt{gapminder} data frame included in the \texttt{gapminder} package. However, to keep things simple, let's \texttt{filter()} for only those observations/rows corresponding to the year 2007. Additionally, let's \texttt{select()} only the subset of the variables we'll consider in this chapter. We'll save this data in a new data frame called \texttt{gapminder2007}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(gapminder)}
\NormalTok{gapminder2007 }\OtherTok{\textless{}{-}}\NormalTok{ gapminder }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(year }\SpecialCharTok{==} \DecValTok{2007}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(country, lifeExp, continent, gdpPercap)}
\end{Highlighting}
\end{Shaded}

Let's perform the first common step in an exploratory data analysis: looking at the raw data values. You can do this by using RStudio's spreadsheet viewer or by using the \texttt{glimpse()} command as introduced in Subsection \ref{exploredataframes} on exploring data frames:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(gapminder2007)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 142
Columns: 4
$ country   <fct> Afghanistan, Albania, Algeria, Angola, Argentina, Au...
$ lifeExp   <dbl> 43.8, 76.4, 72.3, 42.7, 75.3, 81.2, 79.8, 75.6, 64.1...
$ continent <fct> Asia, Europe, Africa, Africa, Americas, Oceania, Eur...
$ gdpPercap <dbl> 975, 5937, 6223, 4797, 12779, 34435, 36126, 29796, 1...
\end{verbatim}

Observe that \texttt{Observations:\ 142} indicates that there are 142 rows/observations in \texttt{gapminder2007}, where each row corresponds to one country. In other words, the \emph{observational unit} is an individual country. Furthermore, observe that the variable \texttt{continent} is of type \texttt{\textless{}fct\textgreater{}}, which stands for \emph{factor}, which is R's way of encoding categorical variables.

A full description of all the variables included in \texttt{gapminder} can be found by reading the associated help file (run \texttt{?gapminder} in the console). However, let's fully describe only the 4 variables we selected in \texttt{gapminder2007}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{country}: An identification variable of type character/text used to distinguish the 142 countries in the dataset.
\item
  \texttt{lifeExp}: A numerical variable of that country's life expectancy at birth. This is the outcome variable \(y\) of interest.
\item
  \texttt{continent}: A categorical variable with five levels. Here ``levels'' correspond to the possible categories: Africa, Asia, Americas, Europe, and Oceania. This is the explanatory variable \(x\) of interest.
\item
  \texttt{gdpPercap}: A numerical variable of that country's GDP per capita in US inflation-adjusted dollars that we'll use as another outcome variable \(y\) in the \emph{Learning check} at the end of this subsection.
\end{enumerate}

Let's look at a random sample of five out of the 142 countries in Table \ref{tab:model2-data-preview}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder2007 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{sample\_n}\NormalTok{(}\AttributeTok{size =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]

\caption{\label{tab:model2-data-preview}Random sample of 5 out of 142 countries}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{lrlr}
\toprule
country & lifeExp & continent & gdpPercap\\
\midrule
Togo & 58.4 & Africa & 883\\
Sao Tome and Principe & 65.5 & Africa & 1598\\
Congo, Dem. Rep. & 46.5 & Africa & 278\\
Lesotho & 42.6 & Africa & 1569\\
Bulgaria & 73.0 & Europe & 10681\\
\bottomrule
\end{tabular}
\end{table}

Note that random sampling will likely produce a different subset of 5 rows for you than what's shown. Now that we've looked at the raw values in our \texttt{gapminder2007} data frame and got a sense of the data, let's move on to computing summary statistics. Let's once again apply the \texttt{skim()} function from the \texttt{skimr} package. Recall from our previous EDA that this function takes in a data frame, ``skims'' it, and returns commonly used summary statistics. Let's take our \texttt{gapminder2007} data frame, \texttt{select()} only the outcome and explanatory variables \texttt{lifeExp} and \texttt{continent}, and pipe them into the \texttt{skim()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gapminder2007 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(lifeExp, continent) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{skim}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Skim summary statistics
 n obs: 142 
 n variables: 2 

── Variable type:factor
  variable missing complete   n n_unique                         top_counts ordered
 continent       0      142 142        5 Afr: 52, Asi: 33, Eur: 30, Ame: 25   FALSE

── Variable type:numeric
 variable missing complete   n  mean    sd    p0   p25   p50   p75 p100
  lifeExp       0      142 142 67.01 12.07 39.61 57.16 71.94 76.41 82.6
\end{verbatim}

The \texttt{skim()} output now reports summaries for categorical variables (\texttt{Variable\ type:factor}) separately from the numerical variables (\texttt{Variable\ type:numeric}). For the categorical variable \texttt{continent}, it reports:

\begin{itemize}
\tightlist
\item
  \texttt{missing}, \texttt{complete}, and \texttt{n}, which are the number of missing, complete, and total number of values as before, respectively.
\item
  \texttt{n\_unique}: The number of unique levels to this variable, corresponding to Africa, Asia, Americas, Europe, and Oceania. This refers to how many countries are in the data for each continent.
\item
  \texttt{top\_counts}: In this case, the top four counts: \texttt{Africa} has 52 countries, \texttt{Asia} has 33, \texttt{Europe} has 30, and \texttt{Americas} has 25. Not displayed is \texttt{Oceania} with 2 countries.
\item
  \texttt{ordered}: This tells us whether the categorical variable is ``ordinal'': whether there is an encoded hierarchy (like low, medium, high). In this case, \texttt{continent} is not ordered.
\end{itemize}

Turning our attention to the summary statistics of the numerical variable \texttt{lifeExp}, we observe that the global median life expectancy in 2007 was 71.94. Thus, half of the world's countries (71 countries) had a life expectancy less than 71.94. The mean life expectancy of 67.01 is lower, however. Why is the mean life expectancy lower than the median?

We can answer this question by performing the last of the three common steps in an exploratory data analysis: creating data visualizations. Let's visualize the distribution of our outcome variable \(y\) = \texttt{lifeExp} in Figure \ref{fig:lifeExp2007hist}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(gapminder2007, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ lifeExp)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{5}\NormalTok{, }\AttributeTok{color =} \StringTok{"white"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Life expectancy"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Number of countries"}\NormalTok{,}
       \AttributeTok{title =} \StringTok{"Histogram of distribution of worldwide life expectancies"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/lifeExp2007hist-1} 

}

\caption{Histogram of life expectancy in 2007.}\label{fig:lifeExp2007hist}
\end{figure}

We see that this data is \emph{left-skewed}, also known as \emph{negatively} \index{skew} skewed: there are a few countries with low life expectancy that are bringing down the mean life expectancy. However, the median is less sensitive to the effects of such outliers; hence, the median is greater than the mean in this case.

Remember, however, that we want to compare life expectancies both between continents and within continents. In other words, our visualizations need to incorporate some notion of the variable \texttt{continent}. We can do this easily with a faceted histogram. Recall from Section \ref{facets} that facets allow us to split a visualization by the different values of another variable. We display the resulting visualization in Figure \ref{fig:catxplot0b} by adding a \index{ggplot2!facet\_wrap()} \texttt{facet\_wrap(\textasciitilde{}\ continent,\ nrow\ =\ 2)} layer.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(gapminder2007, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ lifeExp)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{5}\NormalTok{, }\AttributeTok{color =} \StringTok{"white"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Life expectancy"}\NormalTok{, }
       \AttributeTok{y =} \StringTok{"Number of countries"}\NormalTok{,}
       \AttributeTok{title =} \StringTok{"Histogram of distribution of worldwide life expectancies"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ continent, }\AttributeTok{nrow =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/catxplot0b-1} 

}

\caption{Life expectancy in 2007.}\label{fig:catxplot0b}
\end{figure}

Observe that unfortunately the distribution of African life expectancies is much lower than the other continents, while in Europe life expectancies tend to be higher and furthermore do not vary as much. On the other hand, both Asia and Africa have the most variation in life expectancies. There is the least variation in Oceania, but keep in mind that there are only two countries in Oceania: Australia and New Zealand.

Recall that an alternative method to visualize the distribution of a numerical variable split by a categorical variable is by using a side-by-side boxplot. We map the categorical variable \texttt{continent} to the \(x\)-axis and the different life expectancies within each continent on the \(y\)-axis in Figure \ref{fig:catxplot1}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(gapminder2007, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ continent, }\AttributeTok{y =}\NormalTok{ lifeExp)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Continent"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Life expectancy"}\NormalTok{,}
       \AttributeTok{title =} \StringTok{"Life expectancy by continent"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/catxplot1-1} 

}

\caption{Life expectancy in 2007.}\label{fig:catxplot1}
\end{figure}

Some people prefer comparing the distributions of a numerical variable between different levels of a categorical variable using a boxplot instead of a faceted histogram. This is because we can make quick comparisons between the categorical variable's levels with imaginary horizontal lines. For example, observe in Figure \ref{fig:catxplot1} that we can quickly convince ourselves that Oceania has the highest median life expectancies by drawing an imaginary horizontal line at \(y\) = 80. Furthermore, as we observed in the faceted histogram in Figure \ref{fig:catxplot0b}, Africa and Asia have the largest variation in life expectancy as evidenced by their large interquartile ranges (the heights of the boxes).

It's important to remember, however, that the solid lines in the middle of the boxes correspond to the medians (the middle value) rather than the mean (the average). So, for example, if you look at Asia, the solid line denotes the median life expectancy of around 72 years. This tells us that half of all countries in Asia have a life expectancy below 72 years, whereas half have a life expectancy above 72 years.

Let's compute the median and mean life expectancy for each continent with a little more data wrangling and display the results in Table \ref{tab:catxplot0}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lifeExp\_by\_continent }\OtherTok{\textless{}{-}}\NormalTok{ gapminder2007 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(continent) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{median =} \FunctionTok{median}\NormalTok{(lifeExp), }
            \AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(lifeExp))}
\end{Highlighting}
\end{Shaded}

\begin{table}

\caption{\label{tab:catxplot0}Life expectancy by continent}
\centering
\begin{tabular}[t]{lrr}
\toprule
continent & median & mean\\
\midrule
Africa & 52.9 & 54.8\\
Americas & 72.9 & 73.6\\
Asia & 72.4 & 70.7\\
Europe & 78.6 & 77.6\\
Oceania & 80.7 & 80.7\\
\bottomrule
\end{tabular}
\end{table}

Observe the order of the second column \texttt{median} life expectancy: Africa is lowest, the Americas and Asia are next with similar medians, then Europe, then Oceania. This ordering corresponds to the ordering of the solid black lines inside the boxes in our side-by-side boxplot in Figure \ref{fig:catxplot1}.

Let's now turn our attention to the values in the third column \texttt{mean}. Using Africa's mean life expectancy of 54.8 as a \emph{baseline for comparison}, let's start making comparisons to the mean life expectancies of the other four continents and put these values in Table \ref{tab:continent-mean-life-expectancies}, which we'll revisit later on in this section.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  For the Americas, it is 73.6 - 54.8 = 18.8 years higher.
\item
  For Asia, it is 70.7 - 54.8 = 15.9 years higher.
\item
  For Europe, it is 77.6 - 54.8 = 22.8 years higher.
\item
  For Oceania, it is 80.7 - 54.8 = 25.9 years higher.
\end{enumerate}

\begin{table}[!h]

\caption{\label{tab:continent-mean-life-expectancies}Mean life expectancy by continent and relative differences from mean for Africa}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{lrr}
\toprule
continent & mean & Difference versus Africa\\
\midrule
Africa & 54.8 & 0.0\\
Americas & 73.6 & 18.8\\
Asia & 70.7 & 15.9\\
Europe & 77.6 & 22.8\\
Oceania & 80.7 & 25.9\\
\bottomrule
\end{tabular}
\end{table}

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC5.4)} Conduct a new exploratory data analysis with the same explanatory variable \(x\) being \texttt{continent} but with \texttt{gdpPercap} as the new outcome variable \(y\). What can you say about the differences in GDP per capita between continents based on this exploration?

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{model2table}{%
\subsection{Linear regression}\label{model2table}}

In Subsection \ref{model1table} we introduced simple linear regression, which involves modeling the relationship between a numerical outcome variable \(y\) and a numerical explanatory variable \(x\). In our life expectancy example, we now instead have a categorical explanatory variable \texttt{continent}. Our model will not yield a ``best-fitting'' regression line like in Figure \ref{fig:numxplot3}, but rather \emph{offsets} relative to a baseline for comparison.\index{offset}

As we did in Subsection \ref{model1table} when studying the relationship between teaching scores and ``beauty'' scores, let's output the regression table for this model. Recall that this is done in two steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We first ``fit'' the linear regression model using the \texttt{lm(y\ \textasciitilde{}\ x,\ data)} function and save it in \texttt{lifeExp\_model}.
\item
  We get the regression table by applying the \texttt{get\_regression\_table()} function from the \texttt{moderndive} package to \texttt{lifeExp\_model}.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lifeExp\_model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(lifeExp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ continent, }\AttributeTok{data =}\NormalTok{ gapminder2007)}
\FunctionTok{get\_regression\_table}\NormalTok{(lifeExp\_model)}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]

\caption{\label{tab:catxplot4b}Linear regression table}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{lrrrrrr}
\toprule
term & estimate & std\_error & statistic & p\_value & lower\_ci & upper\_ci\\
\midrule
intercept & 54.8 & 1.02 & 53.45 & 0 & 52.8 & 56.8\\
continentAmericas & 18.8 & 1.80 & 10.45 & 0 & 15.2 & 22.4\\
continentAsia & 15.9 & 1.65 & 9.68 & 0 & 12.7 & 19.2\\
continentEurope & 22.8 & 1.70 & 13.47 & 0 & 19.5 & 26.2\\
continentOceania & 25.9 & 5.33 & 4.86 & 0 & 15.4 & 36.5\\
\bottomrule
\end{tabular}
\end{table}

Let's once again focus on the values in the \texttt{term} and \texttt{estimate} columns of Table \ref{tab:catxplot4b}. Why are there now 5 rows? Let's break them down one-by-one:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{intercept} corresponds to the mean life expectancy of countries in Africa of 54.8 years.
\item
  \texttt{continentAmericas} corresponds to countries in the Americas and the value +18.8 is the same difference in mean life expectancy relative to Africa we displayed in Table \ref{tab:continent-mean-life-expectancies}. In other words, the mean life expectancy of countries in the Americas is \(54.8 + 18.8 = 73.6\).
\item
  \texttt{continentAsia} corresponds to countries in Asia and the value +15.9 is the same difference in mean life expectancy relative to Africa we displayed in Table \ref{tab:continent-mean-life-expectancies}. In other words, the mean life expectancy of countries in Asia is \(54.8 + 15.9 = 70.7\).
\item
  \texttt{continentEurope} corresponds to countries in Europe and the value +22.8 is the same difference in mean life expectancy relative to Africa we displayed in Table \ref{tab:continent-mean-life-expectancies}. In other words, the mean life expectancy of countries in Europe is \(54.8 + 22.8 = 77.6\).
\item
  \texttt{continentOceania} corresponds to countries in Oceania and the value +25.9 is the same difference in mean life expectancy relative to Africa we displayed in Table \ref{tab:continent-mean-life-expectancies}. In other words, the mean life expectancy of countries in Oceania is \(54.8 + 25.9 = 80.7\).
\end{enumerate}

To summarize, the 5 values in the \texttt{estimate} column in Table \ref{tab:catxplot4b} correspond to the ``baseline for comparison'' continent Africa (the intercept) as well as four ``offsets'' from this baseline for the remaining 4 continents: the Americas, Asia, Europe, and Oceania.

You might be asking at this point why was Africa chosen as the ``baseline for comparison'' group. This is the case for no other reason than it comes first alphabetically of the five continents; by default R arranges factors/categorical variables in alphanumeric order. You can change this baseline group to be another continent if you manipulate the variable \texttt{continent}'s factor ``levels'' using the \texttt{forcats} package. See \href{https://r4ds.had.co.nz/factors.html}{Chapter 15} of \emph{R for Data Science} \citep{rds2016} for examples.

Let's now write the equation for our fitted values \(\widehat{y} = \widehat{\text{life exp}}\).

\[
\begin{aligned}
\widehat{y} = \widehat{\text{life exp}} &= b_0 + b_{\text{Amer}}\cdot\mathbb{1}_{\text{Amer}}(x) + b_{\text{Asia}}\cdot\mathbb{1}_{\text{Asia}}(x) + \\
& \qquad b_{\text{Euro}}\cdot\mathbb{1}_{\text{Euro}}(x) + b_{\text{Ocean}}\cdot\mathbb{1}_{\text{Ocean}}(x)\\
&= 54.8 + 18.8\cdot\mathbb{1}_{\text{Amer}}(x) + 15.9\cdot\mathbb{1}_{\text{Asia}}(x) + \\
& \qquad 22.8\cdot\mathbb{1}_{\text{Euro}}(x) + 25.9\cdot\mathbb{1}_{\text{Ocean}}(x)
\end{aligned}
\]

Whoa! That looks daunting! Don't fret, however, as once you understand what all the elements mean, things simplify greatly. First, \(\mathbb{1}_{A}(x)\) is what's known in mathematics as an ``indicator function.'' It returns only one of two possible values, 0 and 1, where

\[
\mathbb{1}_{A}(x) = \left\{
\begin{array}{ll}
1 & \text{if } x \text{ is in } A \\
0 & \text{if } \text{otherwise} \end{array}
\right.
\]

In a statistical modeling context, this is also known as a \emph{dummy variable}. \index{dummy variable} In our case, let's consider the first such indicator variable \(\mathbb{1}_{\text{Amer}}(x)\). This indicator function returns 1 if a country is in the Americas, 0 otherwise:

\[
\mathbb{1}_{\text{Amer}}(x) = \left\{
\begin{array}{ll}
1 & \text{if } \text{country } x \text{ is in the Americas} \\
0 & \text{otherwise}\end{array}
\right.
\]

Second, \(b_0\) corresponds to the intercept as before; in this case, it's the mean life expectancy of all countries in Africa. Third, the \(b_{\text{Amer}}\), \(b_{\text{Asia}}\), \(b_{\text{Euro}}\), and \(b_{\text{Ocean}}\) represent the 4 ``offsets relative to the baseline for comparison'' in the regression table output in Table \ref{tab:catxplot4b}: \texttt{continentAmericas}, \texttt{continentAsia}, \texttt{continentEurope}, and \texttt{continentOceania}.

Let's put this all together and compute the fitted value \(\widehat{y} = \widehat{\text{life exp}}\) for a country in Africa. Since the country is in Africa, all four indicator functions \(\mathbb{1}_{\text{Amer}}(x)\), \(\mathbb{1}_{\text{Asia}}(x)\), \(\mathbb{1}_{\text{Euro}}(x)\), and \(\mathbb{1}_{\text{Ocean}}(x)\) will equal 0, and thus:

\[
\begin{aligned}
\widehat{\text{life exp}} &= b_0 + b_{\text{Amer}}\cdot\mathbb{1}_{\text{Amer}}(x) + b_{\text{Asia}}\cdot\mathbb{1}_{\text{Asia}}(x)
+ \\
& \qquad b_{\text{Euro}}\cdot\mathbb{1}_{\text{Euro}}(x) + b_{\text{Ocean}}\cdot\mathbb{1}_{\text{Ocean}}(x)\\
&= 54.8 + 18.8\cdot\mathbb{1}_{\text{Amer}}(x) + 15.9\cdot\mathbb{1}_{\text{Asia}}(x)
+ \\
& \qquad 22.8\cdot\mathbb{1}_{\text{Euro}}(x) + 25.9\cdot\mathbb{1}_{\text{Ocean}}(x)\\
&= 54.8 + 18.8\cdot 0 + 15.9\cdot 0 + 22.8\cdot 0 + 25.9\cdot 0\\
&= 54.8
\end{aligned}
\]

In other words, all that's left is the intercept \(b_0\), corresponding to the average life expectancy of African countries of 54.8 years. Next, say we are considering a country in the Americas. In this case, only the indicator function \(\mathbb{1}_{\text{Amer}}(x)\) for the Americas will equal 1, while all the others will equal 0, and thus:

\[
\begin{aligned}
\widehat{\text{life exp}} &= 54.8 + 18.8\cdot\mathbb{1}_{\text{Amer}}(x) + 15.9\cdot\mathbb{1}_{\text{Asia}}(x)
+ 22.8\cdot\mathbb{1}_{\text{Euro}}(x) + \\
& \qquad 25.9\cdot\mathbb{1}_{\text{Ocean}}(x)\\
&= 54.8 + 18.8\cdot 1 + 15.9\cdot 0 + 22.8\cdot 0 + 25.9\cdot 0\\
&= 54.8 + 18.8 \\
& = 73.6
\end{aligned}
\]

which is the mean life expectancy for countries in the Americas of 73.6 years in Table \ref{tab:continent-mean-life-expectancies}. Note the ``offset from the baseline for comparison'' is +18.8 years.

Let's do one more. Say we are considering a country in Asia. In this case, only the indicator function \(\mathbb{1}_{\text{Asia}}(x)\) for Asia will equal 1, while all the others will equal 0, and thus:

\[
\begin{aligned}
\widehat{\text{life exp}} &= 54.8 + 18.8\cdot\mathbb{1}_{\text{Amer}}(x) + 15.9\cdot\mathbb{1}_{\text{Asia}}(x)
+ 22.8\cdot\mathbb{1}_{\text{Euro}}(x) + \\
& \qquad 25.9\cdot\mathbb{1}_{\text{Ocean}}(x)\\
&= 54.8 + 18.8\cdot 0 + 15.9\cdot 1 + 22.8\cdot 0 + 25.9\cdot 0\\
&= 54.8 + 15.9 \\
& = 70.7
\end{aligned}
\]

which is the mean life expectancy for Asian countries of 70.7 years in Table \ref{tab:continent-mean-life-expectancies}. The ``offset from the baseline for comparison'' here is +15.9 years.

Let's generalize this idea a bit. If we fit a linear regression model using a categorical explanatory variable \(x\) that has \(k\) possible categories, the regression table will return an intercept and \(k - 1\) ``offsets.'' In our case, since there are \(k = 5\) continents, the regression model returns an intercept corresponding to the baseline for comparison group of Africa and \(k - 1 = 4\) offsets corresponding to the Americas, Asia, Europe, and Oceania.

Understanding a regression table output when you're using a categorical explanatory variable is a topic those new to regression often struggle with. The only real remedy for these struggles is practice, practice, practice. However, once you equip yourselves with an understanding of how to create regression models using categorical explanatory variables, you'll be able to incorporate many new variables into your models, given the large amount of the world's data that is categorical. If you feel like you're still struggling at this point, however, we suggest you closely compare Tables \ref{tab:continent-mean-life-expectancies} and \ref{tab:catxplot4b} and note how you can compute all the values from one table using the values in the other.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC5.5)} Fit a new linear regression using \texttt{lm(gdpPercap\ \textasciitilde{}\ continent,\ data\ =\ gapminder2007)} where \texttt{gdpPercap} is the new outcome variable \(y\). Get information about the ``best-fitting'' line from the regression table by applying the \texttt{get\_regression\_table()} function. How do the regression results match up with the results from your previous exploratory data analysis?

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{model2points}{%
\subsection{Observed/fitted values and residuals}\label{model2points}}

Recall in Subsection \ref{model1points}, we defined the following three concepts:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Observed values \(y\), or the observed value of the outcome variable \index{regression!observed values}
\item
  Fitted values \(\widehat{y}\), or the value on the regression line for a given \(x\) value
\item
  Residuals \(y - \widehat{y}\), or the error between the observed value and the fitted value
\end{enumerate}

We obtained these values and other values using the \texttt{get\_regression\_points()} function from the \texttt{moderndive} package. This time, however, let's add an argument setting \texttt{ID\ =\ "country"}: this is telling the function to use the variable \texttt{country} in \texttt{gapminder2007} as an \emph{identification variable} in the output. This will help contextualize our analysis by matching values to countries.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{regression\_points }\OtherTok{\textless{}{-}} \FunctionTok{get\_regression\_points}\NormalTok{(lifeExp\_model, }\AttributeTok{ID =} \StringTok{"country"}\NormalTok{)}
\NormalTok{regression\_points}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]

\caption{\label{tab:model2-residuals}Regression points (First 10 out of 142 countries)}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{lrlrr}
\toprule
country & lifeExp & continent & lifeExp\_hat & residual\\
\midrule
Afghanistan & 43.8 & Asia & 70.7 & -26.900\\
Albania & 76.4 & Europe & 77.6 & -1.226\\
Algeria & 72.3 & Africa & 54.8 & 17.495\\
Angola & 42.7 & Africa & 54.8 & -12.075\\
Argentina & 75.3 & Americas & 73.6 & 1.712\\
Australia & 81.2 & Oceania & 80.7 & 0.516\\
Austria & 79.8 & Europe & 77.6 & 2.180\\
Bahrain & 75.6 & Asia & 70.7 & 4.907\\
Bangladesh & 64.1 & Asia & 70.7 & -6.666\\
Belgium & 79.4 & Europe & 77.6 & 1.792\\
\bottomrule
\end{tabular}
\end{table}

Observe in Table \ref{tab:model2-residuals} that \texttt{lifeExp\_hat} contains the fitted values \(\widehat{y}\) = \(\widehat{\text{lifeExp}}\). If you look closely, there are only 5 possible values for \texttt{lifeExp\_hat}. These correspond to the five mean life expectancies for the 5 continents that we displayed in Table \ref{tab:continent-mean-life-expectancies} and computed using the values in the \texttt{estimate} column of the regression table in Table \ref{tab:catxplot4b}.

The \texttt{residual} column is simply \(y - \widehat{y}\) = \texttt{lifeExp\ -\ lifeExp\_hat}. These values can be interpreted as the deviation of a country's life expectancy from its continent's average life expectancy. For example, look at the first row of Table \ref{tab:model2-residuals} corresponding to Afghanistan. The residual of \(y - \widehat{y} = 43.8 - 70.7 = -26.9\) is telling us that Afghanistan's life expectancy is a whopping 26.9 years lower than the mean life expectancy of all Asian countries. This can in part be explained by the many years of war that country has suffered.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC5.6)} Using either the sorting functionality of RStudio's spreadsheet viewer or using the data wrangling tools you learned in Chapter \ref{wrangling}, identify the five countries with the five smallest (most negative) residuals? What do these negative residuals say about their life expectancy relative to their continents' life expectancy?

\textbf{(LC5.7)} Repeat this process, but identify the five countries with the five largest (most positive) residuals. What do these positive residuals say about their life expectancy relative to their continents' life expectancy?

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{reg-related-topics}{%
\section{Related topics}\label{reg-related-topics}}

\hypertarget{correlation-is-not-causation}{%
\subsection{Correlation is not necessarily causation}\label{correlation-is-not-causation}}

Throughout this chapter we've been cautious when interpreting regression slope coefficients. We always discussed the ``associated'' effect of an explanatory variable \(x\) on an outcome variable \(y\). For example, our statement from Subsection \ref{model1table} that ``for every increase of 1 unit in \texttt{bty\_avg}, there is an \emph{associated} increase of on average 0.067 units of \texttt{score}.'' We include the term ``associated'' to be extra careful not to suggest we are making a \emph{causal} statement. So while ``beauty'' score of \texttt{bty\_avg} is positively correlated with teaching \texttt{score}, we can't necessarily make any statements about ``beauty'' scores' direct causal effect on teaching score without more information on how this study was conducted. Here is another example: a not-so-great medical doctor goes through medical records and finds that patients who slept with their shoes on tended to wake up more with headaches. So this doctor declares, ``Sleeping with shoes on causes headaches!''

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth,height=0.6\textheight]{images/shutterstock/shoes_headache} 

}

\caption{Does sleeping with shoes on cause headaches?}\label{fig:moderndive-figure-causal-graph-2}
\end{figure}

However, there is a good chance that if someone is sleeping with their shoes on, it's potentially because they are intoxicated from alcohol. Furthermore, higher levels of drinking leads to more hangovers, and hence more headaches. The amount of alcohol consumption here is what's known as a \emph{confounding/lurking} variable\index{confounding variable}. It ``lurks'' behind the scenes, confounding the causal relationship (if any) of ``sleeping with shoes on'' with ``waking up with a headache.'' We can summarize this in Figure \ref{fig:moderndive-figure-causal-graph} with a \emph{causal graph} where:

\begin{itemize}
\tightlist
\item
  Y is a \emph{response} variable; here it is ``waking up with a headache.'' \index{variables!response / outcome / dependent}
\item
  X is a \emph{treatment} variable whose causal effect we are interested in; here it is ``sleeping with shoes on.''\index{variables!treatment}
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/flowcharts/flowchart.009-cropped} 

}

\caption{Causal graph.}\label{fig:moderndive-figure-causal-graph}
\end{figure}

To study the relationship between Y and X, we could use a regression model where the outcome variable is set to Y and the explanatory variable is set to be X, as you've been doing throughout this chapter. However, Figure \ref{fig:moderndive-figure-causal-graph} also includes a third variable with arrows pointing at both X and Y:

\begin{itemize}
\tightlist
\item
  Z is a \emph{confounding} variable \index{variables!confounding} that affects both X and Y, thereby ``confounding'' their relationship. Here the confounding variable is alcohol.
\end{itemize}

Alcohol will cause people to be both more likely to sleep with their shoes on as well as be more likely to wake up with a headache. Thus any regression model of the relationship between X and Y should also use Z as an explanatory variable. In other words, our doctor needs to take into account who had been drinking the night before. In the next chapter, we'll start covering multiple regression models that allow us to incorporate more than one variable in our regression models.

Establishing causation is a tricky problem and frequently takes either carefully designed experiments or methods to control for the effects of confounding variables. Both these approaches attempt, as best they can, either to take all possible confounding variables into account or negate their impact. This allows researchers to focus only on the relationship of interest: the relationship between the outcome variable Y and the treatment variable X.

As you read news stories, be careful not to fall into the trap of thinking that correlation necessarily implies causation. Check out the \href{http://www.tylervigen.com/spurious-correlations}{Spurious Correlations} website for some rather comical examples of variables that are correlated, but are definitely not causally related.

\hypertarget{leastsquares}{%
\subsection{Best-fitting line}\label{leastsquares}}

Regression lines are also known as ``best-fitting'' lines. But what do we mean by ``best''? Let's unpack the criteria that is used in regression to determine ``best.'' Recall Figure \ref{fig:numxplot4}, where for an instructor with a beauty score of \(x = 7.333\) we mark the \emph{observed value} \(y\) with a circle, the \emph{fitted value} \(\widehat{y}\) with a square, and the \emph{residual} \(y - \widehat{y}\) with an arrow. We re-display Figure \ref{fig:numxplot4} in the top-left plot of Figure \ref{fig:best-fitting-line} in addition to three more arbitrarily chosen course instructors:

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/best-fitting-line-1} 

}

\caption{Example of observed value, fitted value, and residual.}\label{fig:best-fitting-line}
\end{figure}

The three other plots refer to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A course whose instructor had a ``beauty'' score \(x\) = 2.333 and teaching score \(y\) = 2.7. The residual in this case is \(2.7 - 4.036 = -1.336\), which we mark with a new arrow in the top-right plot.
\item
  A course whose instructor had a ``beauty'' score \(x = 3.667\) and teaching score \(y = 4.4\). The residual in this case is \(4.4 - 4.125 = 0.2753\), which we mark with a new arrow in the bottom-left plot.
\item
  A course whose instructor had a ``beauty'' score \(x = 6\) and teaching score \(y = 3.8\). The residual in this case is \(3.8 - 4.28 = -0.4802\), which we mark with a new arrow in the bottom-right plot.
\end{enumerate}

Now say we repeated this process of computing residuals for all 463 courses' instructors, then we squared all the residuals, and then we summed them. We call this quantity the \emph{sum of squared residuals}\index{sum of squared residuals}; it is a measure of the \emph{lack of fit} of a model. Larger values of the sum of squared residuals indicate a bigger lack of fit. This corresponds to a worse fitting model.

If the regression line fits all the points perfectly, then the sum of squared residuals is 0. This is because if the regression line fits all the points perfectly, then the fitted value \(\widehat{y}\) equals the observed value \(y\) in all cases, and hence the residual \(y-\widehat{y}\) = 0 in all cases, and the sum of even a large number of 0's is still 0.

Furthermore, of all possible lines we can draw through the cloud of 463 points, the regression line minimizes this value. In other words, the regression and its corresponding fitted values \(\widehat{y}\) minimizes the sum of the squared residuals:

\[
\sum_{i=1}^{n}(y_i - \widehat{y}_i)^2
\]

Let's use our data wrangling tools from Chapter \ref{wrangling} to compute the sum of squared residuals exactly:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit regression model:}
\NormalTok{score\_model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(score }\SpecialCharTok{\textasciitilde{}}\NormalTok{ bty\_avg, }
                  \AttributeTok{data =}\NormalTok{ evals\_ch5)}

\CommentTok{\# Get regression points:}
\NormalTok{regression\_points }\OtherTok{\textless{}{-}} \FunctionTok{get\_regression\_points}\NormalTok{(score\_model)}
\NormalTok{regression\_points}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 463 x 5
      ID score bty_avg score_hat residual
   <int> <dbl>   <dbl>     <dbl>    <dbl>
 1     1 4.7   5           4.214  0.486  
 2     2 4.100 5           4.214 -0.114  
 3     3 3.9   5           4.214 -0.314  
 4     4 4.8   5           4.214  0.586  
 5     5 4.600 3           4.08   0.52   
 6     6 4.3   3           4.08   0.22   
 7     7 2.8   3           4.08  -1.28   
 8     8 4.100 3.333       4.102 -0.002  
 9     9 3.4   3.333       4.102 -0.702  
10    10 4.5   3.16700     4.091  0.40900
# ... with 453 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Compute sum of squared residuals}
\NormalTok{regression\_points }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{squared\_residuals =}\NormalTok{ residual}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{sum\_of\_squared\_residuals =} \FunctionTok{sum}\NormalTok{(squared\_residuals))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 1
  sum_of_squared_residuals
                     <dbl>
1                  131.879
\end{verbatim}

Any other straight line drawn in the figure would yield a sum of squared residuals greater than 132. This is a mathematically guaranteed fact that you can prove using calculus and linear algebra. That's why alternative names for the linear regression line are the \emph{best-fitting line} and the \emph{least-squares line}. Why do we square the residuals (i.e., the arrow lengths)? So that both positive and negative deviations of the same amount are treated equally.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC5.8)} Note in Figure \ref{fig:three-lines} there are 3 points marked with dots and:

\begin{itemize}
\tightlist
\item
  The ``best'' fitting solid regression line
\item
  An arbitrarily chosen dotted line
\item
  Another arbitrarily chosen dashed line
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.85\linewidth]{HR_Analytics.live_files/figure-latex/three-lines-1} 

}

\caption{Regression line and two others.}\label{fig:three-lines}
\end{figure}

Compute the sum of squared residuals by hand for each line and show that of these three lines, the regression line has the smallest value.

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{underthehood}{%
\subsection{\texorpdfstring{\texttt{get\_regression\_x()} functions}{get\_regression\_x() functions}}\label{underthehood}}

Recall in this chapter we introduced two functions from the \texttt{moderndive} package:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{get\_regression\_table()} that returns a regression table in Subsection \ref{model1table} and
\item
  \texttt{get\_regression\_points()} that returns point-by-point information from a regression model in Subsection \ref{model1points}.
\end{enumerate}

What is going on behind the scenes with the \texttt{get\_regression\_table()} and \texttt{get\_regression\_points()} functions? We mentioned in Subsection \ref{model1table} that these were examples of \emph{wrapper functions}. Such functions take other pre-existing functions and ``wrap'' them into single functions that hide the user from their inner workings. This way all the user needs to worry about is what the inputs look like and what the outputs look like. In this subsection, we'll ``get under the hood'' of these functions and see how the ``engine'' of these wrapper functions works.

Recall our two-step process to generate a regression table from Subsection \ref{model1table}:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit regression model:}
\NormalTok{score\_model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ score }\SpecialCharTok{\textasciitilde{}}\NormalTok{ bty\_avg, }\AttributeTok{data =}\NormalTok{ evals\_ch5)}
\CommentTok{\# Get regression table:}
\FunctionTok{get\_regression\_table}\NormalTok{(score\_model)}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]

\caption{\label{tab:recall-table}Regression table}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{lrrrrrr}
\toprule
term & estimate & std\_error & statistic & p\_value & lower\_ci & upper\_ci\\
\midrule
intercept & 3.880 & 0.076 & 50.96 & 0 & 3.731 & 4.030\\
bty\_avg & 0.067 & 0.016 & 4.09 & 0 & 0.035 & 0.099\\
\bottomrule
\end{tabular}
\end{table}

The \texttt{get\_regression\_table()} wrapper function takes two pre-existing functions in other R packages:

\begin{itemize}
\tightlist
\item
  \texttt{tidy()} \index{R packages!broom!tidy()} from the \href{https://broom.tidyverse.org/}{\texttt{broom} package} \citep{R-broom} and
\item
  \texttt{clean\_names()} \index{R packages!janitor!clean\_names()} from the \href{https://github.com/sfirke/janitor}{\texttt{janitor} package} \citep{R-janitor}
\end{itemize}

and ``wraps'' them into a single function that takes in a saved \texttt{lm()} linear model, here \texttt{score\_model}, and returns a regression table saved as a ``tidy'' data frame. Here is how we used the \texttt{tidy()} and \texttt{clean\_names()} functions to produce Table \ref{tab:regtable-broom}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(broom)}
\FunctionTok{library}\NormalTok{(janitor)}
\NormalTok{score\_model }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tidy}\NormalTok{(}\AttributeTok{conf.int =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate\_if}\NormalTok{(is.numeric, round, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{clean\_names}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{lower\_ci =}\NormalTok{ conf\_low, }\AttributeTok{upper\_ci =}\NormalTok{ conf\_high)}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]

\caption{\label{tab:regtable-broom}Regression table using tidy() from broom package}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{lrrrrrr}
\toprule
term & estimate & std\_error & statistic & p\_value & lower\_ci & upper\_ci\\
\midrule
(Intercept) & 3.880 & 0.076 & 50.96 & 0 & 3.731 & 4.030\\
bty\_avg & 0.067 & 0.016 & 4.09 & 0 & 0.035 & 0.099\\
\bottomrule
\end{tabular}
\end{table}

Yikes! That's a lot of code! So, in order to simplify your lives, we made the editorial decision to ``wrap'' all the code into \texttt{get\_regression\_table()}, freeing you from the need to understand the inner workings of the function. Note that the \texttt{mutate\_if()} function is from the \texttt{dplyr} package and applies the \texttt{round()} function to three significant digits precision only to those variables that are numerical.

Similarly, the \texttt{get\_regression\_points()} function is another wrapper function, but this time returning information about the individual points involved in a regression model like the fitted values, observed values, and the residuals. \texttt{get\_regression\_points()} \index{moderndive!get\_regression\_points()} uses the \texttt{augment()} \index{R packages!broom!augment()} function in the \href{https://broom.tidyverse.org/}{\texttt{broom} package} instead of the \texttt{tidy()} function as with \texttt{get\_regression\_table()} to produce the data shown in Table \ref{tab:regpoints-augment}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(broom)}
\FunctionTok{library}\NormalTok{(janitor)}
\NormalTok{score\_model }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{augment}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate\_if}\NormalTok{(is.numeric, round, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{clean\_names}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(}\StringTok{"std\_resid"}\NormalTok{, }\StringTok{"hat"}\NormalTok{, }\StringTok{"sigma"}\NormalTok{, }\StringTok{"cooksd"}\NormalTok{, }\StringTok{"std\_resid"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]

\caption{\label{tab:regpoints-augment}Regression points using augment() from broom package}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{rrrr}
\toprule
score & bty\_avg & fitted & resid\\
\midrule
4.7 & 5.00 & 4.21 & 0.486\\
4.1 & 5.00 & 4.21 & -0.114\\
3.9 & 5.00 & 4.21 & -0.314\\
4.8 & 5.00 & 4.21 & 0.586\\
4.6 & 3.00 & 4.08 & 0.520\\
4.3 & 3.00 & 4.08 & 0.220\\
2.8 & 3.00 & 4.08 & -1.280\\
4.1 & 3.33 & 4.10 & -0.002\\
3.4 & 3.33 & 4.10 & -0.702\\
4.5 & 3.17 & 4.09 & 0.409\\
\bottomrule
\end{tabular}
\end{table}

In this case, it outputs only the variables of interest to students learning regression: the outcome variable \(y\) (\texttt{score}), all explanatory/predictor variables (\texttt{bty\_avg}), all resulting \texttt{fitted} values \(\hat{y}\) used by applying the equation of the regression line to \texttt{bty\_avg}, and the \texttt{resid}ual \(y - \hat{y}\).

If you're even more curious about how these and other wrapper functions work, take a look at the source code for these functions on \href{https://github.com/moderndive/moderndive/blob/master/R/regression_functions.R}{GitHub}.

\hypertarget{reg-conclusion}{%
\section{Conclusion}\label{reg-conclusion}}

\hypertarget{additional-resources-basic-regression}{%
\subsection{Additional resources}\label{additional-resources-basic-regression}}

Solutions to all \emph{Learning checks} can be found online in \href{https://moderndive.com/D-appendixD.html}{Appendix D}.

An R script file of all R code used in this chapter is available at \url{https://www.moderndive.com/scripts/05-regression.R}.

As we suggested in Subsection \ref{model1EDA}, interpreting coefficients that are not close to the extreme values of -1, 0, and 1 can be somewhat subjective. To help develop your sense of correlation coefficients, we suggest you play the 80s-style video game called, ``Guess the Correlation'', at \url{http://guessthecorrelation.com/}.



\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/copyright/guess_the_correlation} 

}

\caption{Preview of ``Guess the Correlation'' game.}\label{fig:guess-the-correlation}
\end{figure}

\hypertarget{whats-to-come-4}{%
\subsection{What's to come?}\label{whats-to-come-4}}

In this chapter, you've studied the term \emph{basic regression}, where you fit models that only have one explanatory variable. In Chapter \ref{multiple-regression}, we'll study \emph{multiple regression}, where our regression models can now have more than one explanatory variable! In particular, we'll consider two scenarios: regression models with one numerical and one categorical explanatory variable and regression models with two numerical explanatory variables. This will allow you to construct more sophisticated and more powerful models, all in the hopes of better explaining your outcome variable \(y\).

\hypertarget{multiple-regression}{%
\chapter{Multiple Regression}\label{multiple-regression}}

In Chapter \ref{regression} we introduced ideas related to modeling for explanation, in particular that the goal of modeling is to make explicit the relationship between some outcome variable \(y\) and some explanatory variable \(x\). While there are many approaches to modeling, we focused on one particular technique: \emph{linear regression}, one of the most commonly used and easy-to-understand approaches to modeling. Furthermore to keep things simple, we only considered models with one explanatory \(x\) variable that was either numerical in Section \ref{model1} or categorical in Section \ref{model2}.

In this chapter on multiple regression, we'll start considering models that include more than one explanatory variable \(x\). You can imagine when trying to model a particular outcome variable, like teaching evaluation scores as in Section \ref{model1} or life expectancy as in Section \ref{model2}, that it would be useful to include more than just one explanatory variable's worth of information.

Since our regression models will now consider more than one explanatory variable, the interpretation of the associated effect of any one explanatory variable must be made in conjunction with the other explanatory variables included in your model. Let's begin!

\hypertarget{mult-reg-packages}{%
\subsection*{Needed packages}\label{mult-reg-packages}}


Let's load all the packages needed for this chapter (this assumes you've already installed them). Recall from our discussion in Section \ref{tidyverse-package} that loading the \texttt{tidyverse} package by running \texttt{library(tidyverse)} loads the following commonly used data science packages all at once:

\begin{itemize}
\tightlist
\item
  \texttt{ggplot2} for data visualization
\item
  \texttt{dplyr} for data wrangling
\item
  \texttt{tidyr} for converting data to ``tidy'' format
\item
  \texttt{readr} for importing spreadsheet data into R
\item
  As well as the more advanced \texttt{purrr}, \texttt{tibble}, \texttt{stringr}, and \texttt{forcats} packages
\end{itemize}

If needed, read Section \ref{packages} for information on how to install and load R packages.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(moderndive)}
\FunctionTok{library}\NormalTok{(skimr)}
\FunctionTok{library}\NormalTok{(ISLR)}
\end{Highlighting}
\end{Shaded}

\hypertarget{model4}{%
\section{One numerical and one categorical explanatory variable}\label{model4}}

Let's revisit the instructor evaluation data from UT Austin we introduced in Section \ref{model1}. We studied the relationship between teaching evaluation scores as given by students and ``beauty'' scores. The variable teaching \texttt{score} was the numerical outcome variable \(y\), and the variable ``beauty'' score (\texttt{bty\_avg}) was the numerical explanatory \(x\) variable.

In this section, we are going to consider a different model. Our outcome variable will still be teaching score, but we'll now include two different explanatory variables: age and (binary) gender. Could it be that instructors who are older receive better teaching evaluations from students? Or could it instead be that younger instructors receive better evaluations? Are there differences in evaluations given by students for instructors of different genders? We'll answer these questions by modeling the relationship between these variables using \emph{multiple regression},\index{regression!multiple linear} where we have:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A numerical outcome variable \(y\), the instructor's teaching score, and
\item
  Two explanatory variables:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    A numerical explanatory variable \(x_1\), the instructor's age.
  \item
    A categorical explanatory variable \(x_2\), the instructor's (binary) gender.
  \end{enumerate}
\end{enumerate}

It is important to note that at the time of this study due to then commonly held beliefs about gender, this variable was often recorded as a binary variable. While the results of a model that oversimplifies gender this way may be imperfect, we still found the results to be pertinent and relevant today.

\hypertarget{model4EDA}{%
\subsection{Exploratory data analysis}\label{model4EDA}}

Recall that data on the 463 courses at UT Austin can be found in the \texttt{evals} data frame included in the \texttt{moderndive} package. However, to keep things simple, let's \texttt{select()} only the subset of the variables we'll consider in this chapter, and save this data in a new data frame called \texttt{evals\_ch6}. Note that these are different than the variables chosen in Chapter \ref{regression}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{evals\_ch6 }\OtherTok{\textless{}{-}}\NormalTok{ evals }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(ID, score, age, gender)}
\end{Highlighting}
\end{Shaded}

Recall the three common steps in an exploratory data analysis we saw in Subsection \ref{model1EDA}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Looking at the raw data values.
\item
  Computing summary statistics.
\item
  Creating data visualizations.
\end{enumerate}

Let's first look at the raw data values by either looking at \texttt{evals\_ch6} using RStudio's spreadsheet viewer or by using the \texttt{glimpse()} function from the \texttt{dplyr} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(evals\_ch6)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 463
Columns: 4
$ ID     <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, ...
$ score  <dbl> 4.7, 4.1, 3.9, 4.8, 4.6, 4.3, 2.8, 4.1, 3.4, 4.5, 3.8, ...
$ age    <int> 36, 36, 36, 36, 59, 59, 59, 51, 51, 40, 40, 40, 40, 40,...
$ gender <fct> female, female, female, female, male, male, male, male,...
\end{verbatim}

Let's also display a random sample of 5 rows of the 463 rows corresponding to different courses in Table \ref{tab:model4-data-preview}. Remember due to the random nature of the sampling, you will likely end up with a different subset of 5 rows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{evals\_ch6 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{sample\_n}\NormalTok{(}\AttributeTok{size =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]

\caption{\label{tab:model4-data-preview}A random sample of 5 out of the 463 courses at UT Austin}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{rrrl}
\toprule
ID & score & age & gender\\
\midrule
129 & 3.7 & 62 & male\\
109 & 4.7 & 46 & female\\
28 & 4.8 & 62 & male\\
434 & 2.8 & 62 & male\\
330 & 4.0 & 64 & male\\
\bottomrule
\end{tabular}
\end{table}

Now that we've looked at the raw values in our \texttt{evals\_ch6} data frame and got a sense of the data, let's compute summary statistics. As we did in our exploratory data analyses in Sections \ref{model1EDA} and \ref{model2EDA} from the previous chapter, let's use the \texttt{skim()} function from the \texttt{skimr} package, being sure to only \texttt{select()} the variables of interest in our model:\index{R packages!skimr!skim()}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{evals\_ch6 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(score, age, gender) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{skim}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Skim summary statistics
 n obs: 463 
 n variables: 3 

── Variable type:factor 
 variable missing complete   n n_unique                top_counts ordered
   gender       0      463 463        2 mal: 268, fem: 195, NA: 0   FALSE

── Variable type:integer 
 variable missing complete   n  mean  sd p0 p25 p50 p75 p100
      age       0      463 463 48.37 9.8 29  42  48  57   73

── Variable type:numeric
 variable missing complete   n mean   sd  p0 p25 p50 p75 p100
    score       0      463 463 4.17 0.54 2.3 3.8 4.3 4.6    5
\end{verbatim}

Observe that we have no missing data, that there are 268 courses taught by male instructors and 195 courses taught by female instructors, and that the average instructor age is 48.37. Recall that each row represents a particular course and that the same instructor often teaches more than one course. Therefore, the average age of the unique instructors may differ.

Furthermore, let's compute the correlation coefficient between our two numerical variables: \texttt{score} and \texttt{age}. Recall from Subsection \ref{model1EDA} that correlation coefficients only exist between numerical variables. We observe that they are ``weakly negatively'' correlated.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{evals\_ch6 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{get\_correlation}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ score }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 1
        cor
      <dbl>
1 -0.107032
\end{verbatim}

Let's now perform the last of the three common steps in an exploratory data analysis: creating data visualizations. Given that the outcome variable \texttt{score} and explanatory variable \texttt{age} are both numerical, we'll use a scatterplot to display their relationship. How can we incorporate the categorical variable \texttt{gender}, however? By \texttt{mapping} the variable \texttt{gender} to the \texttt{color} aesthetic, thereby creating a \emph{colored} scatterplot. The following code is similar to the code that created the scatterplot of teaching score over ``beauty'' score in Figure \ref{fig:numxplot1}, but with \texttt{color\ =\ gender} added to the \texttt{aes()}thetic mapping.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(evals\_ch6, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{y =}\NormalTok{ score, }\AttributeTok{color =}\NormalTok{ gender)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Teaching Score"}\NormalTok{, }\AttributeTok{color =} \StringTok{"Gender"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/numxcatxplot1-1} 

}

\caption{Colored scatterplot of relationship of teaching and beauty scores.}\label{fig:numxcatxplot1}
\end{figure}

In the resulting Figure \ref{fig:numxcatxplot1}, observe that \texttt{ggplot()} assigns a default color scheme to the points and to the lines associated with the two levels of \texttt{gender}: \texttt{female} and \texttt{male}. Furthermore, the \texttt{geom\_smooth(method\ =\ "lm",\ se\ =\ FALSE)} layer automatically fits a different regression line for each group.

We notice some interesting trends. First, there are almost no women faculty over the age of 60 as evidenced by lack of darker-colored dots above \(x\) = 60. Second, while both regression lines are negatively sloped with age (i.e., older instructors tend to have lower scores), the slope for age for the female instructors is \emph{more} negative. In other words, female instructors are paying a harsher penalty for advanced age than the male instructors.

\hypertarget{model4interactiontable}{%
\subsection{Interaction model}\label{model4interactiontable}}

Let's now quantify the relationship of our outcome variable \(y\) and the two explanatory variables using one type of multiple regression model known as an \emph{interaction model}. \index{interaction model} We'll explain where the term ``interaction'' comes from at the end of this section.

In particular, we'll write out the equation of the two regression lines in Figure \ref{fig:numxcatxplot1} using the values from a regression table. Before we do this, however, let's go over a brief refresher of regression when you have a categorical explanatory variable \(x\).

Recall in Subsection \ref{model2table} we fit a regression model for countries' life expectancies as a function of which continent the country was in. In other words, we had a numerical outcome variable \(y\) = \texttt{lifeExp} and a categorical explanatory variable \(x\) = \texttt{continent} which had 5 levels: \texttt{Africa}, \texttt{Americas}, \texttt{Asia}, \texttt{Europe}, and \texttt{Oceania}. Let's re-display the regression table you saw in Table \ref{tab:catxplot4b}:

\begin{table}[!h]

\caption{\label{tab:unnamed-chunk-220}Regression table for life expectancy as a function of continent}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{lrrrrrr}
\toprule
term & estimate & std\_error & statistic & p\_value & lower\_ci & upper\_ci\\
\midrule
intercept & 54.8 & 1.02 & 53.45 & 0 & 52.8 & 56.8\\
continentAmericas & 18.8 & 1.80 & 10.45 & 0 & 15.2 & 22.4\\
continentAsia & 15.9 & 1.65 & 9.68 & 0 & 12.7 & 19.2\\
continentEurope & 22.8 & 1.70 & 13.47 & 0 & 19.5 & 26.2\\
continentOceania & 25.9 & 5.33 & 4.86 & 0 & 15.4 & 36.5\\
\bottomrule
\end{tabular}
\end{table}

Recall our interpretation of the \texttt{estimate} column. Since \texttt{Africa} was the ``baseline for comparison'' group, the \texttt{intercept} term corresponds to the mean life expectancy for all countries in Africa of 54.8 years. The other four values of \texttt{estimate} correspond to ``offsets'' relative to the baseline group. So, for example, the ``offset'' corresponding to the Americas is +18.8 as compared to the baseline for comparison group Africa. In other words, the average life expectancy for countries in the Americas is 18.8 years \emph{higher}. Thus the mean life expectancy for all countries in the Americas is 54.8 + 18.8 = 73.6. The same interpretation holds for Asia, Europe, and Oceania.

Going back to our multiple regression model for teaching \texttt{score} using \texttt{age} and \texttt{gender} in Figure \ref{fig:numxcatxplot1}, we generate the regression table using the same two-step approach from Chapter \ref{regression}: we first ``fit'' the model using the \texttt{lm()} ``linear model'' function and then we apply the \texttt{get\_regression\_table()} function. This time, however, our model formula won't be of the form \texttt{y\ \textasciitilde{}\ x}, but rather of the form \texttt{y\ \textasciitilde{}\ x1\ *\ x2}. In other words, our two explanatory variables \texttt{x1} and \texttt{x2} are separated by a \texttt{*} sign:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit regression model:}
\NormalTok{score\_model\_interaction }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(score }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{*}\NormalTok{ gender, }\AttributeTok{data =}\NormalTok{ evals\_ch6)}

\CommentTok{\# Get regression table:}
\FunctionTok{get\_regression\_table}\NormalTok{(score\_model\_interaction)}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]

\caption{\label{tab:regtable-interaction}Regression table for interaction model}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{lrrrrrr}
\toprule
term & estimate & std\_error & statistic & p\_value & lower\_ci & upper\_ci\\
\midrule
intercept & 4.883 & 0.205 & 23.80 & 0.000 & 4.480 & 5.286\\
age & -0.018 & 0.004 & -3.92 & 0.000 & -0.026 & -0.009\\
gendermale & -0.446 & 0.265 & -1.68 & 0.094 & -0.968 & 0.076\\
age:gendermale & 0.014 & 0.006 & 2.45 & 0.015 & 0.003 & 0.024\\
\bottomrule
\end{tabular}
\end{table}

Looking at the regression table output in Table \ref{tab:regtable-interaction}, there are four rows of values in the \texttt{estimate} column. While it is not immediately apparent, using these four values we can write out the equations of both lines in Figure \ref{fig:numxcatxplot1}. First, since the word \texttt{female} comes alphabetically before \texttt{male}, female instructors are the ``baseline for comparison'' group. Thus, \texttt{intercept} is the intercept \emph{for only the female instructors}.

This holds similarly for \texttt{age}. It is the slope for age \emph{for only the female instructors}. Thus, the darker-colored regression line in Figure \ref{fig:numxcatxplot1} has an intercept of 4.883 and slope for age of -0.018. Remember that for this data, while the intercept has a mathematical interpretation, it has no \emph{practical} interpretation since instructors can't have zero age.

What about the intercept and slope for age of the male instructors in the lighter-colored line in Figure \ref{fig:numxcatxplot1}? This is where our notion of ``offsets'' comes into play once again.

The value for \texttt{gendermale} of -0.446 is not the intercept for the male instructors, but rather the \emph{offset}\index{offset} in intercept for male instructors relative to female instructors. The intercept for the male instructors is \texttt{intercept\ +\ gendermale} = 4.883 + (-0.446) = 4.883 - 0.446 = 4.437.

Similarly, \texttt{age:gendermale} = 0.014 is not the slope for age for the male instructors, but rather the \emph{offset} in slope for the male instructors. Therefore, the slope for age for the male instructors is \texttt{age\ +\ age:gendermale} \(= -0.018 + 0.014 = -0.004\). Thus, the lighter-colored regression line in Figure \ref{fig:numxcatxplot1} has intercept 4.437 and slope for age of -0.004. Let's summarize these values in Table \ref{tab:interaction-summary} and focus on the two slopes for age:

\begin{table}[!h]

\caption{\label{tab:interaction-summary}Comparison of intercepts and slopes for interaction model}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{lrr}
\toprule
Gender & Intercept & Slope for age\\
\midrule
Female instructors & 4.883 & -0.018\\
Male instructors & 4.437 & -0.004\\
\bottomrule
\end{tabular}
\end{table}

Since the slope for age for the female instructors was -0.018, it means that on average, a female instructor who is a year older would have a teaching score that is 0.018 units \textbf{lower}. For the male instructors, however, the corresponding associated decrease was on average only 0.004 units. While both slopes for age were negative, the slope for age for the female instructors is \emph{more negative}. This is consistent with our observation from Figure \ref{fig:numxcatxplot1}, that this model is suggesting that age impacts teaching scores for female instructors more than for male instructors.

Let's now write the equation for our regression lines, which we can use to compute our fitted values \(\widehat{y} = \widehat{\text{score}}\).

\[
\begin{aligned}
\widehat{y} = \widehat{\text{score}} &= b_0 + b_{\text{age}} \cdot \text{age} + b_{\text{male}} \cdot \mathbb{1}_{\text{is male}}(x) + b_{\text{age,male}} \cdot \text{age} \cdot \mathbb{1}_{\text{is male}}(x)\\
&= 4.883 -0.018 \cdot \text{age} - 0.446 \cdot \mathbb{1}_{\text{is male}}(x) + 0.014 \cdot \text{age} \cdot \mathbb{1}_{\text{is male}}(x)
\end{aligned}
\]

Whoa! That's even more daunting than the equation you saw for the life expectancy as a function of continent in Subsection \ref{model2table}! However, if you recall what an ``indicator function'' does, the equation simplifies greatly. In the previous equation, we have one indicator function of interest:

\[
\mathbb{1}_{\text{is male}}(x) = \left\{
\begin{array}{ll}
1 & \text{if } \text{instructor } x \text{ is male} \\
0 & \text{otherwise}\end{array}
\right.
\]

Second, let's match coefficients in the previous equation with values in the \texttt{estimate} column in our regression table in Table \ref{tab:regtable-interaction}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(b_0\) is the \texttt{intercept} = 4.883 for the female instructors
\item
  \(b_{\text{age}}\) is the slope for \texttt{age} = -0.018 for the female instructors
\item
  \(b_{\text{male}}\) is the offset in intercept = -0.446 for the male instructors
\item
  \(b_{\text{age,male}}\) is the offset in slope for age = 0.014 for the male instructors
\end{enumerate}

Let's put this all together and compute the fitted value \(\widehat{y} = \widehat{\text{score}}\) for female instructors. Since for female instructors \(\mathbb{1}_{\text{is male}}(x)\) = 0, the previous equation becomes

\[
\begin{aligned}
\widehat{y} = \widehat{\text{score}} &= 4.883 - 0.018 \cdot \text{age} - 0.446 \cdot 0 + 0.014 \cdot \text{age} \cdot 0\\
&= 4.883 - 0.018 \cdot \text{age} - 0 + 0\\
&= 4.883 - 0.018 \cdot \text{age}\\
\end{aligned}
\]

which is the equation of the darker-colored regression line in Figure \ref{fig:numxcatxplot1} corresponding to the female instructors in Table \ref{tab:interaction-summary}. Correspondingly, since for male instructors \(\mathbb{1}_{\text{is male}}(x)\) = 1, the previous equation becomes

\[
\begin{aligned}
\widehat{y} = \widehat{\text{score}} &= 4.883 - 0.018 \cdot \text{age} - 0.446 + 0.014 \cdot \text{age}\\
&= (4.883 - 0.446) + (- 0.018 + 0.014) * \text{age}\\
&= 4.437 - 0.004 \cdot \text{age}\\
\end{aligned}
\]

which is the equation of the lighter-colored regression line in Figure \ref{fig:numxcatxplot1} corresponding to the male instructors in Table \ref{tab:interaction-summary}.

Phew! That was a lot of arithmetic! Don't fret, however, this is as hard as modeling will get in this book. If you're still a little unsure about using indicator functions and using categorical explanatory variables in a regression model, we \emph{highly} suggest you re-read Subsection \ref{model2table}. This involves only a single categorical explanatory variable and thus is much simpler.

Before we end this section, we explain why we refer to this type of model as an ``interaction model.'' The \(b_{\text{age,male}}\) term in the equation for the fitted value \(\widehat{y}\) = \(\widehat{\text{score}}\) is what's known in statistical modeling as an ``interaction effect.'' The interaction term corresponds to the \texttt{age:gendermale} = 0.014 in the final row of the regression table in Table \ref{tab:regtable-interaction}.

We say there is an interaction effect if the associated effect of one variable \emph{depends on the value of another variable}. That is to say, the two variables are ``interacting'' with each other. Here, the associated effect of the variable age \emph{depends} on the value of the other variable gender. The difference in slopes for age of +0.014 of male instructors relative to female instructors shows this. \index{regression!multiple linear!interactions model}

Another way of thinking about interaction effects on teaching scores is as follows. For a given instructor at UT Austin, there might be an associated effect of their age \emph{by itself}, there might be an associated effect of their gender \emph{by itself}, but when age and gender are considered \emph{together} there might be an \emph{additional effect} above and beyond the two individual effects.

\hypertarget{model4table}{%
\subsection{Parallel slopes model}\label{model4table}}

When creating regression models with one numerical and one categorical explanatory variable, we are not just limited to interaction models as we just saw. Another type of model we can use is known as a \emph{parallel slopes} model.\index{parallel slopes model} Unlike interaction models where the regression lines can have different intercepts and different slopes, parallel slopes models still allow for different intercepts but \emph{force} all lines to have the same slope. The resulting regression lines are thus parallel. Let's visualize the best-fitting parallel slopes model to \texttt{evals\_ch6}.

Unfortunately, the \texttt{geom\_smooth()} function in the \texttt{ggplot2} package does not have a convenient way to plot parallel slopes models. Evgeni Chasnovski thus created a special purpose function called \texttt{geom\_parallel\_slopes()}\index{moderndive!geom\_parallel\_slopes()} that is included in the \texttt{moderndive} package. You won't find \texttt{geom\_parallel\_slopes()} in the \texttt{ggplot2} package, but rather the \texttt{moderndive} package. Thus, if you want to be able to use it, you will need to load both the \texttt{ggplot2} and \texttt{moderndive} packages. Using this function, let's now plot the parallel slopes model for teaching score. Notice how the code is identical to the code that produced the visualization of the interaction model in Figure \ref{fig:numxcatxplot1}, but now the \texttt{geom\_smooth(method\ =\ "lm",\ se\ =\ FALSE)} layer is replaced with \texttt{geom\_parallel\_slopes(se\ =\ FALSE)}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(evals\_ch6, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{y =}\NormalTok{ score, }\AttributeTok{color =}\NormalTok{ gender)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Teaching Score"}\NormalTok{, }\AttributeTok{color =} \StringTok{"Gender"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_parallel\_slopes}\NormalTok{(}\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/numxcatx-parallel-1} 

}

\caption{Parallel slopes model of score with age and gender.}\label{fig:numxcatx-parallel}
\end{figure}

Observe in Figure \ref{fig:numxcatx-parallel} that we now have parallel lines corresponding to the female and male instructors, respectively: here they have the same negative slope. This is telling us that instructors who are older will tend to receive lower teaching scores than instructors who are younger. Furthermore, since the lines are parallel, the associated penalty for being older is assumed to be the same for both female and male instructors.

However, observe also in Figure \ref{fig:numxcatx-parallel} that these two lines have different intercepts as evidenced by the fact that the lighter-colored line corresponding to the male instructors is higher than the darker-colored line corresponding to the female instructors. This is telling us that irrespective of age, female instructors tended to receive lower teaching scores than male instructors.

In order to obtain the precise numerical values of the two intercepts and the single common slope, we once again ``fit'' the model using the \texttt{lm()} ``linear model'' function and then apply the \texttt{get\_regression\_table()} function. However, unlike the interaction model which had a model formula of the form \texttt{y\ \textasciitilde{}\ x1\ *\ x2}, our model formula is now of the form \texttt{y\ \textasciitilde{}\ x1\ +\ x2}. In other words, our two explanatory variables \texttt{x1} and \texttt{x2} are separated by a \texttt{+} sign:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit regression model:}
\NormalTok{score\_model\_parallel\_slopes }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(score }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ gender, }\AttributeTok{data =}\NormalTok{ evals\_ch6)}
\CommentTok{\# Get regression table:}
\FunctionTok{get\_regression\_table}\NormalTok{(score\_model\_parallel\_slopes)}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]

\caption{\label{tab:regtable-parallel-slopes}Regression table for parallel slopes model}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{lrrrrrr}
\toprule
term & estimate & std\_error & statistic & p\_value & lower\_ci & upper\_ci\\
\midrule
intercept & 4.484 & 0.125 & 35.79 & 0.000 & 4.238 & 4.730\\
age & -0.009 & 0.003 & -3.28 & 0.001 & -0.014 & -0.003\\
gendermale & 0.191 & 0.052 & 3.63 & 0.000 & 0.087 & 0.294\\
\bottomrule
\end{tabular}
\end{table}

Similarly to the regression table for the interaction model from Table \ref{tab:regtable-interaction}, we have an \texttt{intercept} term corresponding to the intercept for the ``baseline for comparison'' female instructor group and a \texttt{gendermale} term corresponding to the \emph{offset} in intercept for the male instructors relative to female instructors. In other words, in Figure \ref{fig:numxcatx-parallel} the darker-colored regression line corresponding to the female instructors has an intercept of 4.484 while the lighter-colored regression line corresponding to the male instructors has an intercept of 4.484 + 0.191 = 4.675. Once again, since there aren't any instructors of age 0, the intercepts only have a mathematical interpretation but no practical one.

Unlike in Table \ref{tab:regtable-interaction}, however, we now only have a single slope for age of -0.009. This is because the model dictates that both the female and male instructors have a common slope for age. \index{regression!multiple linear!parallel slopes model} This is telling us that an instructor who is a year older than another instructor received a teaching score that is on average 0.009 units \emph{lower}. This penalty for being of advanced age applies equally to both female and male instructors.

Let's summarize these values in Table \ref{tab:parallel-slopes-summary}, noting the different intercepts but common slopes:

\begin{table}[!h]

\caption{\label{tab:parallel-slopes-summary}Comparison of intercepts and slope for parallel slopes model}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{lrr}
\toprule
Gender & Intercept & Slope for age\\
\midrule
Female instructors & 4.484 & -0.009\\
Male instructors & 4.675 & -0.009\\
\bottomrule
\end{tabular}
\end{table}

Let's now write the equation for our regression lines, which we can use to compute our fitted values \(\widehat{y} = \widehat{\text{score}}\).

\[
\begin{aligned}
\widehat{y} = \widehat{\text{score}} &= b_0 + b_{\text{age}} \cdot \text{age} + b_{\text{male}} \cdot \mathbb{1}_{\text{is male}}(x)\\
&= 4.484 -0.009 \cdot \text{age} + 0.191 \cdot \mathbb{1}_{\text{is male}}(x) 
\end{aligned}
\]

Let's put this all together and compute the fitted value \(\widehat{y} = \widehat{\text{score}}\) for female instructors. Since for female instructors the indicator function \(\mathbb{1}_{\text{is male}}(x)\) = 0, the previous equation becomes

\[
\begin{aligned}
\widehat{y} = \widehat{\text{score}} &= 4.484 -0.009 \cdot \text{age} + 0.191 \cdot 0\\
&= 4.484 -0.009 \cdot \text{age}
\end{aligned}
\]

which is the equation of the darker-colored regression line in Figure \ref{fig:numxcatx-parallel} corresponding to the female instructors. Correspondingly, since for male instructors the indicator function \(\mathbb{1}_{\text{is male}}(x)\) = 1, the previous equation becomes

\[
\begin{aligned}
\widehat{y} = \widehat{\text{score}} &= 4.484 -0.009 \cdot \text{age} + 0.191 \cdot 1\\
&= (4.484 + 0.191) - 0.009 \cdot \text{age}\\
&= 4.675 -0.009 \cdot \text{age}
\end{aligned}
\]

which is the equation of the lighter-colored regression line in Figure \ref{fig:numxcatx-parallel} corresponding to the male instructors.

Great! We've considered both an interaction model and a parallel slopes model for our data. Let's compare the visualizations for both models side-by-side in Figure \ref{fig:numxcatx-comparison}.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/numxcatx-comparison-1} 

}

\caption{Comparison of interaction and parallel slopes models.}\label{fig:numxcatx-comparison}
\end{figure}

At this point, you might be asking yourself: ``Why would we ever use a parallel slopes model?''. Looking at the left-hand plot in Figure \ref{fig:numxcatx-comparison}, the two lines definitely do not appear to be parallel, so why would we \emph{force} them to be parallel? For this data, we agree! It can easily be argued that the interaction model on the left is more appropriate. However, in the upcoming Subsection \ref{model-selection} on model selection, we'll present an example where it can be argued that the case for a parallel slopes model might be stronger.

\hypertarget{model4points}{%
\subsection{Observed/fitted values and residuals}\label{model4points}}

For brevity's sake, in this section we'll only compute the observed values, fitted values, and residuals for the interaction model which we saved in \texttt{score\_model\_interaction}. You'll have an opportunity to study the corresponding values for the parallel slopes model in the upcoming \emph{Learning check}.

Say, you have an instructor who identifies as female and is 36 years old. What fitted value \(\widehat{y}\) = \(\widehat{\text{score}}\) would our model yield? Say, you have another instructor who identifies as male and is 59 years old. What would their fitted value \(\widehat{y}\) be?

We answer this question visually first for the female instructor by finding the intersection of the darker-colored regression line and the vertical line at \(x\) = age = 36. We mark this value with a large darker-colored dot in Figure \ref{fig:fitted-values}. Similarly, we can identify the fitted value \(\widehat{y}\) = \(\widehat{\text{score}}\) for the male instructor by finding the intersection of the lighter-colored regression line and the vertical line at \(x\) = age = 59. We mark this value with a large lighter-colored dot in Figure \ref{fig:fitted-values}.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/fitted-values-1} 

}

\caption{Fitted values for two new professors.}\label{fig:fitted-values}
\end{figure}

What are these two values of \(\widehat{y}\) = \(\widehat{\text{score}}\) precisely? We can use the equations of the two regression lines we computed in Subsection \ref{model4interactiontable}, which in turn were based on values from the regression table in Table \ref{tab:regtable-interaction}:

\begin{itemize}
\tightlist
\item
  For all female instructors: \(\widehat{y} = \widehat{\text{score}} = 4.883 - 0.018 \cdot \text{age}\)
\item
  For all male instructors: \(\widehat{y} = \widehat{\text{score}} = 4.437 - 0.004 \cdot \text{age}\)
\end{itemize}

So our fitted values would be: \(4.883 - 0.018 \cdot 36 = 4.24\) and \(4.437 - 0.004 \cdot 59 = 4.20\), respectively.

Now what if we want the fitted values not just for these two instructors, but for the instructors of all 463 courses included in the \texttt{evals\_ch6} data frame? Doing this by hand would be long and tedious! This is where the \texttt{get\_regression\_points()} function from the \texttt{moderndive} package can help: it will quickly automate the above calculations for all 463 courses. We present a preview of just the first 10 rows out of 463 in Table \ref{tab:model4-points-table}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{regression\_points }\OtherTok{\textless{}{-}} \FunctionTok{get\_regression\_points}\NormalTok{(score\_model\_interaction)}
\NormalTok{regression\_points}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]

\caption{\label{tab:model4-points-table}Regression points (First 10 out of 463 courses)}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{r|r|r|l|r|r}
\hline
ID & score & age & gender & score\_hat & residual\\
\hline
1 & 4.7 & 36 & female & 4.25 & 0.448\\
\hline
2 & 4.1 & 36 & female & 4.25 & -0.152\\
\hline
3 & 3.9 & 36 & female & 4.25 & -0.352\\
\hline
4 & 4.8 & 36 & female & 4.25 & 0.548\\
\hline
5 & 4.6 & 59 & male & 4.20 & 0.399\\
\hline
6 & 4.3 & 59 & male & 4.20 & 0.099\\
\hline
7 & 2.8 & 59 & male & 4.20 & -1.401\\
\hline
8 & 4.1 & 51 & male & 4.23 & -0.133\\
\hline
9 & 3.4 & 51 & male & 4.23 & -0.833\\
\hline
10 & 4.5 & 40 & female & 4.18 & 0.318\\
\hline
\end{tabular}
\end{table}

It turns out that the female instructor of age 36 taught the first four courses, while the male instructor taught the next 3. The resulting \(\widehat{y}\) = \(\widehat{\text{score}}\) fitted values are in the \texttt{score\_hat} column. Furthermore, the \texttt{get\_regression\_points()} function also returns the residuals \(y-\widehat{y}\). Notice, for example, the first and fourth courses the female instructor of age 36 taught had positive residuals, indicating that the actual teaching scores they received from students were greater than their fitted score of 4.25. On the other hand, the second and third courses this instructor taught had negative residuals, indicating that the actual teaching scores they received from students were less than 4.25.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC6.1)} Compute the observed values, fitted values, and residuals not for the interaction model as we just did, but rather for the parallel slopes model we saved in \texttt{score\_model\_parallel\_slopes}.

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{model3}{%
\section{Two numerical explanatory variables}\label{model3}}

Let's now switch gears and consider multiple regression models where instead of one numerical and one categorical explanatory variable, we now have two numerical explanatory variables. The dataset we'll use is from \href{http://www-bcf.usc.edu/~gareth/ISL/}{\emph{An Introduction to Statistical Learning with Applications in R (ISLR)}}, an intermediate-level textbook on statistical and machine learning \citep{islr2017}. Its accompanying \texttt{ISLR} R package contains the datasets to which the authors apply various machine learning methods.

One frequently used dataset in this book is the \texttt{Credit} dataset, where the outcome variable of interest is the credit card debt of 400 individuals. Other variables like income, credit limit, credit rating, and age are included as well. Note that the \texttt{Credit} data is not based on real individuals' financial information, but rather is a simulated dataset used for educational purposes.

In this section, we'll fit a regression model where we have

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A numerical outcome variable \(y\), the cardholder's credit card debt
\item
  Two explanatory variables:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    One numerical explanatory variable \(x_1\), the cardholder's credit limit
  \item
    Another numerical explanatory variable \(x_2\), the cardholder's income (in thousands of dollars).
  \end{enumerate}
\end{enumerate}

\hypertarget{model3EDA}{%
\subsection{Exploratory data analysis}\label{model3EDA}}

Let's load the \texttt{Credit} dataset\index{R packages!ISLR!Credit data frame}. To keep things simple let's \texttt{select()} the subset of the variables we'll consider in this chapter, and save this data in the new data frame \texttt{credit\_ch6}. Notice our slightly different use of the \texttt{select()} verb here than we introduced in Subsection \ref{select}. For example, we'll select the \texttt{Balance} variable from \texttt{Credit} but then save it with a new variable name \texttt{debt}. We do this because here the term ``debt'' is easier to interpret than ``balance.''

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ISLR)}
\NormalTok{credit\_ch6 }\OtherTok{\textless{}{-}}\NormalTok{ Credit }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(ID, }\AttributeTok{debt =}\NormalTok{ Balance, }\AttributeTok{credit\_limit =}\NormalTok{ Limit, }
         \AttributeTok{income =}\NormalTok{ Income, }\AttributeTok{credit\_rating =}\NormalTok{ Rating, }\AttributeTok{age =}\NormalTok{ Age)}
\end{Highlighting}
\end{Shaded}

You can observe the effect of our use of \texttt{select()} in the first common step of an exploratory data analysis: looking at the raw values either in RStudio's spreadsheet viewer or by using \texttt{glimpse()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(credit\_ch6)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 400
Columns: 6
$ ID            <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...
$ debt          <int> 333, 903, 580, 964, 331, 1151, 203, 872, 279, 13...
$ credit_limit  <int> 3606, 6645, 7075, 9504, 4897, 8047, 3388, 7114, ...
$ income        <dbl> 14.9, 106.0, 104.6, 148.9, 55.9, 80.2, 21.0, 71....
$ credit_rating <int> 283, 483, 514, 681, 357, 569, 259, 512, 266, 491...
$ age           <int> 34, 82, 71, 36, 68, 77, 37, 87, 66, 41, 30, 64, ...
\end{verbatim}

Furthermore, let's look at a random sample of five out of the 400 credit card holders in Table \ref{tab:model3-data-preview}. Once again, note that due to the random nature of the sampling, you will likely end up with a different subset of five rows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{credit\_ch6 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{sample\_n}\NormalTok{(}\AttributeTok{size =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]

\caption{\label{tab:model3-data-preview}Random sample of 5 credit card holders}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{rrrrrr}
\toprule
ID & debt & credit\_limit & income & credit\_rating & age\\
\midrule
272 & 436 & 4866 & 45.0 & 347 & 30\\
239 & 52 & 2910 & 26.5 & 236 & 58\\
87 & 815 & 6340 & 55.4 & 448 & 33\\
108 & 0 & 3189 & 39.1 & 263 & 72\\
149 & 0 & 2420 & 15.2 & 192 & 69\\
\bottomrule
\end{tabular}
\end{table}

Now that we've looked at the raw values in our \texttt{credit\_ch6} data frame and got a sense of the data, let's move on to the next common step in an exploratory data analysis: computing summary statistics. Let's use the \texttt{skim()} function from the \texttt{skimr} package, being sure to only \texttt{select()} the columns of interest for our model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{credit\_ch6 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(debt, credit\_limit, income) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{skim}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Skim summary statistics
 n obs: 400 
 n variables: 3 

── Variable type:integer 
  variable missing complete   n    mean      sd  p0     p25    p50     p75  p100
credit_limit     0      400 400 4735.6  2308.2  855 3088    4622.5 5872.75 13913
         debt    0      400 400  520.01  459.76   0   68.75  459.5  863     1999

── Variable type:numeric 
 variable missing complete   n  mean    sd    p0   p25   p50   p75   p100
   income       0      400 400 45.22 35.24 10.35 21.01 33.12 57.47 186.63
\end{verbatim}

Observe the summary statistics for the outcome variable \texttt{debt}: the mean and median credit card debt are \$520.01 and \$459.50, respectively, and that 25\% of card holders had debts of \$68.75 or less. Let's now look at one of the explanatory variables \texttt{credit\_limit}: the mean and median credit card limit are \$4735.6 and \$4622.50, respectively, while 75\% of card holders had incomes of \$57,470 or less.

Since our outcome variable \texttt{debt} and the explanatory variables \texttt{credit\_limit} and \texttt{income} are numerical, we can compute the correlation coefficient between the different possible pairs of these variables. First, we can run the \texttt{get\_correlation()} command as seen in Subsection \ref{model1EDA} twice, once for each explanatory variable:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{credit\_ch6 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{get\_correlation}\NormalTok{(debt }\SpecialCharTok{\textasciitilde{}}\NormalTok{ credit\_limit)}
\NormalTok{credit\_ch6 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{get\_correlation}\NormalTok{(debt }\SpecialCharTok{\textasciitilde{}}\NormalTok{ income)}
\end{Highlighting}
\end{Shaded}

Or we can simultaneously compute them by returning a \emph{correlation matrix} which we display in Table \ref{tab:model3-correlation}. \index{correlation (coefficient)} We can see the correlation coefficient for any pair of variables by looking them up in the appropriate row/column combination.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{credit\_ch6 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(debt, credit\_limit, income) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{cor}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]

\caption{\label{tab:model3-correlation}Correlation coefficients between credit card debt, credit limit, and income}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{lrrr}
\toprule
  & debt & credit\_limit & income\\
\midrule
debt & 1.000 & 0.862 & 0.464\\
credit\_limit & 0.862 & 1.000 & 0.792\\
income & 0.464 & 0.792 & 1.000\\
\bottomrule
\end{tabular}
\end{table}

For example, the correlation coefficient of:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{debt} with itself is 1 as we would expect based on the definition of the correlation coefficient.
\item
  \texttt{debt} with \texttt{credit\_limit} is 0.862. This indicates a strong positive linear relationship, which makes sense as only individuals with large credit limits can accrue large credit card debts.
\item
  \texttt{debt} with \texttt{income} is 0.464. This is suggestive of another positive linear relationship, although not as strong as the relationship between \texttt{debt} and \texttt{credit\_limit}.
\item
  As an added bonus, we can read off the correlation coefficient between the two explanatory variables of \texttt{credit\_limit} and \texttt{income} as 0.792.
\end{enumerate}

We say there is a high degree of \emph{collinearity}\index{collinearity} between the \texttt{credit\_limit} and \texttt{income} explanatory variables. Collinearity (or multicollinearity) is a phenomenon where one explanatory variable in a multiple regression model is highly correlated with another.

So in our case since \texttt{credit\_limit} and \texttt{income} are highly correlated, if we knew someone's \texttt{credit\_limit}, we could make pretty good guesses about their \texttt{income} as well. Thus, these two variables provide somewhat redundant information. However, we'll leave discussion on how to work with collinear explanatory variables to a more intermediate-level book on regression modeling.

Let's visualize the relationship of the outcome variable with each of the two explanatory variables in two separate plots in Figure \ref{fig:2numxplot1}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(credit\_ch6, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ credit\_limit, }\AttributeTok{y =}\NormalTok{ debt)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Credit limit (in $)"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Credit card debt (in $)"}\NormalTok{, }
       \AttributeTok{title =} \StringTok{"Debt and credit limit"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(credit\_ch6, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ income, }\AttributeTok{y =}\NormalTok{ debt)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Income (in $1000)"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Credit card debt (in $)"}\NormalTok{, }
       \AttributeTok{title =} \StringTok{"Debt and income"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/2numxplot1-1} 

}

\caption{Relationship between credit card debt and credit limit/income.}\label{fig:2numxplot1}
\end{figure}

Observe there is a positive relationship between credit limit and credit card debt: as credit limit increases so also does credit card debt. This is consistent with the strongly positive correlation coefficient of 0.862 we computed earlier. In the case of income, the positive relationship doesn't appear as strong, given the weakly positive correlation coefficient of 0.464.

However, the two plots in Figure \ref{fig:2numxplot1} only focus on the relationship of the outcome variable with each of the two explanatory variables \emph{separately}. To visualize the \emph{joint} relationship of all three variables simultaneously, we need a 3-dimensional (3D) scatterplot as seen in Figure \ref{fig:3D-scatterplot}. Each of the 400 observations in the \texttt{credit\_ch6} data frame are marked with a point where

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The numerical outcome variable \(y\) \texttt{debt} is on the vertical axis.
\item
  The two numerical explanatory variables, \(x_1\) \texttt{income} and \(x_2\) \texttt{credit\_limit}, are on the two axes that form the bottom plane.
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{images/credit_card_balance_regression_plane} 

}

\caption{3D scatterplot and regression plane.}\label{fig:3D-scatterplot}
\end{figure}

Furthermore, we also include the \emph{regression plane}\index{regression!regression plane}. Recall from Subsection \ref{leastsquares} that regression lines are ``best-fitting'' in that of all possible lines we can draw through a cloud of points, the regression line minimizes the \emph{sum of squared residuals}\index{sum of squared residuals}. This concept also extends to models with two numerical explanatory variables. The difference is instead of a ``best-fitting'' line, we now have a ``best-fitting'' plane that similarly minimizes the sum of squared residuals. Head to \href{https://moderndive.com/regression-plane}{this website} to open an interactive version of this plot in your browser.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC6.2)} Conduct a new exploratory data analysis with the same outcome variable \(y\) \texttt{debt} but with \texttt{credit\_rating} and \texttt{age} as the new explanatory variables \(x_1\) and \(x_2\). What can you say about the relationship between a credit card holder's debt and their credit rating and age?

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{model3table}{%
\subsection{Regression plane}\label{model3table}}

Let's now fit a regression model and get the regression table corresponding to the regression plane in Figure \ref{fig:3D-scatterplot}. To keep things brief in this subsection, we won't consider an interaction model for the two numerical explanatory variables \texttt{income} and \texttt{credit\_limit} like we did in Subsection \ref{model4interactiontable} using the model formula \texttt{score\ \textasciitilde{}\ age\ *\ gender}. Rather we'll only consider a model fit with a formula of the form \texttt{y\ \textasciitilde{}\ x1\ +\ x2}. Confusingly, however, since we now have a regression plane instead of multiple lines, the label ``parallel slopes'' doesn't apply when you have two numerical explanatory variables. Just as we have done multiple times throughout Chapters \ref{regression} and this chapter, the regression table for this model using our two-step process is in Table \ref{tab:model3-table-output}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit regression model:}
\NormalTok{debt\_model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(debt }\SpecialCharTok{\textasciitilde{}}\NormalTok{ credit\_limit }\SpecialCharTok{+}\NormalTok{ income, }\AttributeTok{data =}\NormalTok{ credit\_ch6)}
\CommentTok{\# Get regression table:}
\FunctionTok{get\_regression\_table}\NormalTok{(debt\_model)}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]

\caption{\label{tab:model3-table-output}Multiple regression table}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{lrrrrrr}
\toprule
term & estimate & std\_error & statistic & p\_value & lower\_ci & upper\_ci\\
\midrule
intercept & -385.179 & 19.465 & -19.8 & 0 & -423.446 & -346.912\\
credit\_limit & 0.264 & 0.006 & 45.0 & 0 & 0.253 & 0.276\\
income & -7.663 & 0.385 & -19.9 & 0 & -8.420 & -6.906\\
\bottomrule
\end{tabular}
\end{table}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We first ``fit'' the linear regression model using the \texttt{lm(y\ \textasciitilde{}\ x1\ +\ x2,\ data)} function and save it in \texttt{debt\_model}.
\item
  We get the regression table by applying the \texttt{get\_regression\_table()} function from the \texttt{moderndive} package to \texttt{debt\_model}.
\end{enumerate}

Let's interpret the three values in the \texttt{estimate} column. First, the \texttt{intercept} value is -\$385.179. This intercept represents the credit card debt for an individual who has \texttt{credit\_limit} of \$0 and \texttt{income} of \$0. In our data, however, the intercept has no practical interpretation since no individuals had \texttt{credit\_limit} or \texttt{income} values of \$0. Rather, the intercept is used to situate the regression plane in 3D space.

Second, the \texttt{credit\_limit} value is \$0.264. Taking into account all the other explanatory variables in our model, for every increase of one dollar in \texttt{credit\_limit}, there is an associated increase of on average \$0.26 in credit card debt. Just as we did in Subsection \ref{model1table}, we are cautious \emph{not} to imply causality as we saw in Subsection \ref{correlation-is-not-causation} that ``correlation is not necessarily causation.'' We do this merely stating there was an \emph{associated} increase.

Furthermore, we preface our interpretation with the statement, ``taking into account all the other explanatory variables in our model.'' Here, by all other explanatory variables we mean \texttt{income}. We do this to emphasize that we are now jointly interpreting the associated effect of multiple explanatory variables in the same model at the same time.

Third, \texttt{income} = -\$7.66. Taking into account all other explanatory variables in our model, for every increase of one unit of \texttt{income} (\$1000 in actual income), there is an associated decrease of, on average, \$7.66 in credit card debt.

Putting these results together, the equation of the regression plane that gives us fitted values \(\widehat{y}\) = \(\widehat{\text{debt}}\) is:

\[
\begin{aligned}
\widehat{y} &= b_0 + b_1 \cdot x_1 +  b_2 \cdot x_2\\
\widehat{\text{debt}} &= b_0 + b_{\text{limit}} \cdot \text{limit} + b_{\text{income}} \cdot \text{income}\\
&= -385.179 + 0.263 \cdot\text{limit} - 7.663 \cdot\text{income}
\end{aligned}
\]

Recall however in the right-hand plot of Figure \ref{fig:2numxplot1} that when plotting the relationship between \texttt{debt} and \texttt{income} in isolation, there appeared to be a \emph{positive} relationship. In the last discussed multiple regression, however, when \emph{jointly} modeling the relationship between \texttt{debt}, \texttt{credit\_limit}, and \texttt{income}, there appears to be a \emph{negative} relationship of \texttt{debt} and \texttt{income} as evidenced by the negative slope for \texttt{income} of -\$7.663. What explains these contradictory results? A phenomenon known as \emph{Simpson's Paradox}\index{Simpson's Paradox}, whereby overall trends that exist in aggregate either disappear or reverse when the data are broken down into groups. In Subsection \ref{simpsonsparadox} we elaborate on this idea by looking at the relationship between \texttt{credit\_limit} and credit card \texttt{debt}, but split along different \texttt{income} brackets.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC6.3)} Fit a new simple linear regression using \texttt{lm(debt\ \textasciitilde{}\ credit\_rating\ +\ age,\ data\ =\ credit\_ch6)} where \texttt{credit\_rating} and \texttt{age} are the new numerical explanatory variables \(x_1\) and \(x_2\). Get information about the ``best-fitting'' regression plane from the regression table by applying the \texttt{get\_regression\_table()} function. How do the regression results match up with the results from your previous exploratory data analysis?

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{model3points}{%
\subsection{Observed/fitted values and residuals}\label{model3points}}

Let's also compute all fitted values and residuals for our regression model using the \texttt{get\_regression\_points()} function and present only the first 10 rows of output in Table \ref{tab:model3-points-table}. Remember that the coordinates of each of the points in our 3D scatterplot in Figure \ref{fig:3D-scatterplot} can be found in the \texttt{income}, \texttt{credit\_limit}, and \texttt{debt} columns. The fitted values on the regression plane are found in the \texttt{debt\_hat} column and are computed using our equation for the regression plane in the previous section:

\[
\begin{aligned}
\widehat{y} = \widehat{\text{debt}} &= -385.179 + 0.263 \cdot \text{limit} - 7.663 \cdot \text{income}
\end{aligned}
\]

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{get\_regression\_points}\NormalTok{(debt\_model)}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]

\caption{\label{tab:model3-points-table}Regression points (First 10 credit card holders out of 400)}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{rrrrrr}
\toprule
ID & debt & credit\_limit & income & debt\_hat & residual\\
\midrule
1 & 333 & 3606 & 14.9 & 454 & -120.8\\
2 & 903 & 6645 & 106.0 & 559 & 344.3\\
3 & 580 & 7075 & 104.6 & 683 & -103.4\\
4 & 964 & 9504 & 148.9 & 986 & -21.7\\
5 & 331 & 4897 & 55.9 & 481 & -150.0\\
6 & 1151 & 8047 & 80.2 & 1127 & 23.6\\
7 & 203 & 3388 & 21.0 & 349 & -146.4\\
8 & 872 & 7114 & 71.4 & 948 & -76.0\\
9 & 279 & 3300 & 15.1 & 371 & -92.2\\
10 & 1350 & 6819 & 71.1 & 873 & 477.3\\
\bottomrule
\end{tabular}
\end{table}

\hypertarget{mult-reg-related-topics}{%
\section{Related topics}\label{mult-reg-related-topics}}

\hypertarget{model-selection}{%
\subsection{Model selection}\label{model-selection}}

When should we use an interaction model versus a parallel slopes model? Recall in Sections \ref{model4interactiontable} and \ref{model4table} we fit both interaction and parallel slopes models for the outcome variable \(y\) (teaching score) using a numerical explanatory variable \(x_1\) (age) and a categorical explanatory variable \(x_2\) (gender recorded as a binary variable). We compared these models in Figure \ref{fig:numxcatx-comparison}, which we display again now.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/recall-parallel-vs-interaction-1} 

}

\caption{Previously seen comparison of interaction and parallel slopes models.}\label{fig:recall-parallel-vs-interaction}
\end{figure}

A lot of you might have asked yourselves: ``Why would I force the lines to have parallel slopes (as seen in the right-hand plot) when they clearly have different slopes (as seen in the left-hand plot)?''.

The answer lies in a philosophical principle known as ``Occam's Razor.'' It states that, ``all other things being equal, simpler solutions are more likely to be correct than complex ones.'' When viewed in a modeling framework, Occam's Razor \index{Occam's Razor} can be restated as, ``all other things being equal, simpler models are to be preferred over complex ones.'' In other words, we should only favor the more complex model if the additional complexity is \emph{warranted}.

Let's revisit the equations for the regression line for both the interaction and parallel slopes model:

\[
\begin{aligned}
\text{Interaction} &: \widehat{y} = \widehat{\text{score}} = b_0 + b_{\text{age}} \cdot \text{age} + b_{\text{male}} \cdot \mathbb{1}_{\text{is male}}(x) + \\
& \qquad b_{\text{age,male}} \cdot \text{age} \cdot \mathbb{1}_{\text{is male}}\\
\text{Parallel slopes} &: \widehat{y} = \widehat{\text{score}} = b_0 + b_{\text{age}} \cdot \text{age} + b_{\text{male}} \cdot \mathbb{1}_{\text{is male}}(x)
\end{aligned}
\]

The interaction model is ``more complex'' in that there is an additional \(b_{\text{age,male}} \cdot \text{age} \cdot \mathbb{1}_{\text{is male}}\) interaction term in the equation not present for the parallel slopes model. Or viewed alternatively, the regression table for the interaction model in Table \ref{tab:regtable-interaction} has \emph{four} rows, whereas the regression table for the parallel slopes model in Table \ref{tab:regtable-parallel-slopes} has \emph{three} rows. The question becomes: ``Is this additional complexity warranted?''. In this case, it can be argued that this additional complexity is warranted, as evidenced by the clear x-shaped pattern of the two regression lines in the left-hand plot of Figure \ref{fig:recall-parallel-vs-interaction}.

However, let's consider an example where the additional complexity might \emph{not} be warranted. Let's consider the \texttt{MA\_schools} data included in the \texttt{moderndive} package which contains 2017 data on Massachusetts public high schools provided by the Massachusetts Department of Education. For more details, read the help file for this data by running \texttt{?MA\_schools} in the console.

Let's model the numerical outcome variable \(y\), average SAT math score for a given high school, as a function of two explanatory variables:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A numerical explanatory variable \(x_1\), the percentage of that high school's student body that are economically disadvantaged and
\item
  A categorical explanatory variable \(x_2\), the school size as measured by enrollment: small (13-341 students), medium (342-541 students), and large (542-4264 students).
\end{enumerate}

Let's create visualizations of both the interaction and parallel slopes model once again and display the output in Figure \ref{fig:numxcatx-comparison-2}. Recall from Subsection \ref{model4table} that the \texttt{geom\_parallel\_slopes()} function is a special purpose function included in the \texttt{moderndive} package, since the \texttt{geom\_smooth()} method in the \texttt{ggplot2} package does not have a convenient way to plot parallel slopes models.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Interaction model}
\FunctionTok{ggplot}\NormalTok{(MA\_schools, }
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ perc\_disadvan, }\AttributeTok{y =}\NormalTok{ average\_sat\_math, }\AttributeTok{color =}\NormalTok{ size)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.25}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Percent economically disadvantaged"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Math SAT Score"}\NormalTok{, }
       \AttributeTok{color =} \StringTok{"School size"}\NormalTok{, }\AttributeTok{title =} \StringTok{"Interaction model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Parallel slopes model}
\FunctionTok{ggplot}\NormalTok{(MA\_schools, }
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ perc\_disadvan, }\AttributeTok{y =}\NormalTok{ average\_sat\_math, }\AttributeTok{color =}\NormalTok{ size)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.25}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_parallel\_slopes}\NormalTok{(}\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Percent economically disadvantaged"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Math SAT Score"}\NormalTok{, }
       \AttributeTok{color =} \StringTok{"School size"}\NormalTok{, }\AttributeTok{title =} \StringTok{"Parallel slopes model"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/numxcatx-comparison-2-1} 

}

\caption{Comparison of interaction and parallel slopes models for Massachusetts schools.}\label{fig:numxcatx-comparison-2}
\end{figure}

Look closely at the left-hand plot of Figure \ref{fig:numxcatx-comparison-2} corresponding to an interaction model. While the slopes are indeed different, they do not differ \emph{by much} and are nearly identical. Now compare the left-hand plot with the right-hand plot corresponding to a parallel slopes model. The two models don't appear all that different. So in this case, it can be argued that the additional complexity of the interaction model is \emph{not warranted}. Thus following Occam's Razor, we should prefer the ``simpler'' parallel slopes model. Let's explicitly define what ``simpler'' means in this case. Let's compare the regression tables for the interaction and parallel slopes models in Tables \ref{tab:model2-interaction} and \ref{tab:model2-parallel-slopes}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_2\_interaction }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(average\_sat\_math }\SpecialCharTok{\textasciitilde{}}\NormalTok{ perc\_disadvan }\SpecialCharTok{*}\NormalTok{ size, }
                          \AttributeTok{data =}\NormalTok{ MA\_schools)}
\FunctionTok{get\_regression\_table}\NormalTok{(model\_2\_interaction)}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]

\caption{\label{tab:model2-interaction}Interaction model regression table}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{lrrrrrr}
\toprule
term & estimate & std\_error & statistic & p\_value & lower\_ci & upper\_ci\\
\midrule
intercept & 594.327 & 13.288 & 44.726 & 0.000 & 568.186 & 620.469\\
perc\_disadvan & -2.932 & 0.294 & -9.961 & 0.000 & -3.511 & -2.353\\
sizemedium & -17.764 & 15.827 & -1.122 & 0.263 & -48.899 & 13.371\\
sizelarge & -13.293 & 13.813 & -0.962 & 0.337 & -40.466 & 13.880\\
perc\_disadvan:sizemedium & 0.146 & 0.371 & 0.393 & 0.694 & -0.585 & 0.877\\
perc\_disadvan:sizelarge & 0.189 & 0.323 & 0.586 & 0.559 & -0.446 & 0.824\\
\bottomrule
\end{tabular}
\end{table}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_2\_parallel\_slopes }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(average\_sat\_math }\SpecialCharTok{\textasciitilde{}}\NormalTok{ perc\_disadvan }\SpecialCharTok{+}\NormalTok{ size, }
                              \AttributeTok{data =}\NormalTok{ MA\_schools)}
\FunctionTok{get\_regression\_table}\NormalTok{(model\_2\_parallel\_slopes)}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]

\caption{\label{tab:model2-parallel-slopes}Parallel slopes regression table}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{lrrrrrr}
\toprule
term & estimate & std\_error & statistic & p\_value & lower\_ci & upper\_ci\\
\midrule
intercept & 588.19 & 7.607 & 77.325 & 0.000 & 573.23 & 603.15\\
perc\_disadvan & -2.78 & 0.106 & -26.120 & 0.000 & -2.99 & -2.57\\
sizemedium & -11.91 & 7.535 & -1.581 & 0.115 & -26.74 & 2.91\\
sizelarge & -6.36 & 6.923 & -0.919 & 0.359 & -19.98 & 7.26\\
\bottomrule
\end{tabular}
\end{table}

Observe how the regression table for the interaction model has 2 more rows (6 versus 4). This reflects the additional ``complexity'' of the interaction model over the parallel slopes model.

Furthermore, note in Table \ref{tab:model2-interaction} how the \emph{offsets for the slopes} \texttt{perc\_disadvan:sizemedium} being 0.146 and \texttt{perc\_disadvan:sizelarge} being 0.189 are small relative to the \emph{slope for the baseline group} of small schools of \(-2.932\). In other words, all three slopes are similarly negative: \(-2.932\) for small schools, \(-2.786\) \((=-2.932 + 0.146)\) for medium schools, and \(-2.743\) \((=-2.932 + 0.189)\) for large schools. These results are suggesting that irrespective of school size, the relationship between average math SAT scores and the percent of the student body that is economically disadvantaged is similar and, alas, quite negative.

What you have just performed is a rudimentary \emph{model selection}\index{model selection}: choosing which model fits data best among a set of candidate models. While the model selection approach we just took was visual in nature and hence somewhat qualitative, more statistically rigorous methods for model selection exist in the fields of multiple regression and statistical/machine learning.

\hypertarget{correlationcoefficient2}{%
\subsection{Correlation coefficient}\label{correlationcoefficient2}}

Recall from Table \ref{tab:model3-correlation} that the correlation coefficient\index{correlation (coefficient)} between \texttt{income} in thousands of dollars and credit card \texttt{debt} was 0.464. What if instead we looked at the correlation coefficient between \texttt{income} and credit card \texttt{debt}, but where \texttt{income} was in dollars and not thousands of dollars? This can be done by multiplying \texttt{income} by 1000.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{credit\_ch6 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(debt, income) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{income =}\NormalTok{ income }\SpecialCharTok{*} \DecValTok{1000}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{cor}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]

\caption{\label{tab:cor-credit-2}Correlation between income (in dollars) and credit card debt}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{lrr}
\toprule
  & debt & income\\
\midrule
debt & 1.000 & 0.464\\
income & 0.464 & 1.000\\
\bottomrule
\end{tabular}
\end{table}

We see it is the same! We say that the correlation coefficient is \emph{invariant to linear transformations}. The correlation between \(x\) and \(y\) will be the same as the correlation between \(a\cdot x + b\) and \(y\) for any numerical values \(a\) and \(b\).

\hypertarget{simpsonsparadox}{%
\subsection{Simpson's Paradox}\label{simpsonsparadox}}

Recall in Section \ref{model3}, we saw the two seemingly contradictory results when studying the relationship between credit card \texttt{debt} and \texttt{income}. On the one hand, the right hand plot of Figure \ref{fig:2numxplot1} suggested that the relationship between credit card \texttt{debt} and \texttt{income} was \emph{positive}. We re-display this in Figure \ref{fig:2numxplot1-repeat}.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/2numxplot1-repeat-1} 

}

\caption{Relationship between credit card debt and income.}\label{fig:2numxplot1-repeat}
\end{figure}

On the other hand, the multiple regression results in Table \ref{tab:model3-table-output} suggested that the relationship between \texttt{debt} and \texttt{income} was \emph{negative}. We re-display this information in Table \ref{tab:model3-table-output-repeat}.

\begin{table}[!h]

\caption{\label{tab:model3-table-output-repeat}Multiple regression results}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{lrrrrrr}
\toprule
term & estimate & std\_error & statistic & p\_value & lower\_ci & upper\_ci\\
\midrule
intercept & -385.179 & 19.465 & -19.8 & 0 & -423.446 & -346.912\\
credit\_limit & 0.264 & 0.006 & 45.0 & 0 & 0.253 & 0.276\\
income & -7.663 & 0.385 & -19.9 & 0 & -8.420 & -6.906\\
\bottomrule
\end{tabular}
\end{table}

Observe how the slope for \texttt{income} is \(-7.663\) and, most importantly for now, it is negative. This contradicts our observation in Figure \ref{fig:2numxplot1-repeat} that the relationship is positive. How can this be? Recall the interpretation of the slope for \texttt{income} in the context of a multiple regression model: \emph{taking into account all the other explanatory variables in our model}, for every increase of one unit in \texttt{income} (i.e., \$1000), there is an associated decrease of on average \$7.663 in \texttt{debt}.

In other words, while in \emph{isolation}, the relationship between \texttt{debt} and \texttt{income} may be positive, when taking into account \texttt{credit\_limit} as well, this relationship becomes negative. These seemingly paradoxical results are due to a phenomenon aptly named \href{https://en.wikipedia.org/wiki/Simpson\%27s_paradox}{\emph{Simpson's Paradox}}\index{Simpson's Paradox}. Simpson's Paradox occurs when trends that exist for the data in aggregate either disappear or reverse when the data are broken down into groups.

Let's show how Simpson's Paradox manifests itself in the \texttt{credit\_ch6} data. Let's first visualize the distribution of the numerical explanatory variable \texttt{credit\_limit} with a histogram in Figure \ref{fig:credit-limit-quartiles}.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/credit-limit-quartiles-1} 

}

\caption{Histogram of credit limits and brackets.}\label{fig:credit-limit-quartiles}
\end{figure}

The vertical dashed lines are the \emph{quartiles} that cut up the variable \texttt{credit\_limit} into four equally sized groups. Let's think of these quartiles as converting our numerical variable \texttt{credit\_limit} into a categorical variable ``\texttt{credit\_limit} bracket'' with four levels. This means that

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  25\% of credit limits were between \$0 and \$3088. Let's assign these 100 people to the ``low'' \texttt{credit\_limit} bracket.
\item
  25\% of credit limits were between \$3088 and \$4622. Let's assign these 100 people to the ``medium-low'' \texttt{credit\_limit} bracket.
\item
  25\% of credit limits were between \$4622 and \$5873. Let's assign these 100 people to the ``medium-high'' \texttt{credit\_limit} bracket.
\item
  25\% of credit limits were over \$5873. Let's assign these 100 people to the ``high'' \texttt{credit\_limit} bracket.
\end{enumerate}

Now in Figure \ref{fig:2numxplot4} let's re-display two versions of the scatterplot of \texttt{debt} and \texttt{income} from Figure \ref{fig:2numxplot1-repeat}, but with a slight twist:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The left-hand plot shows the regular scatterplot and the single regression line, just as you saw in Figure \ref{fig:2numxplot1-repeat}.
\item
  The right-hand plot shows the \emph{colored scatterplot}, where the color aesthetic is mapped to ``\texttt{credit\_limit} bracket.'' Furthermore, there are now four separate regression lines.
\end{enumerate}

In other words, the location of the 400 points are the same in both scatterplots, but the right-hand plot shows an additional variable of information: \texttt{credit\_limit} bracket.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/2numxplot4-1} 

}

\caption{Relationship between credit card debt and income by credit limit bracket.}\label{fig:2numxplot4}
\end{figure}

The left-hand plot of Figure \ref{fig:2numxplot4} focuses on the relationship between \texttt{debt} and \texttt{income} in \emph{aggregate}. It is suggesting that overall there exists a positive relationship between \texttt{debt} and \texttt{income}. However, the right-hand plot of Figure \ref{fig:2numxplot4} focuses on the relationship between \texttt{debt} and \texttt{income} \emph{broken down by \texttt{credit\_limit} bracket}. In other words, we focus on four \emph{separate} relationships between \texttt{debt} and \texttt{income}: one for the ``low'' \texttt{credit\_limit} bracket, one for the ``medium-low'' \texttt{credit\_limit} bracket, and so on.

Observe in the right-hand plot that the relationship between \texttt{debt} and \texttt{income} is clearly negative for the ``medium-low'' and ``medium-high'' \texttt{credit\_limit} brackets, while the relationship is somewhat flat for the ``low'' \texttt{credit\_limit} bracket. The only \texttt{credit\_limit} bracket where the relationship remains positive is for the ``high'' \texttt{credit\_limit} bracket. However, this relationship is less positive than in the relationship in aggregate, since the slope is shallower than the slope of the regression line in the left-hand plot.

In this example of Simpson's Paradox, the \texttt{credit\_limit} is a \emph{confounding variable} of the relationship between credit card \texttt{debt} and \texttt{income}\index{confounding variable} as we defined in Subsection \ref{correlation-is-not-causation}. Thus, \texttt{credit\_limit} needs to be accounted for in any appropriate model for the relationship between \texttt{debt} and \texttt{income}.

\hypertarget{mult-reg-conclusion}{%
\section{Conclusion}\label{mult-reg-conclusion}}

\hypertarget{additional-resources-4}{%
\subsection{Additional resources}\label{additional-resources-4}}

Solutions to all \emph{Learning checks} can be found online in \href{https://moderndive.com/D-appendixD.html}{Appendix D}.

An R script file of all R code used in this chapter is available at \url{https://www.moderndive.com/scripts/06-multiple-regression.R}.

\hypertarget{whats-to-come-5}{%
\subsection{What's to come?}\label{whats-to-come-5}}

Congratulations! We've completed the ``Data Modeling with \texttt{moderndive}'' portion of this book. We're ready to proceed to Part III of this book: ``Statistical Inference with \texttt{infer}.'' Statistical inference is the science of inferring about some unknown quantity using sampling.

The most well-known examples of sampling in practice involve \emph{polls}. Because asking an entire population about their opinions would be a long and arduous task, pollsters often take a smaller sample that is hopefully representative of the population. Based on the results of this sample, pollsters hope to make claims about the entire population.

Once we've covered Chapters \ref{sampling} on sampling, \ref{confidence-intervals} on confidence intervals, and \ref{hypothesis-testing} on hypothesis testing, we'll revisit the regression models we studied in Chapters \ref{regression} and \ref{multiple-regression} in Chapter \ref{inference-for-regression} on inference for regression. So far, we've only studied the \texttt{estimate} column of all our regression tables. The next four chapters focus on what the remaining columns mean: the standard error (\texttt{std\_error}), the test \texttt{statistic}, the \texttt{p\_value}, and the lower and upper bounds of confidence intervals (\texttt{lower\_ci} and \texttt{upper\_ci}).

Furthermore in Chapter \ref{inference-for-regression}, we'll revisit the concept of residuals \(y - \widehat{y}\) and discuss their importance when interpreting the results of a regression model. We'll perform what is known as a \emph{residual analysis} of the \texttt{residual} variable of all \texttt{get\_regression\_points()} outputs. Residual analyses allow you to verify what are known as the \emph{conditions for inference for regression}. On to Chapter \ref{sampling} on sampling in Part III as shown in Figure \ref{fig:part3}!



\begin{figure}

{\centering \includegraphics[width=\textwidth]{images/flowcharts/flowchart/flowchart.006} 

}

\caption{\emph{ModernDive} flowchart - on to Part III!}\label{fig:part3}
\end{figure}



\hypertarget{part-refinferpart}{%
\part{Statistical Inference with \texttt{infer}}\label{part-refinferpart}}

\hypertarget{sampling}{%
\chapter{Sampling}\label{sampling}}

In this chapter, we kick off the third portion of this book on statistical inference by learning about \emph{sampling}. The concepts behind sampling form the basis of confidence intervals and hypothesis testing, which we'll cover in Chapters \ref{confidence-intervals} and \ref{hypothesis-testing}. We will see that the tools that you learned in the data science portion of this book, in particular data visualization and data wrangling, will also play an important role in the development of your understanding. As mentioned before, the concepts throughout this text all build into a culmination allowing you to ``tell your story with data.''

\hypertarget{sampling-packages}{%
\subsection*{Needed packages}\label{sampling-packages}}


Let's load all the packages needed for this chapter (this assumes you've already installed them). Recall from our discussion in Section \ref{tidyverse-package} that loading the \texttt{tidyverse} package by running \texttt{library(tidyverse)} loads the following commonly used data science packages all at once:

\begin{itemize}
\tightlist
\item
  \texttt{ggplot2} for data visualization
\item
  \texttt{dplyr} for data wrangling
\item
  \texttt{tidyr} for converting data to ``tidy'' format
\item
  \texttt{readr} for importing spreadsheet data into R
\item
  As well as the more advanced \texttt{purrr}, \texttt{tibble}, \texttt{stringr}, and \texttt{forcats} packages
\end{itemize}

If needed, read Section \ref{packages} for information on how to install and load R packages.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(moderndive)}
\end{Highlighting}
\end{Shaded}

\hypertarget{sampling-activity}{%
\section{Sampling bowl activity}\label{sampling-activity}}

Let's start with a hands-on activity.

\hypertarget{what-proportion-of-this-bowls-balls-are-red}{%
\subsection{What proportion of this bowl's balls are red?}\label{what-proportion-of-this-bowls-balls-are-red}}

Take a look at the bowl in Figure \ref{fig:sampling-exercise-1}. It has a certain number of red and a certain number of white balls all of equal size. (Note that in this printed version of the book ``red'' corresponds to the darker-colored balls, and ``white'' corresponds to the lighter-colored balls. We kept the reference to ``red'' and ``white'' throughout this book since those are the actual colors of the balls as seen in the background of the image on our book's \href{https://moderndive.com/images/logos/book_cover.png}{cover}.) Furthermore, it appears the bowl has been mixed beforehand, as there does not seem to be any coherent pattern to the spatial distribution of the red and white balls.

Let's now ask ourselves, what proportion of this bowl's balls are red?

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{images/sampling/balls/sampling_bowl_1} 

}

\caption{A bowl with red and white balls.}\label{fig:sampling-exercise-1}
\end{figure}

One way to answer this question would be to perform an exhaustive count: remove each ball individually, count the number of red balls and the number of white balls, and divide the number of red balls by the total number of balls. However, this would be a long and tedious process.

\hypertarget{using-the-shovel-once}{%
\subsection{Using the shovel once}\label{using-the-shovel-once}}

Instead of performing an exhaustive count, let's insert a shovel into the bowl as seen in Figure \ref{fig:sampling-exercise-2}. Using the shovel, let's remove \(5 \cdot 10 = 50\) balls, as seen in Figure \ref{fig:sampling-exercise-3}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/sampling/balls/sampling_bowl_2} 

}

\caption{Inserting a shovel into the bowl.}\label{fig:sampling-exercise-2}
\end{figure}

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/sampling/balls/sampling_bowl_3_cropped} 

}

\caption{Removing 50 balls from the bowl.}\label{fig:sampling-exercise-3}
\end{figure}

Observe that 17 of the balls are red and thus 0.34 = 34\% of the shovel's balls are red. We can view the proportion of balls that are red in this shovel as a guess of the proportion of balls that are red in the entire bowl. While not as exact as doing an exhaustive count of all the balls in the bowl, our guess of 34\% took much less time and energy to make.

However, say, we started this activity over from the beginning. In other words, we replace the 50 balls back into the bowl and start over. Would we remove exactly 17 red balls again? In other words, would our guess at the proportion of the bowl's balls that are red be exactly 34\% again? Maybe?

What if we repeated this activity several times following the process shown in Figure \ref{fig:sampling-exercise-3b}? Would we obtain exactly 17 red balls each time? In other words, would our guess at the proportion of the bowl's balls that are red be exactly 34\% every time? Surely not. Let's repeat this exercise several times with the help of 33 groups of friends to understand how the value differs with repetition.

\hypertarget{student-shovels}{%
\subsection{Using the shovel 33 times}\label{student-shovels}}

Each of our 33 groups of friends will do the following:

\begin{itemize}
\tightlist
\item
  Use the shovel to remove 50 balls each.
\item
  Count the number of red balls and thus compute the proportion of the 50 balls that are red.
\item
  Return the balls into the bowl.
\item
  Mix the contents of the bowl a little to not let a previous group's results influence the next group's.
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=0.3\linewidth]{images/sampling/balls/tactile_2_a} \includegraphics[width=0.3\linewidth]{images/sampling/balls/tactile_2_b} \includegraphics[width=0.3\linewidth]{images/sampling/balls/tactile_2_c} 

}

\caption{Repeating sampling activity 33 times.}\label{fig:sampling-exercise-3b}
\end{figure}

Each of our 33 groups of friends make note of their proportion of red balls from their sample collected. Each group then marks their proportion of their 50 balls that were red in the appropriate bin in a hand-drawn histogram as seen in Figure \ref{fig:sampling-exercise-4}.

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{images/sampling/balls/tactile_3_a} 

}

\caption{Constructing a histogram of proportions.}\label{fig:sampling-exercise-4}
\end{figure}

Recall from Section \ref{histograms} that histograms allow us to visualize the \emph{distribution} \index{distribution} of a numerical variable. In particular, where the center of the values falls and how the values vary. A partially completed histogram of the first 10 out of 33 groups of friends' results can be seen in Figure \ref{fig:sampling-exercise-5}.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/sampling/balls/tactile_3_c} 

}

\caption{Hand-drawn histogram of first 10 out of 33 proportions.}\label{fig:sampling-exercise-5}
\end{figure}

Observe the following in the histogram in Figure \ref{fig:sampling-exercise-5}:

\begin{itemize}
\tightlist
\item
  At the low end, one group removed 50 balls from the bowl with proportion red between 0.20 and 0.25.
\item
  At the high end, another group removed 50 balls from the bowl with proportion between 0.45 and 0.5 red.
\item
  However, the most frequently occurring proportions were between 0.30 and 0.35 red, right in the middle of the distribution.
\item
  The shape of this distribution is somewhat bell-shaped.
\end{itemize}

Let's construct this same hand-drawn histogram in R using your data visualization skills that you honed in Chapter \ref{viz}. We saved our 33 groups of friends' results in the \texttt{tactile\_prop\_red} data frame included in the \texttt{moderndive} package. Run the following to display the first 10 of 33 rows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tactile\_prop\_red}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 33 x 4
   group            replicate red_balls prop_red
   <chr>                <int>     <int>    <dbl>
 1 Ilyas, Yohan             1        21     0.42
 2 Morgan, Terrance         2        17     0.34
 3 Martin, Thomas           3        21     0.42
 4 Clark, Frank             4        21     0.42
 5 Riddhi, Karina           5        18     0.36
 6 Andrew, Tyler            6        19     0.38
 7 Julia                    7        19     0.38
 8 Rachel, Lauren           8        11     0.22
 9 Daniel, Caroline         9        15     0.3 
10 Josh, Maeve             10        17     0.34
# ... with 23 more rows
\end{verbatim}

Observe for each \texttt{group} that we have their names, the number of \texttt{red\_balls} they obtained, and the corresponding proportion out of 50 balls that were red named \texttt{prop\_red}. We also have a \texttt{replicate} variable enumerating each of the 33 groups. We chose this name because each row can be viewed as one instance of a replicated (in other words repeated) activity: using the shovel to remove 50 balls and computing the proportion of those balls that are red.

Let's visualize the distribution of these 33 proportions using \texttt{geom\_histogram()} with \texttt{binwidth\ =\ 0.05} in Figure \ref{fig:samplingdistribution-tactile}. This is a computerized and complete version of the partially completed hand-drawn histogram you saw in Figure \ref{fig:sampling-exercise-5}. Note that setting \texttt{boundary\ =\ 0.4} indicates that we want a binning scheme such that one of the bins' boundary is at 0.4. This helps us to more closely align this histogram with the hand-drawn histogram in Figure \ref{fig:sampling-exercise-5}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(tactile\_prop\_red, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ prop\_red)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \FloatTok{0.05}\NormalTok{, }\AttributeTok{boundary =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{color =} \StringTok{"white"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Proportion of 50 balls that were red"}\NormalTok{, }
       \AttributeTok{title =} \StringTok{"Distribution of 33 proportions red"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/samplingdistribution-tactile-1} 

}

\caption{Distribution of 33 proportions based on 33 samples of size 50.}\label{fig:samplingdistribution-tactile}
\end{figure}

\hypertarget{what-did-we-just-do}{%
\subsection{What did we just do?}\label{what-did-we-just-do}}

What we just demonstrated in this activity is the statistical concept of \index{sampling} \emph{sampling}. We would like to know the proportion of the bowl's balls that are red. Because the bowl has a large number of balls, performing an exhaustive count of the red and white balls would be time-consuming. We thus extracted a \emph{sample} of 50 balls using the shovel to make an \emph{estimate}. Using this sample of 50 balls, we estimated the proportion of the \emph{bowl's} balls that are red to be 34\%.

Moreover, because we mixed the balls before each use of the shovel, the samples were randomly drawn. Because each sample was drawn at random, the samples were different from each other. Because the samples were different from each other, we obtained the different proportions red observed in Figure \ref{fig:samplingdistribution-tactile}. This is known as the concept of \emph{sampling variation}. \index{sampling!variation}

The purpose of this sampling activity was to develop an understanding of two key concepts relating to sampling:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Understanding the effect of sampling variation.
\item
  Understanding the effect of sample size on sampling variation.
\end{enumerate}

In Section \ref{sampling-simulation}, we'll mimic the hands-on sampling activity we just performed on a computer. This will allow us not only to repeat the sampling exercise much more than 33 times, but it will also allow us to use shovels with different numbers of slots than just 50.

Afterwards, we'll present you with definitions, terminology, and notation related to sampling in Section \ref{sampling-framework}. As in many disciplines, such necessary background knowledge may seem inaccessible and even confusing at first. However, as with many difficult topics, if you truly understand the underlying concepts and practice, practice, practice, you'll be able to master them.

To tie the contents of this chapter to the real world, we'll present an example of one of the most recognizable uses of sampling: polls. In Section \ref{sampling-case-study} we'll look at a particular case study: a 2013 poll on then U.S. President Barack Obama's popularity among young Americans, conducted by Kennedy School's Institute of Politics at Harvard University. To close this chapter, we'll generalize the ``sampling from a bowl'' exercise to other sampling scenarios and present a theoretical result known as the \emph{Central Limit Theorem}.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC7.1)} Why was it important to mix the bowl before we sampled the balls?

\textbf{(LC7.2)} Why is it that our 33 groups of friends did not all have the same numbers of balls that were red out of 50, and hence different proportions red?

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{sampling-simulation}{%
\section{Virtual sampling}\label{sampling-simulation}}

In the previous Section \ref{sampling-activity}, we performed a \emph{tactile} sampling activity by hand. In other words, we used a physical bowl of balls and a physical shovel. We performed this sampling activity by hand first so that we could develop a firm understanding of the root ideas behind sampling. In this section, we'll mimic this tactile sampling activity with a \emph{virtual} sampling activity using a computer. In other words, we'll use a virtual analog to the bowl of balls and a virtual analog to the shovel.

\hypertarget{using-the-virtual-shovel-once}{%
\subsection{Using the virtual shovel once}\label{using-the-virtual-shovel-once}}

Let's start by performing the virtual analog of the tactile sampling exercise we performed in Section \ref{sampling-activity}. We first need a virtual analog of the bowl seen in Figure \ref{fig:sampling-exercise-1}. To this end, we included a data frame named \texttt{bowl} in the \texttt{moderndive} package. The rows of \texttt{bowl} correspond exactly with the contents of the actual bowl.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bowl}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2,400 x 2
   ball_ID color
     <int> <chr>
 1       1 white
 2       2 white
 3       3 white
 4       4 red  
 5       5 white
 6       6 white
 7       7 red  
 8       8 white
 9       9 red  
10      10 white
# ... with 2,390 more rows
\end{verbatim}

Observe that \texttt{bowl} has 2400 rows, telling us that the bowl contains 2400 equally sized balls. The first variable \texttt{ball\_ID} is used as an \emph{identification variable} as discussed in Subsection \ref{identification-vs-measurement-variables}; none of the balls in the actual bowl are marked with numbers. The second variable \texttt{color} indicates whether a particular virtual ball is red or white. View the contents of the bowl in RStudio's data viewer and scroll through the contents to convince yourself that \texttt{bowl} is indeed a virtual analog of the actual bowl in Figure \ref{fig:sampling-exercise-1}.

Now that we have a virtual analog of our bowl, we now need a virtual analog to the shovel seen in Figure \ref{fig:sampling-exercise-2} to generate virtual samples of 50 balls. We're going to use the \texttt{rep\_sample\_n()} function included in the \texttt{moderndive} package. This function allows us to take \texttt{rep}eated, or \texttt{rep}licated, \texttt{samples} of size \texttt{n}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{virtual\_shovel }\OtherTok{\textless{}{-}}\NormalTok{ bowl }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rep\_sample\_n}\NormalTok{(}\AttributeTok{size =} \DecValTok{50}\NormalTok{)}
\NormalTok{virtual\_shovel}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 50 x 3
# Groups:   replicate [1]
   replicate ball_ID color
       <int>   <int> <chr>
 1         1    1970 white
 2         1     842 red  
 3         1    2287 white
 4         1     599 white
 5         1     108 white
 6         1     846 red  
 7         1     390 red  
 8         1     344 white
 9         1     910 white
10         1    1485 white
# ... with 40 more rows
\end{verbatim}

Observe that \texttt{virtual\_shovel} has 50 rows corresponding to our virtual sample of size 50. The \texttt{ball\_ID} variable identifies which of the 2400 balls from \texttt{bowl} are included in our sample of 50 balls while \texttt{color} denotes its color. However, what does the \texttt{replicate} variable indicate? In \texttt{virtual\_shovel}'s case, \texttt{replicate} is equal to 1 for all 50 rows. This is telling us that these 50 rows correspond to the first repeated/replicated use of the shovel, in our case our first sample. We'll see shortly that when we ``virtually'' take 33 samples, \texttt{replicate} will take values between 1 and 33.

Let's compute the proportion of balls in our virtual sample that are red using the \texttt{dplyr} data wrangling verbs you learned in Chapter \ref{wrangling}. First, for each of our 50 sampled balls, let's identify if it is red or not using a test for equality with \texttt{==}. Let's create a new Boolean variable \texttt{is\_red} using the \texttt{mutate()} function from Section \ref{mutate}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{virtual\_shovel }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{is\_red =}\NormalTok{ (color }\SpecialCharTok{==} \StringTok{"red"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 50 x 4
# Groups:   replicate [1]
   replicate ball_ID color is_red
       <int>   <int> <chr> <lgl> 
 1         1    1970 white FALSE 
 2         1     842 red   TRUE  
 3         1    2287 white FALSE 
 4         1     599 white FALSE 
 5         1     108 white FALSE 
 6         1     846 red   TRUE  
 7         1     390 red   TRUE  
 8         1     344 white FALSE 
 9         1     910 white FALSE 
10         1    1485 white FALSE 
# ... with 40 more rows
\end{verbatim}

Observe that for every row where \texttt{color\ ==\ "red"}, the Boolean (logical) value \texttt{TRUE} is returned and for every row where \texttt{color} is not equal to \texttt{"red"}, the Boolean \texttt{FALSE} is returned.

Second, let's compute the number of balls out of 50 that are red using the \texttt{summarize()} function. Recall from Section \ref{summarize} that \texttt{summarize()} takes a data frame with many rows and returns a data frame with a single row containing summary statistics, like the \texttt{mean()} or \texttt{median()}. In this case, we use the \texttt{sum()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{virtual\_shovel }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{is\_red =}\NormalTok{ (color }\SpecialCharTok{==} \StringTok{"red"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{num\_red =} \FunctionTok{sum}\NormalTok{(is\_red))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 2
  replicate num_red
      <int>   <int>
1         1      12
\end{verbatim}

Why does this work? Because R treats \texttt{TRUE} like the number \texttt{1} and \texttt{FALSE} like the number \texttt{0}. So summing the number of \texttt{TRUE}s and \texttt{FALSE}s is equivalent to summing \texttt{1}'s and \texttt{0}'s. In the end, this operation counts the number of balls where \texttt{color} is \texttt{red}. In our case, 12 of the 50 balls were red. However, you might have gotten a different number red because of the randomness of the virtual sampling.

Third and lastly, let's compute the proportion of the 50 sampled balls that are red by dividing \texttt{num\_red} by 50:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{virtual\_shovel }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{is\_red =}\NormalTok{ color }\SpecialCharTok{==} \StringTok{"red"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{num\_red =} \FunctionTok{sum}\NormalTok{(is\_red)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{prop\_red =}\NormalTok{ num\_red }\SpecialCharTok{/} \DecValTok{50}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 3
  replicate num_red prop_red
      <int>   <int>    <dbl>
1         1      12     0.24
\end{verbatim}

In other words, 24\% of this virtual sample's balls were red. Let's make this code a little more compact and succinct by combining the first \texttt{mutate()} and the \texttt{summarize()} as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{virtual\_shovel }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{num\_red =} \FunctionTok{sum}\NormalTok{(color }\SpecialCharTok{==} \StringTok{"red"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{prop\_red =}\NormalTok{ num\_red }\SpecialCharTok{/} \DecValTok{50}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 3
  replicate num_red prop_red
      <int>   <int>    <dbl>
1         1      12     0.24
\end{verbatim}

Great! 24\% of \texttt{virtual\_shovel}'s 50 balls were red! So based on this particular sample of 50 balls, our guess at the proportion of the \texttt{bowl}'s balls that are red is 24\%. But remember from our earlier tactile sampling activity that if we repeat this sampling, we will not necessarily obtain the same value of 24\% again. There will likely be some variation. In fact, our 33 groups of friends computed 33 such proportions whose distribution we visualized in Figure \ref{fig:sampling-exercise-5}. We saw that these estimates \emph{varied}. Let's now perform the virtual analog of having 33 groups of students use the sampling shovel!

\hypertarget{using-the-virtual-shovel-33-times}{%
\subsection{Using the virtual shovel 33 times}\label{using-the-virtual-shovel-33-times}}

Recall that in our tactile sampling exercise in Section \ref{sampling-activity}, we had 33 groups of students each use the shovel, yielding 33 samples of size 50 balls. We then used these 33 samples to compute 33 proportions. In other words, we repeated/replicated using the shovel 33 times. We can perform this repeated/replicated sampling virtually by once again using our virtual shovel function \texttt{rep\_sample\_n()}, but by adding the \texttt{reps\ =\ 33} argument. This is telling R that we want to repeat the sampling 33 times.

We'll save these results in a data frame called \texttt{virtual\_samples}. While we provide a preview of the first 10 rows of \texttt{virtual\_samples} in what follows, we highly suggest you scroll through its contents using RStudio's spreadsheet viewer by running \texttt{View(virtual\_samples)}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{virtual\_samples }\OtherTok{\textless{}{-}}\NormalTok{ bowl }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rep\_sample\_n}\NormalTok{(}\AttributeTok{size =} \DecValTok{50}\NormalTok{, }\AttributeTok{reps =} \DecValTok{33}\NormalTok{)}
\NormalTok{virtual\_samples}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,650 x 3
# Groups:   replicate [33]
   replicate ball_ID color
       <int>   <int> <chr>
 1         1     875 white
 2         1    1851 red  
 3         1    1548 red  
 4         1    1975 white
 5         1     835 white
 6         1      16 white
 7         1     327 white
 8         1    1803 red  
 9         1     740 red  
10         1     179 red  
# ... with 1,640 more rows
\end{verbatim}

Observe in the spreadsheet viewer that the first 50 rows of \texttt{replicate} are equal to \texttt{1} while the next 50 rows of \texttt{replicate} are equal to \texttt{2}. This is telling us that the first 50 rows correspond to the first sample of 50 balls while the next 50 rows correspond to the second sample of 50 balls. This pattern continues for all \texttt{reps\ =\ 33} replicates and thus \texttt{virtual\_samples} has 33 \(\cdot\) 50 = 1650 rows.

Let's now take \texttt{virtual\_samples} and compute the resulting 33 proportions red. We'll use the same \texttt{dplyr} verbs as before, but this time with an additional \texttt{group\_by()} of the \texttt{replicate} variable. Recall from Section \ref{groupby} that by assigning the grouping variable ``meta-data'' before we \texttt{summarize()}, we'll obtain 33 different proportions red. We display a preview of the first 10 out of 33 rows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{virtual\_prop\_red }\OtherTok{\textless{}{-}}\NormalTok{ virtual\_samples }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(replicate) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{red =} \FunctionTok{sum}\NormalTok{(color }\SpecialCharTok{==} \StringTok{"red"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{prop\_red =}\NormalTok{ red }\SpecialCharTok{/} \DecValTok{50}\NormalTok{)}
\NormalTok{virtual\_prop\_red}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 33 x 3
   replicate   red prop_red
       <int> <int>    <dbl>
 1         1    23     0.46
 2         2    19     0.38
 3         3    18     0.36
 4         4    19     0.38
 5         5    15     0.3 
 6         6    21     0.42
 7         7    21     0.42
 8         8    16     0.32
 9         9    24     0.48
10        10    14     0.28
# ... with 23 more rows
\end{verbatim}

As with our 33 groups of friends' tactile samples, there is variation in the resulting 33 virtual proportions red. Let's visualize this variation in a histogram in Figure \ref{fig:samplingdistribution-virtual}. Note that we add \texttt{binwidth\ =\ 0.05} and \texttt{boundary\ =\ 0.4} arguments as well. Recall that setting \texttt{boundary\ =\ 0.4} ensures a binning scheme with one of the bins' boundaries at 0.4. Since the \texttt{binwidth\ =\ 0.05} is also set, this will create bins with boundaries at 0.30, 0.35, 0.45, 0.5, etc. as well.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(virtual\_prop\_red, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ prop\_red)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \FloatTok{0.05}\NormalTok{, }\AttributeTok{boundary =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{color =} \StringTok{"white"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Proportion of 50 balls that were red"}\NormalTok{, }
       \AttributeTok{title =} \StringTok{"Distribution of 33 proportions red"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/samplingdistribution-virtual-1} 

}

\caption{Distribution of 33 proportions based on 33 samples of size 50.}\label{fig:samplingdistribution-virtual}
\end{figure}

Observe that we occasionally obtained proportions red that are less than 30\%. On the other hand, we occasionally obtained proportions that are greater than 45\%. However, the most frequently occurring proportions were between 35\% and 40\% (for 11 out of 33 samples). Why do we have these differences in proportions red? Because of \emph{sampling variation}.

Let's now compare our virtual results with our tactile results from the previous section in Figure \ref{fig:tactile-vs-virtual}. Observe that both histograms are somewhat similar in their center and variation, although not identical. These slight differences are again due to random sampling variation. Furthermore, observe that both distributions are somewhat bell-shaped.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/tactile-vs-virtual-1} 

}

\caption{Comparing 33 virtual and 33 tactile proportions red.}\label{fig:tactile-vs-virtual}
\end{figure}

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC7.3)} Why couldn't we study the effects of sampling variation when we used the virtual shovel only once? Why did we need to take more than one virtual sample (in our case 33 virtual samples)?

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{shovel-1000-times}{%
\subsection{Using the virtual shovel 1000 times}\label{shovel-1000-times}}

Now say we want to study the effects of sampling variation not for 33 samples, but rather for a larger number of samples, say 1000. We have two choices at this point. We could have our groups of friends manually take 1000 samples of 50 balls and compute the corresponding 1000 proportions. However, this would be a tedious and time-consuming task. This is where computers excel: automating long and repetitive tasks while performing them quite quickly. Thus, at this point we will abandon tactile sampling in favor of only virtual sampling. Let's once again use the \texttt{rep\_sample\_n()} function with sample \texttt{size} set to be 50 once again, but this time with the number of replicates \texttt{reps} set to \texttt{1000}. Be sure to scroll through the contents of \texttt{virtual\_samples} in RStudio's viewer.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{virtual\_samples }\OtherTok{\textless{}{-}}\NormalTok{ bowl }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rep\_sample\_n}\NormalTok{(}\AttributeTok{size =} \DecValTok{50}\NormalTok{, }\AttributeTok{reps =} \DecValTok{1000}\NormalTok{)}
\NormalTok{virtual\_samples}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 50,000 x 3
# Groups:   replicate [1,000]
   replicate ball_ID color
       <int>   <int> <chr>
 1         1    1236 red  
 2         1    1944 red  
 3         1    1939 white
 4         1     780 white
 5         1    1956 white
 6         1    1003 white
 7         1    2113 white
 8         1    2213 white
 9         1     782 white
10         1     898 white
# ... with 49,990 more rows
\end{verbatim}

Observe that now \texttt{virtual\_samples} has 1000 \(\cdot\) 50 = 50,000 rows, instead of the 33 \(\cdot\) 50 = 1650 rows from earlier. Using the same data wrangling code as earlier, let's take the data frame \texttt{virtual\_samples} with 1000 \(\cdot\) 50 = 50,000 rows and compute the resulting 1000 proportions of red balls.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{virtual\_prop\_red }\OtherTok{\textless{}{-}}\NormalTok{ virtual\_samples }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(replicate) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{red =} \FunctionTok{sum}\NormalTok{(color }\SpecialCharTok{==} \StringTok{"red"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{prop\_red =}\NormalTok{ red }\SpecialCharTok{/} \DecValTok{50}\NormalTok{)}
\NormalTok{virtual\_prop\_red}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,000 x 3
   replicate   red prop_red
       <int> <int>    <dbl>
 1         1    18     0.36
 2         2    19     0.38
 3         3    20     0.4 
 4         4    15     0.3 
 5         5    17     0.34
 6         6    16     0.32
 7         7    23     0.46
 8         8    23     0.46
 9         9    15     0.3 
10        10    18     0.36
# ... with 990 more rows
\end{verbatim}

Observe that we now have 1000 replicates of \texttt{prop\_red}, the proportion of 50 balls that are red. Using the same code as earlier, let's now visualize the distribution of these 1000 replicates of \texttt{prop\_red} in a histogram in Figure \ref{fig:samplingdistribution-virtual-1000}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(virtual\_prop\_red, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ prop\_red)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \FloatTok{0.05}\NormalTok{, }\AttributeTok{boundary =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{color =} \StringTok{"white"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Proportion of 50 balls that were red"}\NormalTok{, }
       \AttributeTok{title =} \StringTok{"Distribution of 1000 proportions red"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/samplingdistribution-virtual-1000-1} 

}

\caption{Distribution of 1000 proportions based on 1000 samples of size 50.}\label{fig:samplingdistribution-virtual-1000}
\end{figure}

Once again, the most frequently occurring proportions of red balls occur between 35\% and 40\%. Every now and then, we obtain proportions as low as between 20\% and 25\%, and others as high as between 55\% and 60\%. These are rare, however. Furthermore, observe that we now have a much more symmetric and smoother bell-shaped distribution. This distribution is, in fact, approximated well by a normal distribution. At this point we recommend you read the ``Normal distribution'' section (Appendix \ref{appendix-normal-curve}) for a brief discussion on the properties of the normal distribution.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC7.4)} Why did we not take 1000 ``tactile'' samples of 50 balls by hand?

\textbf{(LC7.5)} Looking at Figure \ref{fig:samplingdistribution-virtual-1000}, would you say that sampling 50 balls where 30\% of them were red is likely or not? What about sampling 50 balls where 10\% of them were red?

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{different-shovels}{%
\subsection{Using different shovels}\label{different-shovels}}

Now say instead of just one shovel, you have three choices of shovels to extract a sample of balls with: shovels of size 25, 50, and 100.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/sampling/balls/three_shovels} 

}

\caption{Three shovels to extract three different sample sizes.}\label{fig:three-shovels}
\end{figure}

If your goal is still to estimate the proportion of the bowl's balls that are red, which shovel would you choose? In our experience, most people would choose the largest shovel with 100 slots because it would yield the ``best'' guess of the proportion of the bowl's balls that are red. Let's define some criteria for ``best'' in this subsection.

Using our newly developed tools for virtual sampling, let's unpack the effect of having different sample sizes! In other words, let's use \texttt{rep\_sample\_n()} with \texttt{size} set to \texttt{25}, \texttt{50}, and \texttt{100}, respectively, while keeping the number of repeated/replicated samples at 1000:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Virtually use the appropriate shovel to generate 1000 samples with \texttt{size} balls.
\item
  Compute the resulting 1000 replicates of the proportion of the shovel's balls that are red.
\item
  Visualize the distribution of these 1000 proportions red using a histogram.
\end{enumerate}

Run each of the following code segments individually and then compare the three resulting histograms.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Segment 1: sample size = 25 {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\# 1.a) Virtually use shovel 1000 times}
\NormalTok{virtual\_samples\_25 }\OtherTok{\textless{}{-}}\NormalTok{ bowl }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rep\_sample\_n}\NormalTok{(}\AttributeTok{size =} \DecValTok{25}\NormalTok{, }\AttributeTok{reps =} \DecValTok{1000}\NormalTok{)}

\CommentTok{\# 1.b) Compute resulting 1000 replicates of proportion red}
\NormalTok{virtual\_prop\_red\_25 }\OtherTok{\textless{}{-}}\NormalTok{ virtual\_samples\_25 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(replicate) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{red =} \FunctionTok{sum}\NormalTok{(color }\SpecialCharTok{==} \StringTok{"red"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{prop\_red =}\NormalTok{ red }\SpecialCharTok{/} \DecValTok{25}\NormalTok{)}

\CommentTok{\# 1.c) Plot distribution via a histogram}
\FunctionTok{ggplot}\NormalTok{(virtual\_prop\_red\_25, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ prop\_red)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \FloatTok{0.05}\NormalTok{, }\AttributeTok{boundary =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{color =} \StringTok{"white"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Proportion of 25 balls that were red"}\NormalTok{, }\AttributeTok{title =} \StringTok{"25"}\NormalTok{) }


\CommentTok{\# Segment 2: sample size = 50 {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\# 2.a) Virtually use shovel 1000 times}
\NormalTok{virtual\_samples\_50 }\OtherTok{\textless{}{-}}\NormalTok{ bowl }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rep\_sample\_n}\NormalTok{(}\AttributeTok{size =} \DecValTok{50}\NormalTok{, }\AttributeTok{reps =} \DecValTok{1000}\NormalTok{)}

\CommentTok{\# 2.b) Compute resulting 1000 replicates of proportion red}
\NormalTok{virtual\_prop\_red\_50 }\OtherTok{\textless{}{-}}\NormalTok{ virtual\_samples\_50 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(replicate) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{red =} \FunctionTok{sum}\NormalTok{(color }\SpecialCharTok{==} \StringTok{"red"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{prop\_red =}\NormalTok{ red }\SpecialCharTok{/} \DecValTok{50}\NormalTok{)}

\CommentTok{\# 2.c) Plot distribution via a histogram}
\FunctionTok{ggplot}\NormalTok{(virtual\_prop\_red\_50, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ prop\_red)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \FloatTok{0.05}\NormalTok{, }\AttributeTok{boundary =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{color =} \StringTok{"white"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Proportion of 50 balls that were red"}\NormalTok{, }\AttributeTok{title =} \StringTok{"50"}\NormalTok{)  }


\CommentTok{\# Segment 3: sample size = 100 {-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}{-}}
\CommentTok{\# 3.a) Virtually using shovel with 100 slots 1000 times}
\NormalTok{virtual\_samples\_100 }\OtherTok{\textless{}{-}}\NormalTok{ bowl }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rep\_sample\_n}\NormalTok{(}\AttributeTok{size =} \DecValTok{100}\NormalTok{, }\AttributeTok{reps =} \DecValTok{1000}\NormalTok{)}

\CommentTok{\# 3.b) Compute resulting 1000 replicates of proportion red}
\NormalTok{virtual\_prop\_red\_100 }\OtherTok{\textless{}{-}}\NormalTok{ virtual\_samples\_100 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(replicate) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{red =} \FunctionTok{sum}\NormalTok{(color }\SpecialCharTok{==} \StringTok{"red"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{prop\_red =}\NormalTok{ red }\SpecialCharTok{/} \DecValTok{100}\NormalTok{)}

\CommentTok{\# 3.c) Plot distribution via a histogram}
\FunctionTok{ggplot}\NormalTok{(virtual\_prop\_red\_100, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ prop\_red)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \FloatTok{0.05}\NormalTok{, }\AttributeTok{boundary =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{color =} \StringTok{"white"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Proportion of 100 balls that were red"}\NormalTok{, }\AttributeTok{title =} \StringTok{"100"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

For easy comparison, we present the three resulting histograms in a single row with matching x and y axes in Figure \ref{fig:comparing-sampling-distributions}.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/comparing-sampling-distributions-1} 

}

\caption{Comparing the distributions of proportion red for different sample sizes.}\label{fig:comparing-sampling-distributions}
\end{figure}

Observe that as the sample size increases, the variation of the 1000 replicates of the proportion of red decreases. In other words, as the sample size increases, there are fewer differences due to sampling variation and the distribution centers more tightly around the same value. Eyeballing Figure \ref{fig:comparing-sampling-distributions}, all three histograms appear to center around roughly 40\%.

We can be numerically explicit about the amount of variation in our three sets of 1000 values of \texttt{prop\_red} using the \index{standard deviation} \emph{standard deviation}. A standard deviation is a summary statistic that measures the amount of variation within a numerical variable (see Appendix \ref{appendix-stat-terms} for a brief discussion on the properties of the standard deviation). For all three sample sizes, let's compute the standard deviation of the 1000 proportions red by running the following data wrangling code that uses the \texttt{sd()} summary function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# n = 25}
\NormalTok{virtual\_prop\_red\_25 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{sd =} \FunctionTok{sd}\NormalTok{(prop\_red))}

\CommentTok{\# n = 50}
\NormalTok{virtual\_prop\_red\_50 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{sd =} \FunctionTok{sd}\NormalTok{(prop\_red))}

\CommentTok{\# n = 100}
\NormalTok{virtual\_prop\_red\_100 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{sd =} \FunctionTok{sd}\NormalTok{(prop\_red))}
\end{Highlighting}
\end{Shaded}

Let's compare these three measures of distributional variation in Table \ref{tab:comparing-n}.

\begin{table}[!h]

\caption{\label{tab:comparing-n}Comparing standard deviations of proportions red for three different shovels}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{rr}
\toprule
Number of slots in shovel & Standard deviation of proportions red\\
\midrule
25 & 0.094\\
50 & 0.069\\
100 & 0.045\\
\bottomrule
\end{tabular}
\end{table}

As we observed in Figure \ref{fig:comparing-sampling-distributions}, as the sample size increases, the variation decreases. In other words, there is less variation in the 1000 values of the proportion red. So as the sample size increases, our guesses at the true proportion of the bowl's balls that are red get more precise.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC7.6)} In Figure \ref{fig:comparing-sampling-distributions}, we used shovels to take 1000 samples each, computed the resulting 1000 proportions of the shovel's balls that were red, and then visualized the distribution of these 1000 proportions in a histogram. We did this for shovels with 25, 50, and 100 slots in them. As the size of the shovels increased, the histograms got narrower. In other words, as the size of the shovels increased from 25 to 50 to 100, did the 1000 proportions

\begin{itemize}
\tightlist
\item
  A. vary less,
\item
  B. vary by the same amount, or
\item
  C. vary more?
\end{itemize}

\textbf{(LC7.7)} What summary statistic did we use to quantify how much the 1000 proportions red varied?

\begin{itemize}
\tightlist
\item
  A. The interquartile range
\item
  B. The standard deviation
\item
  C. The range: the largest value minus the smallest.
\end{itemize}

\begin{learncheck}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{sampling-framework}{%
\section{Sampling framework}\label{sampling-framework}}

In both our tactile and our virtual sampling activities, we used sampling for the purpose of estimation. We extracted samples in order to \emph{estimate} the proportion of the bowl's balls that are red. We used sampling as a less time-consuming approach than performing an exhaustive count of all the balls. Our virtual sampling activity built up to the results shown in Figure \ref{fig:comparing-sampling-distributions} and Table \ref{tab:comparing-n}: comparing 1000 proportions red based on samples of size 25, 50, and 100. This was our first attempt at understanding two key concepts relating to sampling for estimation:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The effect of \emph{sampling variation} on our estimates.
\item
  The effect of sample size on \emph{sampling variation}.
\end{enumerate}

Let's now introduce some terminology and notation as well as statistical definitions related to sampling. Given the number of new words you'll need to learn, you will likely have to read this section a few times. Keep in mind, however, that all of the concepts underlying these terminology, notation, and definitions tie directly to the concepts underlying our tactile and virtual sampling activities. It will simply take time and practice to master them.

\hypertarget{terminology-and-notation}{%
\subsection{Terminology and notation}\label{terminology-and-notation}}

Here is a list of terminology and mathematical notation relating to sampling.

First, a \textbf{population} is a collection of individuals or observations we are interested in. This is also commonly denoted as a \textbf{study population}. We mathematically denote the population's size using upper-case \(N\). In our sampling activities, the (study) population is the collection of \(N\) = 2400 identically sized red and white balls contained in the bowl.

Second, a \textbf{population parameter} is a numerical summary quantity about the population that is unknown, but you wish you knew. For example, when this quantity is a mean, the population parameter of interest is the \emph{population mean}. This is mathematically denoted with the Greek letter \(\mu\) pronounced ``mu'' (we'll see a sampling activity involving means in the upcoming Section \ref{resampling-tactile}). In our earlier sampling from the bowl activity, however, since we were interested in the proportion of the bowl's balls that were red, the population parameter is the \emph{population proportion}. This is mathematically denoted with the letter \(p\).

Third, a \textbf{census} is an exhaustive enumeration or counting of all \(N\) individuals or observations in the population in order to compute the population parameter's value \emph{exactly}. In our sampling activity, this would correspond to counting the number of balls out of \(N\) = 2400 that are red and computing the \emph{population proportion} \(p\) that are red \emph{exactly}. When the number \(N\) of individuals or observations in our population is large as was the case with our bowl, a census can be quite expensive in terms of time, energy, and money.

Fourth, \textbf{sampling} is the act of collecting a sample from the population when we don't have the means to perform a census. We mathematically denote the sample's size using lower case \(n\), as opposed to upper case \(N\) which denotes the population's size. Typically the sample size \(n\) is much smaller than the population size \(N\). Thus sampling is a much cheaper alternative than performing a census. In our sampling activities, we used shovels with 25, 50, and 100 slots to extract samples of size \(n\) = 25, \(n\) = 50, and \(n\) = 100.

Fifth, a \textbf{point estimate (AKA sample statistic)} is a summary statistic computed from a sample that \emph{estimates} an unknown population parameter. In our sampling activities, recall that the unknown population parameter was the population proportion and that this is mathematically denoted with \(p\). Our point estimate is the \emph{sample proportion}: the proportion of the shovel's balls that are red. In other words, it is our guess of the proportion of the bowl's balls that are red. We mathematically denote the sample proportion using \(\widehat{p}\). The ``hat'' on top of the \(p\) indicates that it is an estimate of the unknown population proportion \(p\).

Sixth is the idea of \textbf{representative sampling}. A sample is said to be a \emph{representative sample} if it roughly \emph{looks like} the population. In other words, are the sample's characteristics a good representation of the population's characteristics? In our sampling activity, are the samples of \(n\) balls extracted using our shovels representative of the bowl's \(N\) = 2400 balls?

Seventh is the idea of \textbf{generalizability}. We say a sample is generalizable if any results based on the sample can generalize to the population. In other words, does the value of the point estimate \emph{generalize} to the population? In our sampling activity, can we generalize the sample proportion from our shovels to the entire bowl? Using our mathematical notation, this is akin to asking if \(\widehat{p}\) is a ``good guess'' of \(p\)?

Eighth, we say \textbf{biased sampling} occurs if certain individuals or observations in a population have a higher chance of being included in a sample than others. We say a sampling procedure is \emph{unbiased} if every observation in a population had an equal chance of being sampled. In our sampling activities, since we mixed all \(N = 2400\) balls prior to each group's sampling and since each of the equally sized balls had an equal chance of being sampled, our samples were unbiased.

Ninth and lastly, the idea of \textbf{random sampling}. We say a sampling procedure is \emph{random} if we sample randomly from the population in an unbiased fashion. In our sampling activities, this would correspond to sufficiently mixing the bowl before each use of the shovel.

Phew, that's a lot of new terminology and notation to learn! Let's put them all together to describe the paradigm of sampling.

\textbf{In general:}

\begin{itemize}
\tightlist
\item
  If the sampling of a sample of size \(n\) is done at \textbf{random}, then
\item
  the sample is \textbf{unbiased} and \textbf{representative} of the population of size \(N\), thus
\item
  any result based on the sample can \textbf{generalize} to the population, thus
\item
  the point estimate is a \textbf{``good guess''} of the unknown population parameter, thus
\item
  instead of performing a census, we can \textbf{infer} about the population using sampling.
\end{itemize}

\textbf{Specific to our sampling activity:}

\begin{itemize}
\tightlist
\item
  If we extract a sample of \(n=50\) balls at \textbf{random}, in other words, we mix all of the equally sized balls before using the shovel, then
\item
  the contents of the shovel are an \textbf{unbiased representation} of the contents of the bowl's 2400 balls, thus
\item
  any result based on the shovel's balls can \textbf{generalize} to the bowl, thus
\item
  the sample proportion \(\widehat{p}\) of the \(n=50\) balls in the shovel that are red is a \textbf{``good guess''} of the population proportion \(p\) of the \(N=2400\) balls that are red, thus
\item
  instead of manually going over all 2400 balls in the bowl, we can \textbf{infer} about the bowl using the shovel.
\end{itemize}

Note that last word we wrote in bold: \textbf{infer}. The act of ``inferring'' means to deduce or conclude information from evidence and reasoning. In our sampling activities, we wanted to infer about the proportion of the bowl's balls that are red. \href{https://en.wikipedia.org/wiki/Statistical_inference}{\emph{Statistical inference}} is the ``theory, methods, and practice of forming judgments about the parameters of a population and the reliability of statistical relationships, typically on the basis of random sampling.'' In other words, statistical inference is the act of inference via sampling. In the upcoming Chapter \ref{confidence-intervals} on confidence intervals, we'll introduce the \texttt{infer} package, which makes statistical inference ``tidy'' and transparent. It is why this third portion of the book is called ``Statistical inference via infer.''

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC7.8)} In the case of our bowl activity, what is the \emph{population parameter}? Do we know its value?

\textbf{(LC7.9)} What would performing a census in our bowl activity correspond to? Why did we not perform a census?

\textbf{(LC7.10)} What purpose do \emph{point estimates} serve in general? What is the name of the point estimate specific to our bowl activity? What is its mathematical notation?

\textbf{(LC7.11)} How did we ensure that our tactile samples using the shovel were random?

\textbf{(LC7.12)} Why is it important that sampling be done \emph{at random}?

\textbf{(LC7.13)} What are we \emph{inferring} about the bowl based on the samples using the shovel?

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{sampling-definitions}{%
\subsection{Statistical definitions}\label{sampling-definitions}}

Now, for some important statistical definitions related to sampling. As a refresher of our 1000 repeated/replicated virtual samples of size \(n\) = 25, \(n\) = 50, and \(n\) = 100 in Section \ref{sampling-simulation}, let's display Figure \ref{fig:comparing-sampling-distributions} again as Figure \ref{fig:comparing-sampling-distributions-1b}.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/comparing-sampling-distributions-1b-1} 

}

\caption{Previously seen three distributions of the sample proportion $\widehat{p}$.}\label{fig:comparing-sampling-distributions-1b}
\end{figure}

These types of distributions have a special name: \textbf{sampling distributions}; \index{sampling distributions} their visualization displays the effect of sampling variation on the distribution of any point estimate, in this case, the sample proportion \(\widehat{p}\). Using these sampling distributions, for a given sample size \(n\), we can make statements about what values we can typically expect.

For example, observe the centers of all three sampling distributions: they are all roughly centered around \(0.4 = 40\%\). Furthermore, observe that while we are somewhat likely to observe sample proportions of red balls of \(0.2 = 20\%\) when using the shovel with 25 slots, we will almost never observe a proportion of 20\% when using the shovel with 100 slots. Observe also the effect of sample size on the sampling variation. As the sample size \(n\) increases from 25 to 50 to 100, \index{sampling distributions!relationship to sample size} the variation of the sampling distribution decreases and thus the values cluster more and more tightly around the same center of around 40\%. We quantified this variation using the standard deviation of our sample proportions in Table \ref{tab:comparing-n}, which we display again as Table \ref{tab:comparing-n-repeat}:

\begin{table}[!h]

\caption{\label{tab:comparing-n-repeat}Previously seen comparing standard deviations of proportions red for three different shovels}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{rr}
\toprule
Number of slots in shovel & Standard deviation of proportions red\\
\midrule
25 & 0.094\\
50 & 0.069\\
100 & 0.045\\
\bottomrule
\end{tabular}
\end{table}

So as the sample size increases, the standard deviation of the proportion of red balls decreases. This type of standard deviation has another special name: \index{standard error} \textbf{standard error}. Standard errors quantify the effect of sampling variation induced on our estimates. In other words, they quantify how much we can expect different proportions of a shovel's balls that are red \emph{to vary} from one sample to another sample to another sample, and so on. As a general rule, as sample size increases, the standard error decreases.

Unfortunately, these names confuse many people who are new to statistical inference. For example, it's common for people who are new to statistical inference to call the ``sampling distribution'' the ``sample distribution.'' Another additional source of confusion is the name ``standard deviation'' and ``standard error.'' Remember that a standard error is merely a \emph{kind} of standard deviation: the standard deviation of any point estimate from sampling. In other words, all standard errors are standard deviations, but not every standard deviation is necessarily a standard error.

To help reinforce these concepts, let's re-display Figure \ref{fig:comparing-sampling-distributions} but using our new terminology, notation, and definitions relating to sampling in Figure \ref{fig:comparing-sampling-distributions-2}.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/comparing-sampling-distributions-2-1} 

}

\caption{Three sampling distributions of the sample proportion $\widehat{p}$.}\label{fig:comparing-sampling-distributions-2}
\end{figure}

Furthermore, let's re-display Table \ref{tab:comparing-n} but using our new terminology, notation, and definitions relating to sampling in Table \ref{tab:comparing-n-2}.

\begin{table}[!h]

\caption{\label{tab:comparing-n-2}Standard errors of the sample proportion based on sample sizes of 25, 50, and 100}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{lr}
\toprule
Sample size (n) & Standard error of $\widehat{p}$\\
\midrule
n = 25 & 0.094\\
n = 50 & 0.069\\
n = 100 & 0.045\\
\bottomrule
\end{tabular}
\end{table}

Remember the key message of this last table: that as the sample size \(n\) goes up, the ``typical'' error of your point estimate will go down, as quantified by the \emph{standard error}.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC7.14)} What purpose did the \emph{sampling distributions} serve?

\textbf{(LC7.15)} What does the \emph{standard error} of the sample proportion \(\widehat{p}\) quantify?

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{moral-of-the-story}{%
\subsection{The moral of the story}\label{moral-of-the-story}}

Let's recap this section so far. We've seen that if a sample is generated at random, then the resulting point estimate is a ``good guess'' of the true unknown population parameter. In our sampling activities, since we made sure to mix the balls first before extracting a sample with the shovel, the resulting sample proportion \(\widehat{p}\) of the shovel's balls that were red was a ``good guess'' of the population proportion \(p\) of the bowl's balls that were red.

However, what do we mean by our point estimate being a ``good guess''? Sometimes, we'll get an estimate that is less than the true value of the population parameter, while at other times we'll get an estimate that is greater. This is due to sampling variation. However, despite this sampling variation, our estimates will ``on average'' be correct and thus will be centered at the true value. This is because our sampling was done at random and thus in an unbiased fashion.

In our sampling activities, sometimes our sample proportion \(\widehat{p}\) was less than the true population proportion \(p\), while at other times it was greater. This was due to the sampling variability. However, despite this sampling variation, our sample proportions \(\widehat{p}\) were ``on average'' correct and thus were centered at the true value of the population proportion \(p\). This is because we mixed our bowl before taking samples and thus the sampling was done at random and thus in an unbiased fashion. This is also known as having an \emph{accurate} estimate\index{accuracy}.

What was the value of the population proportion \(p\) of the \(N\) = 2400 balls in the actual bowl that were red? There were 900 red balls, for a proportion red of 900/2400 = 0.375 = 37.5\%! How do we know this? Did the authors do an exhaustive count of all the balls? No! They were listed in the contents of the box that the bowl came in! Hence we were able to make the contents of the virtual \texttt{bowl} match the tactile bowl:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bowl }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{sum\_red =} \FunctionTok{sum}\NormalTok{(color }\SpecialCharTok{==} \StringTok{"red"}\NormalTok{), }
            \AttributeTok{sum\_not\_red =} \FunctionTok{sum}\NormalTok{(color }\SpecialCharTok{!=} \StringTok{"red"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 2
  sum_red sum_not_red
    <int>       <int>
1     900        1500
\end{verbatim}

Let's re-display our sampling distributions from Figures \ref{fig:comparing-sampling-distributions} and \ref{fig:comparing-sampling-distributions-2}, but now with a vertical red line marking the true population proportion \(p\) of balls that are red = 37.5\% in Figure \ref{fig:comparing-sampling-distributions-3}. We see that while there is a certain amount of error in the sample proportions \(\widehat{p}\) for all three sampling distributions, on average the \(\widehat{p}\) are centered at the true population proportion red \(p\).

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/comparing-sampling-distributions-3-1} 

}

\caption{Three sampling distributions with population proportion $p$ marked by vertical line.}\label{fig:comparing-sampling-distributions-3}
\end{figure}

We also saw in this section that as your sample size \(n\) increases, your point estimates will vary less and less and be more and more concentrated around the true population parameter. This variation is quantified by the decreasing \emph{standard error}. In other words, the typical error of your point estimates will decrease. In our sampling exercise, as the sample size increased, the variation of our sample proportions \(\widehat{p}\) decreased. You can observe this behavior in Figure \ref{fig:comparing-sampling-distributions-3}. This is also known as having a \emph{precise} estimate\index{precision}.

So random sampling ensures our point estimates are \emph{accurate}, while on the other hand having a large sample size ensures our point estimates are \emph{precise}. While the terms ``accuracy'' and ``precision'' may sound like they mean the same thing, there is a subtle difference. Accuracy describes how ``on target'' our estimates are, whereas precision describes how ``consistent'' our estimates are. Figure \ref{fig:accuracy-vs-precision} illustrates the difference.

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth,height=0.75\textheight]{images/accuracy_vs_precision} 

}

\caption{Comparing accuracy and precision.}\label{fig:accuracy-vs-precision}
\end{figure}

At this point, you might be asking yourself: ``If we already knew the true proportion of the bowl's balls that are red was 37.5\%, then why did we do any sampling?''. You might also be asking: ``Why did we take 1000 repeated samples of size n = 25, 50, and 100? Shouldn't we be taking only \emph{one} sample that's as large as possible?''. If you did ask yourself these questions, your suspicion is merited!

The sampling activity involving the bowl is merely an \emph{idealized version} of how sampling is done in real life. We performed this exercise only to study and understand:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The effect of sampling variation.
\item
  The effect of sample size on sampling variation.
\end{enumerate}

This is not how sampling is done in real life. In a real-life scenario, we won't know what the true value of the population parameter is. Furthermore, we wouldn't take 1000 repeated/replicated samples, but rather a single sample that's as large as we can afford. In the next section, let's now study a real-life example of sampling: polls.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC7.16)} The table that follows is a version of Table \ref{tab:comparing-n-2} matching sample sizes \(n\) to different \emph{standard errors} of the sample proportion \(\widehat{p}\), but with the rows randomly re-ordered and the sample sizes removed. Fill in the table by matching the correct sample sizes to the correct standard errors.

\begin{table}[!h]

\caption{\label{tab:comparing-n-3}Standard errors of $\widehat{p}$ based on n = 25, 50, 100}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{lr}
\toprule
Sample size & Standard error of $\widehat{p}$\\
\midrule
n = & 0.094\\
n = & 0.045\\
n = & 0.069\\
\bottomrule
\end{tabular}
\end{table}

For the following four \emph{Learning checks}, let the \emph{estimate} be the sample proportion \(\widehat{p}\): the proportion of a shovel's balls that were red. It estimates the population proportion \(p\): the proportion of the bowl's balls that were red.

\textbf{(LC7.17)} What is the difference between an \emph{accurate} and a \emph{precise} estimate?

\textbf{(LC7.18)} How do we ensure that an estimate is \emph{accurate}? How do we ensure that an estimate is \emph{precise}?

\textbf{(LC7.19)} In a real-life situation, we would not take 1000 different samples to infer about a population, but rather only one. Then, what was the purpose of our exercises where we took 1000 different samples?

\textbf{(LC7.20)} Figure \ref{fig:accuracy-vs-precision} with the targets shows four combinations of ``accurate versus precise'' estimates. Draw four corresponding \emph{sampling distributions} of the sample proportion \(\widehat{p}\), like the one in the leftmost plot in Figure \ref{fig:comparing-sampling-distributions-3}.

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{sampling-case-study}{%
\section{Case study: Polls}\label{sampling-case-study}}

Let's now switch gears to a more realistic sampling scenario than our bowl activity: a poll. In practice, pollsters do not take 1000 repeated samples as we did in our previous sampling activities, but rather take only a \emph{single sample} that's as large as possible.

On December 4, 2013, National Public Radio in the US reported on a poll of President Obama's approval rating among young Americans aged 18-29 in an article, \href{https://www.npr.org/sections/itsallpolitics/2013/12/04/248793753/poll-support-for-obama-among-young-americans-eroding}{``Poll: Support For Obama Among Young Americans Eroding.''} The poll was conducted by the Kennedy School's Institute of Politics at Harvard University. A quote from the article:

\begin{quote}
After voting for him in large numbers in 2008 and 2012, young Americans are souring on President Obama.

According to a new Harvard University Institute of Politics poll, just 41 percent of millennials --- adults ages 18-29 --- approve of Obama's job performance, his lowest-ever standing among the group and an 11-point drop from April.
\end{quote}

Let's tie elements of the real-life poll in this news article with our ``tactile'' and ``virtual'' bowl activity from Sections \ref{sampling-activity} and \ref{sampling-simulation} using the terminology, notations, and definitions we learned in Section \ref{sampling-framework}. You'll see that our sampling activity with the bowl is an idealized version of what pollsters are trying to do in real life.

First, who is the \textbf{(Study) Population} of \(N\) individuals or observations of interest? \index{sampling!population}

\begin{itemize}
\tightlist
\item
  Bowl: \(N\) = 2400 identically sized red and white balls
\item
  Obama poll: \(N\) = ? young Americans aged 18-29
\end{itemize}

Second, what is the \textbf{population parameter}? \index{sampling!population parameter}

\begin{itemize}
\tightlist
\item
  Bowl: The population proportion \(p\) of \emph{all} the balls in the bowl that are red.
\item
  Obama poll: The population proportion \(p\) of \emph{all} young Americans who approve of Obama's job performance.
\end{itemize}

Third, what would a \textbf{census} look like? \index{sampling!census}

\begin{itemize}
\tightlist
\item
  Bowl: Manually going over all \(N\) = 2400 balls and exactly computing the population proportion \(p\) of the balls that are red.
\item
  Obama poll: Locating all \(N\) young Americans and asking them all if they approve of Obama's job performance. In this case, we don't even know what the population size \(N\) is!
\end{itemize}

Fourth, how do you perform \textbf{sampling} to obtain a sample of size \(n\)? \index{sampling}

\begin{itemize}
\tightlist
\item
  Bowl: Using a shovel with \(n\) slots.
\item
  Obama poll: One method is to get a list of phone numbers of all young Americans and pick out \(n\) phone numbers. In this poll's case, the sample size of this poll was \(n = 2089\) young Americans.
\end{itemize}

Fifth, what is your \textbf{point estimate (AKA sample statistic)} of the unknown population parameter?

\begin{itemize}
\tightlist
\item
  Bowl: The sample proportion \(\widehat{p}\) of the balls in the shovel that were red.
\item
  Obama poll: The sample proportion \(\widehat{p}\) of young Americans in the sample that approve of Obama's job performance. In this poll's case, \(\widehat{p} = 0.41 = 41\%\), the quoted percentage in the second paragraph of the article. \index{point estimate} \index{sample statistic}
\end{itemize}

Sixth, is the sampling procedure \textbf{representative}? \index{sampling!representative}

\begin{itemize}
\tightlist
\item
  Bowl: Are the contents of the shovel representative of the contents of the bowl? Because we mixed the bowl before sampling, we can feel confident that they are.
\item
  Obama poll: Is the sample of \(n = 2089\) young Americans representative of \emph{all} young Americans aged 18-29? This depends on whether the sampling was random.
\end{itemize}

Seventh, are the samples \textbf{generalizable} to the greater population? \index{generalizability}

\begin{itemize}
\tightlist
\item
  Bowl: Is the sample proportion \(\widehat{p}\) of the shovel's balls that are red a ``good guess'' of the population proportion \(p\) of the bowl's balls that are red? Given that the sample was representative, the answer is yes.
\item
  Obama poll: Is the sample proportion \(\widehat{p} = 0.41\) of the sample of young Americans who supported Obama a ``good guess'' of the population proportion \(p\) of all young Americans who supported Obama at this time in 2013? In other words, can we confidently say that roughly 41\% of \emph{all} young Americans approved of Obama at the time of the poll? Again, this depends on whether the sampling was random.
\end{itemize}

Eighth, is the sampling procedure \textbf{unbiased}? In other words, do all observations have an equal chance of being included in the sample? \index{bias}

\begin{itemize}
\tightlist
\item
  Bowl: Since each ball was equally sized and we mixed the bowl before using the shovel, each ball had an equal chance of being included in a sample and hence the sampling was unbiased.
\item
  Obama poll: Did all young Americans have an equal chance at being represented in this poll? Again, this depends on whether the sampling was random.
\end{itemize}

Ninth and lastly, was the sampling done at \textbf{random}? \index{sampling!random}

\begin{itemize}
\tightlist
\item
  Bowl: As long as you mixed the bowl sufficiently before sampling, your samples would be random.
\item
  Obama poll: Was the sample conducted at random? We can't answer this question without knowing about the \emph{sampling methodology}\index{sampling methodology} used by Kennedy School's Institute of Politics at Harvard University. We'll discuss this more at the end of this section.
\end{itemize}

In other words, the poll by Kennedy School's Institute of Politics at Harvard University can be thought of as \emph{an instance} of using the shovel to sample balls from the bowl. Furthermore, if another polling company conducted a similar poll of young Americans at roughly the same time, they would likely get a different estimate than 41\%. This is due to \emph{sampling variation}.

Let's now revisit the sampling paradigm from Subsection \ref{terminology-and-notation}:

\textbf{In general}:

\begin{itemize}
\tightlist
\item
  If the sampling of a sample of size \(n\) is done at \textbf{random}, then
\item
  the sample is \textbf{unbiased} and \textbf{representative} of the population of size \(N\), thus
\item
  any result based on the sample can \textbf{generalize} to the population, thus
\item
  the point estimate is a \textbf{``good guess''} of the unknown population parameter, thus
\item
  instead of performing a census, we can \textbf{infer} about the population using sampling.
\end{itemize}

\textbf{Specific to the bowl:}

\begin{itemize}
\tightlist
\item
  If we extract a sample of \(n = 50\) balls at \textbf{random}, in other words, we mix all of the equally sized balls before using the shovel, then
\item
  the contents of the shovel are an \textbf{unbiased representation} of the contents of the bowl's 2400 balls, thus
\item
  any result based on the shovel's balls can \textbf{generalize} to the bowl, thus
\item
  the sample proportion \(\widehat{p}\) of the \(n = 50\) balls in the shovel that are red is a \textbf{``good guess''} of the population proportion \(p\) of the \(N = 2400\) balls that are red, thus
\item
  instead of manually going over all 2400 balls in the bowl, we can \textbf{infer} about the bowl using the shovel.
\end{itemize}

\textbf{Specific to the Obama poll:}

\begin{itemize}
\tightlist
\item
  If we had a way of contacting a \textbf{randomly} chosen sample of 2089 young Americans and polling their approval of President Obama in 2013, then
\item
  these 2089 young Americans would be an \textbf{unbiased} and \textbf{representative} sample of \emph{all} young Americans in 2013, thus
\item
  any results based on this sample of 2089 young Americans can \textbf{generalize} to the entire population of \emph{all} young Americans in 2013, thus
\item
  the reported sample approval rating of 41\% of these 2089 young Americans is a \textbf{good guess} of the true approval rating among all young Americans in 2013, thus
\item
  instead of performing an expensive census of all young Americans in 2013, we can \textbf{infer} about all young Americans in 2013 using polling.
\end{itemize}

So as you can see, it was critical for the sample obtained by Kennedy School's Institute of Politics at Harvard University to be truly random in order to infer about \emph{all} young Americans' opinions about Obama. Was their sample truly random? It's hard to answer such questions without knowing about the \emph{sampling methodology} they used\index{sampling methodology}. For example, if this poll was conducted using only mobile phone numbers, people without mobile phones would be left out and therefore not represented in the sample. What about if Kennedy School's Institute of Politics at Harvard University conducted this poll on an internet news site? Then people who don't read this particular internet news site would be left out. Ensuring that our samples were random was easy to do in our sampling bowl exercises; however, in a real-life situation like the Obama poll, this is much harder to do.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

Comment on the representativeness of the following \emph{sampling methodologies}:

\textbf{(LC7.21)} The Royal Air Force wants to study how resistant all their airplanes are to bullets. They study the bullet holes on all the airplanes on the tarmac after an air battle against the Luftwaffe (German Air Force).

\textbf{(LC7.22)} Imagine it is 1993, a time when almost all households had landlines. You want to know the average number of people in each household in your city. You randomly pick out 500 phone numbers from the phone book and conduct a phone survey.

\textbf{(LC7.23)} You want to know the prevalence of illegal downloading of TV shows among students at a local college. You get the emails of 100 randomly chosen students and ask them, ``How many times did you download a pirated TV show last week?''.

\textbf{(LC7.24)} A local college administrator wants to know the average income of all graduates in the last 10 years. So they get the records of five randomly chosen graduates, contact them, and obtain their answers.

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{sampling-conclusion}{%
\section{Conclusion}\label{sampling-conclusion}}

\hypertarget{sampling-conclusion-table}{%
\subsection{Sampling scenarios}\label{sampling-conclusion-table}}

In this chapter, we performed both tactile and virtual sampling exercises to infer about an unknown proportion. We also presented a case study of sampling in real life with polls. In each case, we used the sample proportion \(\widehat{p}\) to estimate the population proportion \(p\). However, we are not just limited to scenarios related to proportions. In other words, we can use sampling to estimate other population parameters using other point estimates as well. We present four more such scenarios in Table \ref{tab:table-ch8}.

\begin{table}[!h]

\caption{\label{tab:table-ch8}\label{tab:summarytable-ch8}Scenarios of sampling for inference}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{>{\raggedleft\arraybackslash}p{0.5in}>{\raggedright\arraybackslash}p{1.2in}>{\raggedright\arraybackslash}p{0.8in}>{\raggedright\arraybackslash}p{1.5in}>{\raggedright\arraybackslash}p{0.6in}}
\toprule
Scenario & Population parameter & Notation & Point estimate & Symbol(s)\\
\midrule
1 & Population proportion & $p$ & Sample proportion & $\widehat{p}$\\
2 & Population mean & $\mu$ & Sample mean & $\overline{x}$ or $\widehat{\mu}$\\
3 & Difference in population proportions & $p_1 - p_2$ & Difference in sample proportions & $\widehat{p}_1 - \widehat{p}_2$\\
4 & Difference in population means & $\mu_1 - \mu_2$ & Difference in sample means & $\overline{x}_1 - \overline{x}_2$\\
5 & Population regression slope & $\beta_1$ & Fitted regression slope & $b_1$ or $\widehat{\beta}_1$\\
\bottomrule
\end{tabular}
\end{table}

In the rest of this book, we'll cover all the remaining scenarios as follows:

\begin{itemize}
\tightlist
\item
  In Chapter \ref{confidence-intervals}, we'll cover examples of statistical inference for

  \begin{itemize}
  \tightlist
  \item
    Scenario 2: The mean age \(\mu\) of all pennies in circulation in the US.
  \item
    Scenario 3: The difference \(p_1 - p_2\) in the proportion of people who yawn \emph{when seeing someone else yawn first} minus the proportion of people who yawn \emph{without seeing someone else yawn first}. This is an example of \emph{two-sample} inference\index{two-sample inference}.
  \end{itemize}
\item
  In Chapter \ref{hypothesis-testing}, we'll cover an example of statistical inference for

  \begin{itemize}
  \tightlist
  \item
    Scenario 4: The difference \(\mu_1 - \mu_2\) in mean IMDb ratings for action and romance movies. This is another example of \emph{two-sample} inference.
  \end{itemize}
\item
  In Chapter \ref{inference-for-regression}, we'll cover an example of statistical inference for regression by revisiting the regression models for teaching score as a function of various instructor demographic variables you saw in Chapters \ref{regression} and \ref{multiple-regression}.

  \begin{itemize}
  \tightlist
  \item
    Scenario 5: The slope \(\beta_1\) of the population regression line.
  \end{itemize}
\end{itemize}

\hypertarget{sampling-conclusion-central-limit-theorem}{%
\subsection{Central Limit Theorem}\label{sampling-conclusion-central-limit-theorem}}

What you visualized in Figures \ref{fig:comparing-sampling-distributions} and \ref{fig:comparing-sampling-distributions-2} and summarized in Tables \ref{tab:comparing-n} and \ref{tab:comparing-n-2} was a demonstration of a famous theorem, or mathematically proven truth, called the \index{Central Limit Theorem} \emph{Central Limit Theorem}. It loosely states that when sample means are based on larger and larger sample sizes, the sampling distribution of these sample means becomes both more and more normally shaped and more and more narrow.

In other words, their sampling distribution increasingly follows a \emph{normal distribution} and the variation of these sampling distributions gets smaller, as quantified by their standard errors.

Shuyi Chiou, Casey Dunn, and Pathikrit Bhattacharyya created a 3-minute and 38-second video at \url{https://youtu.be/jvoxEYmQHNM} explaining this crucial statistical theorem using the average weight of wild bunny rabbits and the average wingspan of dragons as examples. Figure \ref{fig:CLT-video-preview} shows a preview of this video.

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{images/copyright/CLT_video_preview} 

}

\caption{Preview of Central Limit Theorem video.}\label{fig:CLT-video-preview}
\end{figure}

\hypertarget{additional-resources-5}{%
\subsection{Additional resources}\label{additional-resources-5}}

Solutions to all \emph{Learning checks} can be found online in \href{https://moderndive.com/D-appendixD.html}{Appendix D}.

An R script file of all R code used in this chapter is available at \url{https://www.moderndive.com/scripts/07-sampling.R}.

\hypertarget{whats-to-come-6}{%
\subsection{What's to come?}\label{whats-to-come-6}}

Recall in our Obama poll case study in Section \ref{sampling-case-study} that based on this particular sample, the best guess by Kennedy School's Institute of Politics at Harvard University of the U.S. President Obama's approval rating among all young Americans was 41\%. However, this isn't the end of the story. If you read the article further, it states:

\begin{quote}
The online survey of 2,089 adults was conducted from Oct.~30 to Nov.~11, just weeks after the federal government shutdown ended and the problems surrounding the implementation of the Affordable Care Act began to take center stage. The poll's margin of error was plus or minus 2.1 percentage points.
\end{quote}

Note the term \emph{margin of error}, which here is ``plus or minus 2.1 percentage points.'' Most polls won't produce an estimate that's perfectly right; there will always be a certain amount of error caused by \emph{sampling variation}. The margin of error of plus or minus 2.1 percentage points is saying that a typical range of errors for polls of this type is about \(\pm\) 2.1\%, in words from about 2.1\% too small to about 2.1\% too big. We can restate this as the interval of \([41\% - 2.1\%, 41\% + 2.1\%] = [37.9\%, 43.1\%]\) (this notation indicates the interval contains all values between 37.9\% and 43.1\%, including the end points of 37.9\% and 43.1\%). We'll see in the next chapter that such intervals are known as \emph{confidence intervals}.

\hypertarget{confidence-intervals}{%
\chapter{Bootstrapping and Confidence Intervals}\label{confidence-intervals}}

In Chapter \ref{sampling}, we studied sampling. We started with a ``tactile'' exercise where we wanted to know the proportion of balls in the sampling bowl in Figure \ref{fig:sampling-exercise-1} that are red. While we could have performed an exhaustive count, this would have been a tedious process. So instead, we used a shovel to extract a sample of 50 balls and used the resulting proportion that were red as an \emph{estimate}. Furthermore, we made sure to mix the bowl's contents before every use of the shovel. Because of the randomness created by the mixing, different uses of the shovel yielded different proportions red and hence different estimates of the proportion of the bowl's balls that are red.

We then mimicked this ``tactile'' sampling exercise with an equivalent ``virtual'' sampling exercise performed on the computer. Using our computer's random number generator, we quickly mimicked the above sampling procedure a large number of times. In Subsection \ref{different-shovels}, we quickly repeated this sampling procedure 1000 times, using three different ``virtual'' shovels with 25, 50, and 100 slots. We visualized these three sets of 1000 estimates in Figure \ref{fig:comparing-sampling-distributions-3} and saw that as the sample size increased, the variation in the estimates decreased.

In doing so, what we did was construct \emph{sampling distributions}. The motivation for taking 1000 repeated samples and visualizing the resulting estimates was to study how these estimates varied from one sample to another; in other words, we wanted to study the effect of \emph{sampling variation}. We quantified the variation of these estimates using their standard deviation, which has a special name: the \emph{standard error}. In particular, we saw that as the sample size increased from 25 to 50 to 100, the standard error decreased and thus the sampling distributions narrowed. Larger sample sizes led to more \emph{precise} estimates that varied less around the center.

We then tied these sampling exercises to terminology and mathematical notation related to sampling in Subsection \ref{terminology-and-notation}. Our \emph{study population} was the large bowl with \(N\) = 2400 balls, while the \emph{population parameter}, the unknown quantity of interest, was the population proportion \(p\) of the bowl's balls that were red. Since performing a \emph{census} would be expensive in terms of time and energy, we instead extracted a \emph{sample} of size \(n\) = 50. The \emph{point estimate}, also known as a \emph{sample statistic}, used to estimate \(p\) was the sample proportion \(\widehat{p}\) of these 50 sampled balls that were red. Furthermore, since the sample was obtained at \emph{random}, it can be considered as \emph{unbiased} and \emph{representative} of the population. Thus any results based on the sample could be \emph{generalized} to the population. Therefore, the proportion of the shovel's balls that were red was a ``good guess'' of the proportion of the bowl's balls that are red. In other words, we used the sample to \emph{infer} about the population.

However, as described in Section \ref{sampling-simulation}, both the tactile and virtual sampling exercises are not what one would do in real life; this was merely an activity used to study the effects of sampling variation. In a real-life situation, we would not take 1000 samples of size \(n\), but rather take a \emph{single} representative sample that's as large as possible. Additionally, we knew that the true proportion of the bowl's balls that were red was 37.5\%. In a real-life situation, we will not know what this value is. Because if we did, then why would we take a sample to estimate it?

An example of a realistic sampling situation would be a poll, like the \href{https://www.npr.org/sections/itsallpolitics/2013/12/04/248793753/poll-support-for-obama-among-young-americans-eroding}{Obama poll} you saw in Section \ref{sampling-case-study}. Pollsters did not know the true proportion of \emph{all} young Americans who supported President Obama in 2013, and thus they took a single sample of size \(n\) = 2089 young Americans to estimate this value.

So how does one quantify the effects of sampling variation when you only have a \emph{single sample} to work with? You cannot directly study the effects of sampling variation when you only have one sample. One common method to study this is \emph{bootstrapping resampling}, which will be the focus of the earlier sections of this chapter.

Furthermore, what if we would like not only a single estimate of the unknown population parameter, but also a \emph{range of highly plausible} values? Going back to the Obama poll article, it stated that the pollsters' estimate of the proportion of all young Americans who supported President Obama was 41\%. But in addition it stated that the poll's ``margin of error was plus or minus 2.1 percentage points.'' This ``plausible range'' was {[}41\% - 2.1\%, 41\% + 2.1\%{]} = {[}38.9\%, 43.1\%{]}. This range of plausible values is what's known as a \emph{confidence interval}, which will be the focus of the later sections of this chapter.

\hypertarget{CI-packages}{%
\subsection*{Needed packages}\label{CI-packages}}


Let's load all the packages needed for this chapter (this assumes you've already installed them). Recall from our discussion in Section \ref{tidyverse-package} that loading the \texttt{tidyverse} package by running \texttt{library(tidyverse)} loads the following commonly used data science packages all at once:

\begin{itemize}
\tightlist
\item
  \texttt{ggplot2} for data visualization
\item
  \texttt{dplyr} for data wrangling
\item
  \texttt{tidyr} for converting data to tidy format
\item
  \texttt{readr} for importing spreadsheet data into R
\item
  As well as the more advanced \texttt{purrr}, \texttt{tibble}, \texttt{stringr}, and \texttt{forcats} packages
\end{itemize}

If needed, read Section \ref{packages} for information on how to install and load R packages.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(moderndive)}
\FunctionTok{library}\NormalTok{(infer)}
\end{Highlighting}
\end{Shaded}

\hypertarget{resampling-tactile}{%
\section{Pennies activity}\label{resampling-tactile}}

As we did in Chapter \ref{sampling}, we'll begin with a hands-on tactile activity.

\hypertarget{what-is-the-average-year-on-us-pennies-in-2019}{%
\subsection{What is the average year on US pennies in 2019?}\label{what-is-the-average-year-on-us-pennies-in-2019}}

Try to imagine all the pennies being used in the United States in 2019. That's a lot of pennies! Now say we're interested in the average year of minting of \emph{all} these pennies. One way to compute this value would be to gather up all pennies being used in the US, record the year, and compute the average. However, this would be near impossible! So instead, let's collect a \emph{sample} of 50 pennies from a local bank in downtown Northampton, Massachusetts, USA as seen in Figure \ref{fig:resampling-exercise-a}.

\begin{figure}

{\centering \includegraphics[width=0.4\linewidth]{images/sampling/pennies/bank} \includegraphics[width=0.4\linewidth]{images/sampling/pennies/roll} 

}

\caption{Collecting a sample of 50 US pennies from a local bank.}\label{fig:resampling-exercise-a}
\end{figure}

An image of these 50 pennies can be seen in Figure \ref{fig:resampling-exercise-c}. For each of the 50 pennies starting in the top left, progressing row-by-row, and ending in the bottom right, we assigned an ``ID'' identification variable and marked the year of minting.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/sampling/pennies/deliverable/3} 

}

\caption{50 US pennies labelled.}\label{fig:resampling-exercise-c}
\end{figure}

The \texttt{moderndive} \index{moderndive!pennies\_sample} package contains this data on our 50 sampled pennies in the \texttt{pennies\_sample} data frame:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pennies\_sample}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 50 x 2
      ID  year
   <int> <dbl>
 1     1  2002
 2     2  1986
 3     3  2017
 4     4  1988
 5     5  2008
 6     6  1983
 7     7  2008
 8     8  1996
 9     9  2004
10    10  2000
# ... with 40 more rows
\end{verbatim}

The \texttt{pennies\_sample} data frame has 50 rows corresponding to each penny with two variables. The first variable \texttt{ID} corresponds to the ID labels in Figure \ref{fig:resampling-exercise-c}, whereas the second variable \texttt{year} corresponds to the year of minting saved as a numeric variable, also known as a double (\texttt{dbl}).

Based on these 50 sampled pennies, what can we say about \emph{all} US pennies in 2019? Let's study some properties of our sample by performing an exploratory data analysis. Let's first visualize the distribution of the year of these 50 pennies using our data visualization tools from Chapter \ref{viz}. Since \texttt{year} is a numerical variable, we use a histogram in Figure \ref{fig:pennies-sample-histogram} to visualize its distribution.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(pennies\_sample, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ year)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{10}\NormalTok{, }\AttributeTok{color =} \StringTok{"white"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/pennies-sample-histogram-1} 

}

\caption{Distribution of year on 50 US pennies.}\label{fig:pennies-sample-histogram}
\end{figure}

Observe a slightly left-skewed \index{skew} distribution, since most pennies fall between 1980 and 2010 with only a few pennies older than 1970. What is the average year for the 50 sampled pennies? Eyeballing the histogram it appears to be around 1990. Let's now compute this value exactly using our data wrangling tools from Chapter \ref{wrangling}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x\_bar }\OtherTok{\textless{}{-}}\NormalTok{ pennies\_sample }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{mean\_year =} \FunctionTok{mean}\NormalTok{(year))}
\NormalTok{x\_bar}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 1
  mean_year
      <dbl>
1   1995.44
\end{verbatim}

Thus, if we're willing to assume that \texttt{pennies\_sample} is a representative sample from \emph{all} US pennies, a ``good guess'' of the average year of minting of all US pennies would be 1995.44. In other words, around 1995. This should all start sounding similar to what we did previously in Chapter \ref{sampling}!

In Chapter \ref{sampling}, our \emph{study population} was the bowl of \(N\) = 2400 balls. Our \emph{population parameter} was the \emph{population proportion} of these balls that were red, denoted by \(p\). In order to estimate \(p\), we extracted a sample of 50 balls using the shovel. We then computed the relevant \emph{point estimate}: the \emph{sample proportion} of these 50 balls that were red, denoted mathematically by \(\widehat{p}\).

Here our population is \(N\) = whatever the number of pennies are being used in the US, a value which we don't know and probably never will. The population parameter of interest is now the \emph{population mean} year of all these pennies, a value denoted mathematically by the Greek letter \(\mu\) (pronounced ``mu''). In order to estimate \(\mu\), we went to the bank and obtained a sample of 50 pennies and computed the relevant point estimate: the \emph{sample mean} year of these 50 pennies, denoted mathematically by \(\overline{x}\) (pronounced ``x-bar''). An alternative and more intuitive notation for the sample mean is \(\widehat{\mu}\). However, this is unfortunately not as commonly used, so in this book we'll stick with convention and always denote the sample mean as \(\overline{x}\).

We summarize the correspondence between the sampling bowl exercise in Chapter \ref{sampling} and our pennies exercise in Table \ref{tab:table-ch8-b}, which are the first two rows of the previously seen Table \ref{tab:table-ch8}.

\begin{table}[!h]

\caption{\label{tab:table-ch8-b}Scenarios of sampling for inference}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{>{\raggedleft\arraybackslash}p{0.5in}>{\raggedright\arraybackslash}p{0.7in}>{\raggedright\arraybackslash}p{1in}>{\raggedright\arraybackslash}p{1.1in}>{\raggedright\arraybackslash}p{1in}}
\toprule
Scenario & Population parameter & Notation & Point estimate & Symbol(s)\\
\midrule
1 & Population proportion & $p$ & Sample proportion & $\widehat{p}$\\
2 & Population mean & $\mu$ & Sample mean & $\overline{x}$ or $\widehat{\mu}$\\
\bottomrule
\end{tabular}
\end{table}

Going back to our 50 sampled pennies in Figure \ref{fig:resampling-exercise-c}, the point estimate of interest is the sample mean \(\overline{x}\) of 1995.44. This quantity is an \emph{estimate} of the population mean year of \emph{all} US pennies \(\mu\).

Recall that we also saw in Chapter \ref{sampling} that such estimates are prone to \emph{sampling variation}. For example, in this particular sample in Figure \ref{fig:resampling-exercise-c}, we observed three pennies with the year 1999. If we sampled another 50 pennies, would we observe exactly three pennies with the year 1999 again? More than likely not. We might observe none, one, two, or maybe even all 50! The same can be said for the other 26 unique years that are represented in our sample of 50 pennies.

To study the effects of \emph{sampling variation} in Chapter \ref{sampling}, we took many samples, something we could easily do with our shovel. In our case with pennies, however, how would we obtain another sample? By going to the bank and getting another roll of 50 pennies.

Say we're feeling lazy, however, and don't want to go back to the bank. How can we study the effects of sampling variation using our \emph{single sample}? We will do so using a technique known as \emph{bootstrap resampling with replacement}, which we now illustrate.

\hypertarget{resampling-once}{%
\subsection{Resampling once}\label{resampling-once}}

\textbf{Step 1}: Let's print out identically sized slips of paper representing our 50 pennies as seen in Figure \ref{fig:tactile-resampling-1}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/sampling/pennies/tactile_simulation/1_paper_slips} 

}

\caption{Step 1: 50 slips of paper representing 50 US pennies.}\label{fig:tactile-resampling-1}
\end{figure}

\textbf{Step 2}: Put the 50 slips of paper into a hat or tuque as seen in Figure \ref{fig:tactile-resampling-2}.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/sampling/pennies/tactile_simulation/2_insert_in_hat} 

}

\caption{Step 2: Putting 50 slips of paper in a hat.}\label{fig:tactile-resampling-2}
\end{figure}

\textbf{Step 3}: Mix the hat's contents and draw one slip of paper at random as seen in Figure \ref{fig:tactile-resampling-3}. Record the year.

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{images/sampling/pennies/tactile_simulation/3_draw_at_random} 

}

\caption{Step 3: Drawing one slip of paper at random.}\label{fig:tactile-resampling-3}
\end{figure}

\textbf{Step 4}: Put the slip of paper back in the hat! In other words, replace it as seen in Figure \ref{fig:tactile-resampling-4}.

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth]{images/sampling/pennies/tactile_simulation/4_put_it_back} 

}

\caption{Step 4: Replacing slip of paper.}\label{fig:tactile-resampling-4}
\end{figure}

\textbf{Step 5}: Repeat Steps 3 and 4 a total of 49 more times, resulting in 50 recorded years.

What we just performed was a \emph{resampling} \index{resampling} of the original sample of 50 pennies. We are not sampling 50 pennies from the population of all US pennies as we did in our trip to the bank. Instead, we are mimicking this act by resampling 50 pennies from our original sample of 50 pennies.

Now ask yourselves, why did we replace our resampled slip of paper back into the hat in Step 4? Because if we left the slip of paper out of the hat each time we performed Step 4, we would end up with the same 50 original pennies! In other words, replacing the slips of paper induces \emph{sampling variation}.

Being more precise with our terminology, we just performed a \emph{resampling with replacement} from the original sample of 50 pennies. Had we left the slip of paper out of the hat each time we performed Step 4, this would be \emph{resampling without replacement}.

Let's study our 50 resampled pennies via an exploratory data analysis. First, let's load the data into R by manually creating a data frame \texttt{pennies\_resample} of our 50 resampled values. We'll do this using the \texttt{tibble()} command from the \texttt{dplyr} package. Note that the 50 values you resample will almost certainly not be the same as ours given the inherent randomness.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pennies\_resample }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{year =} \FunctionTok{c}\NormalTok{(}\DecValTok{1976}\NormalTok{, }\DecValTok{1962}\NormalTok{, }\DecValTok{1976}\NormalTok{, }\DecValTok{1983}\NormalTok{, }\DecValTok{2017}\NormalTok{, }\DecValTok{2015}\NormalTok{, }\DecValTok{2015}\NormalTok{, }\DecValTok{1962}\NormalTok{, }\DecValTok{2016}\NormalTok{, }\DecValTok{1976}\NormalTok{, }
           \DecValTok{2006}\NormalTok{, }\DecValTok{1997}\NormalTok{, }\DecValTok{1988}\NormalTok{, }\DecValTok{2015}\NormalTok{, }\DecValTok{2015}\NormalTok{, }\DecValTok{1988}\NormalTok{, }\DecValTok{2016}\NormalTok{, }\DecValTok{1978}\NormalTok{, }\DecValTok{1979}\NormalTok{, }\DecValTok{1997}\NormalTok{, }
           \DecValTok{1974}\NormalTok{, }\DecValTok{2013}\NormalTok{, }\DecValTok{1978}\NormalTok{, }\DecValTok{2015}\NormalTok{, }\DecValTok{2008}\NormalTok{, }\DecValTok{1982}\NormalTok{, }\DecValTok{1986}\NormalTok{, }\DecValTok{1979}\NormalTok{, }\DecValTok{1981}\NormalTok{, }\DecValTok{2004}\NormalTok{, }
           \DecValTok{2000}\NormalTok{, }\DecValTok{1995}\NormalTok{, }\DecValTok{1999}\NormalTok{, }\DecValTok{2006}\NormalTok{, }\DecValTok{1979}\NormalTok{, }\DecValTok{2015}\NormalTok{, }\DecValTok{1979}\NormalTok{, }\DecValTok{1998}\NormalTok{, }\DecValTok{1981}\NormalTok{, }\DecValTok{2015}\NormalTok{, }
           \DecValTok{2000}\NormalTok{, }\DecValTok{1999}\NormalTok{, }\DecValTok{1988}\NormalTok{, }\DecValTok{2017}\NormalTok{, }\DecValTok{1992}\NormalTok{, }\DecValTok{1997}\NormalTok{, }\DecValTok{1990}\NormalTok{, }\DecValTok{1988}\NormalTok{, }\DecValTok{2006}\NormalTok{, }\DecValTok{2000}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The 50 values of \texttt{year} in \texttt{pennies\_resample} represent a resample of size 50 from the original sample of 50 pennies. We display the 50 resampled pennies in Figure \ref{fig:resampling-exercise-d}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/sampling/pennies/deliverable/4} 

}

\caption{50 resampled US pennies labelled.}\label{fig:resampling-exercise-d}
\end{figure}

Let's compare the distribution of the numerical variable \texttt{year} of our 50 resampled pennies with the distribution of the numerical variable \texttt{year} of our original sample of 50 pennies in Figure \ref{fig:origandresample}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(pennies\_resample, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ year)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{10}\NormalTok{, }\AttributeTok{color =} \StringTok{"white"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Resample of 50 pennies"}\NormalTok{)}
\FunctionTok{ggplot}\NormalTok{(pennies\_sample, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ year)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{10}\NormalTok{, }\AttributeTok{color =} \StringTok{"white"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Original sample of 50 pennies"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}



\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/origandresample-1} 

}

\caption{Comparing \texttt{year} in the resampled \texttt{pennies\_resample} with the original sample \texttt{pennies\_sample}.}\label{fig:origandresample}
\end{figure}

Observe in Figure \ref{fig:origandresample} that while the general shapes of both distributions of \texttt{year} are roughly similar, they are not identical.

Recall from the previous section that the sample mean of the original sample of 50 pennies from the bank was 1995.44. What about for our resample? Any guesses? Let's have \texttt{dplyr} help us out as before:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pennies\_resample }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{mean\_year =} \FunctionTok{mean}\NormalTok{(year))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 1
  mean_year
      <dbl>
1      1996
\end{verbatim}

We obtained a different mean year of 1996. This variation is induced by the resampling \emph{with replacement} we performed earlier.

What if we repeated this resampling exercise many times? Would we obtain the same mean \texttt{year} each time? In other words, would our guess at the mean year of all pennies in the US in 2019 be exactly 1996 every time? Just as we did in Chapter \ref{sampling}, let's perform this resampling activity with the help of some of our friends: 35 friends in total.

\hypertarget{student-resamples}{%
\subsection{Resampling 35 times}\label{student-resamples}}

Each of our 35 friends will repeat the same five steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Start with 50 identically sized slips of paper representing the 50 pennies.
\item
  Put the 50 small pieces of paper into a hat or beanie cap.
\item
  Mix the hat's contents and draw one slip of paper at random. Record the year in a spreadsheet.
\item
  Replace the slip of paper back in the hat!
\item
  Repeat Steps 3 and 4 a total of 49 more times, resulting in 50 recorded years.
\end{enumerate}

Since we had 35 of our friends perform this task, we ended up with \(35 \cdot 50 = 1750\) values. We recorded these values in a \href{https://docs.google.com/spreadsheets/d/1y3kOsU_wDrDd5eiJbEtLeHT9L5SvpZb_TrzwFBsouk0/}{shared spreadsheet} with 50 rows (plus a header row) and 35 columns. We display a snapshot of the first 10 rows and five columns of this shared spreadsheet in Figure \ref{fig:tactile-resampling-5}.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/sampling/pennies/tactile_simulation/5_shared_spreadsheet} 

}

\caption{Snapshot of shared spreadsheet of resampled pennies.}\label{fig:tactile-resampling-5}
\end{figure}

For your convenience, we've taken these 35 \(\cdot\) 50 = 1750 values and saved them in \texttt{pennies\_resamples}, a ``tidy'' data frame included in the \texttt{moderndive} package. We saw what it means for a data frame to be ``tidy'' in Subsection \ref{tidy-definition}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pennies\_resamples}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,750 x 3
# Groups:   name [35]
   replicate name     year
       <int> <chr>   <dbl>
 1         1 Arianna  1988
 2         1 Arianna  2002
 3         1 Arianna  2015
 4         1 Arianna  1998
 5         1 Arianna  1979
 6         1 Arianna  1971
 7         1 Arianna  1971
 8         1 Arianna  2015
 9         1 Arianna  1988
10         1 Arianna  1979
# ... with 1,740 more rows
\end{verbatim}

What did each of our 35 friends obtain as the mean year? Once again, \texttt{dplyr} to the rescue! After grouping the rows by \texttt{name}, we summarize each group of 50 rows by their mean \texttt{year}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{resampled\_means }\OtherTok{\textless{}{-}}\NormalTok{ pennies\_resamples }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(name) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{mean\_year =} \FunctionTok{mean}\NormalTok{(year))}
\NormalTok{resampled\_means}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 35 x 2
   name      mean_year
   <chr>         <dbl>
 1 Arianna     1992.5 
 2 Artemis     1996.42
 3 Bea         1996.32
 4 Camryn      1996.9 
 5 Cassandra   1991.22
 6 Cindy       1995.48
 7 Claire      1995.52
 8 Dahlia      1998.48
 9 Dan         1993.86
10 Eindra      1993.56
# ... with 25 more rows
\end{verbatim}

Observe that \texttt{resampled\_means} has 35 rows corresponding to the 35 means based on the 35 resamples. Furthermore, observe the variation in the 35 values in the variable \texttt{mean\_year}. Let's visualize this variation using a histogram in Figure \ref{fig:tactile-resampling-6}. Recall that adding the argument \texttt{boundary\ =\ 1990} to the \texttt{geom\_histogram()} sets the binning structure so that one of the bin boundaries is at 1990 exactly.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(resampled\_means, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mean\_year)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{1}\NormalTok{, }\AttributeTok{color =} \StringTok{"white"}\NormalTok{, }\AttributeTok{boundary =} \DecValTok{1990}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Sampled mean year"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/tactile-resampling-6-1} 

}

\caption{Distribution of 35 sample means from 35 resamples.}\label{fig:tactile-resampling-6}
\end{figure}

Observe in Figure \ref{fig:tactile-resampling-6} that the distribution looks roughly normal and that we rarely observe sample mean years less than 1992 or greater than 2000. Also observe how the distribution is roughly centered at 1995, which is close to the sample mean of 1995.44 of the \emph{original sample} of 50 pennies from the bank.

\hypertarget{what-did-we-just-do-1}{%
\subsection{What did we just do?}\label{what-did-we-just-do-1}}

What we just demonstrated in this activity is the statistical procedure known as \index{bootstrap} \emph{bootstrap resampling with replacement}. We used \emph{resampling} to mimic the sampling variation we studied in Chapter \ref{sampling} on sampling. However, in this case, we did so using only a \emph{single} sample from the population.

In fact, the histogram of sample means from 35 resamples in Figure \ref{fig:tactile-resampling-6} is called the \index{bootstrap!distribution} \emph{bootstrap distribution}. It is an \emph{approximation} to the \emph{sampling distribution} of the sample mean, in the sense that both distributions will have a similar shape and similar spread. In fact in the upcoming Section \ref{ci-conclusion}, we'll show you that this is the case. Using this bootstrap distribution, we can study the effect of sampling variation on our estimates. In particular, we'll study the typical ``error'' of our estimates, known as the \index{standard error} \emph{standard error}.

In Section \ref{resampling-simulation} we'll mimic our tactile resampling activity virtually on the computer, allowing us to quickly perform the resampling many more than 35 times. In Section \ref{ci-build-up} we'll define the statistical concept of a \emph{confidence interval}, which builds off the concept of bootstrap distributions.

In Section \ref{bootstrap-process}, we'll construct confidence intervals using the \texttt{dplyr} package, as well as a new package: the \texttt{infer} package for ``tidy'' and transparent statistical inference. We'll introduce the ``tidy'' statistical inference framework that was the motivation for the \texttt{infer} package pipeline. The \texttt{infer} package will be the driving package throughout the rest of this book.

As we did in Chapter \ref{sampling}, we'll tie all these ideas together with a real-life case study in Section \ref{case-study-two-prop-ci}. This time we'll look at data from an experiment about yawning from the US television show \emph{Mythbusters}.

\hypertarget{resampling-simulation}{%
\section{Computer simulation of resampling}\label{resampling-simulation}}

Let's now mimic our tactile resampling activity virtually with a computer.

\hypertarget{virtually-resampling-once}{%
\subsection{Virtually resampling once}\label{virtually-resampling-once}}

First, let's perform the virtual analog of resampling once. Recall that the \texttt{pennies\_sample} data frame included in the \texttt{moderndive} package contains the years of our original sample of 50 pennies from the bank. Furthermore, recall in Chapter \ref{sampling} on sampling that we used the \texttt{rep\_sample\_n()} function as a virtual shovel to sample balls from our virtual bowl of 2400 balls as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{virtual\_shovel }\OtherTok{\textless{}{-}}\NormalTok{ bowl }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rep\_sample\_n}\NormalTok{(}\AttributeTok{size =} \DecValTok{50}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let's modify this code to perform the resampling with replacement of the 50 slips of paper representing our original sample 50 pennies:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{virtual\_resample }\OtherTok{\textless{}{-}}\NormalTok{ pennies\_sample }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rep\_sample\_n}\NormalTok{(}\AttributeTok{size =} \DecValTok{50}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Observe how we explicitly set the \texttt{replace} argument to \texttt{TRUE} in order to tell \texttt{rep\_sample\_n()} that we would like to sample pennies \index{sampling!with replacement} \emph{with} replacement. Had we not set \texttt{replace\ =\ TRUE}, the function would've assumed the default value of \texttt{FALSE} and hence done resampling \emph{without} replacement. Additionally, since we didn't specify the number of replicates via the \texttt{reps} argument, the function assumes the default of one replicate \texttt{reps\ =\ 1}. Lastly, observe also that the \texttt{size} argument is set to match the original sample size of 50 pennies.

Let's look at only the first 10 out of 50 rows of \texttt{virtual\_resample}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{virtual\_resample}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 50 x 3
# Groups:   replicate [1]
   replicate    ID  year
       <int> <int> <dbl>
 1         1    37  1962
 2         1     1  2002
 3         1    45  1997
 4         1    28  2006
 5         1    50  2017
 6         1    10  2000
 7         1    16  2015
 8         1    47  1982
 9         1    23  1998
10         1    44  2015
# ... with 40 more rows
\end{verbatim}

The \texttt{replicate} variable only takes on the value of 1 corresponding to us only having \texttt{reps\ =\ 1}, the \texttt{ID} variable indicates which of the 50 pennies from \texttt{pennies\_sample} was resampled, and \texttt{year} denotes the year of minting. Let's now compute the mean \texttt{year} in our virtual resample of size 50 using data wrangling functions included in the \texttt{dplyr} package:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{virtual\_resample }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{resample\_mean =} \FunctionTok{mean}\NormalTok{(year))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 2
  replicate resample_mean
      <int>         <dbl>
1         1          1996
\end{verbatim}

As we saw when we did our tactile resampling exercise, the resulting mean year is different than the mean year of our 50 originally sampled pennies of 1995.44.

\hypertarget{bootstrap-35-replicates}{%
\subsection{Virtually resampling 35 times}\label{bootstrap-35-replicates}}

Let's now perform the virtual analog of our 35 friends' resampling. Using these results, we'll be able to study the variability in the sample means from 35 resamples of size 50. Let's first add a \texttt{reps\ =\ 35} argument to \texttt{rep\_sample\_n()} \index{infer!rep\_sample\_n()} to indicate we would like 35 replicates. Thus, we want to repeat the resampling with the replacement of 50 pennies 35 times.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{virtual\_resamples }\OtherTok{\textless{}{-}}\NormalTok{ pennies\_sample }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rep\_sample\_n}\NormalTok{(}\AttributeTok{size =} \DecValTok{50}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{reps =} \DecValTok{35}\NormalTok{)}
\NormalTok{virtual\_resamples}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,750 x 3
# Groups:   replicate [35]
   replicate    ID  year
       <int> <int> <dbl>
 1         1    21  1981
 2         1    34  1985
 3         1     4  1988
 4         1    11  1994
 5         1    26  1979
 6         1     8  1996
 7         1    19  1983
 8         1    21  1981
 9         1    49  2006
10         1     2  1986
# ... with 1,740 more rows
\end{verbatim}

The resulting \texttt{virtual\_resamples} data frame has 35 \(\cdot\) 50 = 1750 rows corresponding to 35 resamples of 50 pennies. Let's now compute the resulting 35 sample means using the same \texttt{dplyr} code as we did in the previous section, but this time adding a \texttt{group\_by(replicate)}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{virtual\_resampled\_means }\OtherTok{\textless{}{-}}\NormalTok{ virtual\_resamples }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(replicate) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{mean\_year =} \FunctionTok{mean}\NormalTok{(year))}
\NormalTok{virtual\_resampled\_means}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 35 x 2
   replicate mean_year
       <int>     <dbl>
 1         1   1995.58
 2         2   1999.74
 3         3   1993.7 
 4         4   1997.1 
 5         5   1999.42
 6         6   1995.12
 7         7   1994.94
 8         8   1997.78
 9         9   1991.26
10        10   1996.88
# ... with 25 more rows
\end{verbatim}

Observe that \texttt{virtual\_resampled\_means} has 35 rows, corresponding to the 35 resampled means. Furthermore, observe that the values of \texttt{mean\_year} vary. Let's visualize this variation using a histogram in Figure \ref{fig:tactile-resampling-7}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(virtual\_resampled\_means, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mean\_year)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{1}\NormalTok{, }\AttributeTok{color =} \StringTok{"white"}\NormalTok{, }\AttributeTok{boundary =} \DecValTok{1990}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Resample mean year"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/tactile-resampling-7-1} 

}

\caption{Distribution of 35 sample means from 35 resamples.}\label{fig:tactile-resampling-7}
\end{figure}

Let's compare our virtually constructed bootstrap distribution with the one our 35 friends constructed via our tactile resampling exercise in Figure \ref{fig:orig-and-resample-means}. Observe how they are somewhat similar, but not identical.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/orig-and-resample-means-1} 

}

\caption{Comparing distributions of means from resamples.}\label{fig:orig-and-resample-means}
\end{figure}

Recall that in the ``resampling with replacement'' scenario we are illustrating here, both of these histograms have a special name: the \emph{bootstrap distribution of the sample mean}. Furthermore, recall they are an approximation to the \emph{sampling distribution} of the sample mean, a concept you saw in Chapter \ref{sampling} on sampling. These distributions allow us to study the effect of sampling variation on our estimates of the true population mean, in this case the true mean year for \emph{all} US pennies. However, unlike in Chapter \ref{sampling} where we took multiple samples (something one would never do in practice), bootstrap distributions are constructed by taking multiple resamples from a \emph{single} sample: in this case, the 50 original pennies from the bank.

\hypertarget{bootstrap-1000-replicates}{%
\subsection{Virtually resampling 1000 times}\label{bootstrap-1000-replicates}}

Remember that one of the goals of resampling with replacement is to construct the bootstrap distribution, which is an approximation of the sampling distribution. However, the bootstrap distribution in Figure \ref{fig:tactile-resampling-7} is based only on 35 resamples and hence looks a little coarse. Let's increase the number of resamples to 1000, so that we can hopefully better see the shape and the variability between different resamples.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Repeat resampling 1000 times}
\NormalTok{virtual\_resamples }\OtherTok{\textless{}{-}}\NormalTok{ pennies\_sample }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rep\_sample\_n}\NormalTok{(}\AttributeTok{size =} \DecValTok{50}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{reps =} \DecValTok{1000}\NormalTok{)}

\CommentTok{\# Compute 1000 sample means}
\NormalTok{virtual\_resampled\_means }\OtherTok{\textless{}{-}}\NormalTok{ virtual\_resamples }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(replicate) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{mean\_year =} \FunctionTok{mean}\NormalTok{(year))}
\end{Highlighting}
\end{Shaded}

However, in the interest of brevity, going forward let's combine these two operations into a single chain of pipe (\texttt{\%\textgreater{}\%}) operators:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{virtual\_resampled\_means }\OtherTok{\textless{}{-}}\NormalTok{ pennies\_sample }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rep\_sample\_n}\NormalTok{(}\AttributeTok{size =} \DecValTok{50}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{reps =} \DecValTok{1000}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(replicate) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{mean\_year =} \FunctionTok{mean}\NormalTok{(year))}
\NormalTok{virtual\_resampled\_means}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,000 x 2
   replicate mean_year
       <int>     <dbl>
 1         1   1992.6 
 2         2   1994.78
 3         3   1994.74
 4         4   1997.88
 5         5   1990   
 6         6   1999.48
 7         7   1990.26
 8         8   1993.2 
 9         9   1994.88
10        10   1996.3 
# ... with 990 more rows
\end{verbatim}

In Figure \ref{fig:one-thousand-sample-means} let's visualize the bootstrap distribution of these 1000 means based on 1000 virtual resamples:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(virtual\_resampled\_means, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mean\_year)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{1}\NormalTok{, }\AttributeTok{color =} \StringTok{"white"}\NormalTok{, }\AttributeTok{boundary =} \DecValTok{1990}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"sample mean"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/one-thousand-sample-means-1} 

}

\caption{Bootstrap resampling distribution based on 1000 resamples.}\label{fig:one-thousand-sample-means}
\end{figure}

Note here that the bell shape is starting to become much more apparent. We now have a general sense for the range of values that the sample mean may take on. But where is this histogram centered? Let's compute the mean of the 1000 resample means:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{virtual\_resampled\_means }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{mean\_of\_means =} \FunctionTok{mean}\NormalTok{(mean\_year))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 1
  mean_of_means
          <dbl>
1       1995.36
\end{verbatim}

The mean of these 1000 means is 1995.36, which is quite close to the mean of our original sample of 50 pennies of 1995.44. This is the case since each of the 1000 resamples is based on the original sample of 50 pennies.

Congratulations! You've just constructed your first bootstrap distribution! In the next section, you'll see how to use this bootstrap distribution to construct \emph{confidence intervals}.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC8.1)} What is the chief difference between a bootstrap distribution and a sampling distribution?

\textbf{(LC8.2)} Looking at the bootstrap distribution for the sample mean in Figure \ref{fig:one-thousand-sample-means}, between what two values would you say \emph{most} values lie?

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{ci-build-up}{%
\section{Understanding confidence intervals}\label{ci-build-up}}

Let's start this section with an analogy involving fishing. Say you are trying to catch a fish. On the one hand, you could use a spear, while on the other you could use a net. Using the net will probably allow you to catch more fish!

Now think back to our pennies exercise where you are trying to estimate the true population mean year \(\mu\) of \emph{all} US pennies. \index{confidence interval!analogy to fishing} Think of the value of \(\mu\) as a fish.

On the one hand, we could use the appropriate \emph{point estimate/sample statistic} to estimate \(\mu\), which we saw in Table \ref{tab:table-ch8-b} is the sample mean \(\overline{x}\). Based on our sample of 50 pennies from the bank, the sample mean was 1995.44. Think of using this value as ``fishing with a spear.''

What would ``fishing with a net'' correspond to? Look at the bootstrap distribution in Figure \ref{fig:one-thousand-sample-means} once more. Between which two years would you say that ``most'' sample means lie? While this question is somewhat subjective, saying that most sample means lie between 1992 and 2000 would not be unreasonable. Think of this interval as the ``net.''

What we've just illustrated is the concept of a \emph{confidence interval}, which we'll abbreviate with ``CI'' throughout this book. As opposed to a point estimate/sample statistic that estimates the value of an unknown population parameter with a single value, a \emph{confidence interval} \index{confidence interval} gives what can be interpreted as a range of plausible values. Going back to our analogy, point estimates/sample statistics can be thought of as spears, whereas confidence intervals can be thought of as nets.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/shutterstock/point_estimate_vs_conf_int} 

}

\caption{Analogy of difference between point estimates and confidence intervals.}\label{fig:point-estimate-vs-conf-int}
\end{figure}

Our proposed interval of 1992 to 2000 was constructed by eye and was thus somewhat subjective. We now introduce two methods for constructing such intervals in a more exact fashion: the \emph{percentile method} and the \emph{standard error method}.

Both methods for confidence interval construction share some commonalities. First, they are both constructed from a bootstrap distribution, as you constructed in Subsection \ref{bootstrap-1000-replicates} and visualized in Figure \ref{fig:one-thousand-sample-means}.

Second, they both require you to specify the \index{confidence interval!confidence level} \emph{confidence level}. Commonly used confidence levels include 90\%, 95\%, and 99\%. All other things being equal, higher confidence levels correspond to wider confidence intervals, and lower confidence levels correspond to narrower confidence intervals. In this book, we'll be mostly using 95\% and hence constructing ``95\% confidence intervals for \(\mu\)'' for our pennies activity.

\hypertarget{percentile-method}{%
\subsection{Percentile method}\label{percentile-method}}

One method to construct a confidence interval is to use the middle 95\% of values of the bootstrap distribution. We can do this by computing the 2.5th and 97.5th percentiles, which are 1991.059 and 1999.283, respectively. This is known as the \emph{percentile method} for constructing confidence intervals.

For now, let's focus only on the concepts behind a percentile method constructed confidence interval; we'll show you the code that computes these values in the next section.

Let's mark these percentiles on the bootstrap distribution with vertical lines in Figure \ref{fig:percentile-method}. About 95\% of the \texttt{mean\_year} variable values in \texttt{virtual\_resampled\_means} fall between 1991.059 and 1999.283, with 2.5\% to the left of the leftmost line and 2.5\% to the right of the rightmost line.



\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/percentile-method-1} 

}

\caption{Percentile method 95\% confidence interval. Interval endpoints marked by vertical lines.}\label{fig:percentile-method}
\end{figure}

\hypertarget{se-method}{%
\subsection{Standard error method}\label{se-method}}

Recall in Appendix \ref{appendix-normal-curve}, we saw that if a numerical variable follows a normal distribution, or, in other words, the histogram of this variable is bell-shaped, then roughly 95\% of values fall between \(\pm\) 1.96 standard deviations of the mean. Given that our bootstrap distribution based on 1000 resamples with replacement in Figure \ref{fig:one-thousand-sample-means} is normally shaped, let's use this fact about normal distributions to construct a confidence interval in a different way.

First, recall the bootstrap distribution has a mean equal to 1995.36. This value almost coincides exactly with the value of the sample mean \(\overline{x}\) of our original 50 pennies of 1995.44. Second, let's compute the standard deviation of the bootstrap distribution using the values of \texttt{mean\_year} in the \texttt{virtual\_resampled\_means} data frame:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{virtual\_resampled\_means }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{SE =} \FunctionTok{sd}\NormalTok{(mean\_year))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 1
       SE
    <dbl>
1 2.15466
\end{verbatim}

What is this value? Recall that the bootstrap distribution is an approximation to the sampling distribution. Recall also that the standard deviation of a sampling distribution has a special name: the \emph{standard error}. Putting these two facts together, we can say that 2.155 is an approximation of the standard error of \(\overline{x}\).

Thus, using our 95\% rule of thumb about normal distributions from Appendix \ref{appendix-normal-curve}, we can use the following formula to determine the lower and upper endpoints of a 95\% confidence interval for \(\mu\):

\[
\begin{aligned}
\overline{x} \pm 1.96 \cdot SE &= (\overline{x} - 1.96 \cdot SE, \overline{x} + 1.96 \cdot SE)\\
&= (1995.44 - 1.96 \cdot 2.15, 1995.44 + 1.96 \cdot 2.15)\\
&= (1991.15, 1999.73)
\end{aligned}
\]

Let's now add the SE method confidence interval with dashed lines in Figure \ref{fig:percentile-and-se-method}.



\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/percentile-and-se-method-1} 

}

\caption{Comparing two 95\% confidence interval methods.}\label{fig:percentile-and-se-method}
\end{figure}

We see that both methods produce nearly identical 95\% confidence intervals for \(\mu\) with the percentile method yielding \((1991.059, 1999.283)\) while the standard error method produces \((1991.217, 1999.663)\). However, recall that we can only use the standard error rule when the bootstrap distribution is roughly normally shaped.

Now that we've introduced the concept of confidence intervals and laid out the intuition behind two methods for constructing them, let's explore the code that allows us to construct them.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC8.3)} What condition about the bootstrap distribution must be met for us to be able to construct confidence intervals using the standard error method?

\textbf{(LC8.4)} Say we wanted to construct a 68\% confidence interval instead of a 95\% confidence interval for \(\mu\). Describe what changes are needed to make this happen. Hint: we suggest you look at Appendix \ref{appendix-normal-curve} on the normal distribution.

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{bootstrap-process}{%
\section{Constructing confidence intervals}\label{bootstrap-process}}

Recall that the process of resampling with replacement we performed by hand in Section \ref{resampling-tactile} and virtually in Section \ref{resampling-simulation} is known as \index{bootstrap!colloquial definition} \emph{bootstrapping}. The term bootstrapping originates in the expression of ``pulling oneself up by their bootstraps,'' meaning to \href{https://en.wiktionary.org/wiki/pull_oneself_up_by_one\%27s_bootstraps}{``succeed only by one's own efforts or abilities.''}

From a statistical perspective, bootstrapping alludes to succeeding in being able to study the effects of sampling variation on estimates from the ``effort'' of a single sample. Or more precisely, \index{bootstrap!statistical reference} it refers to constructing an approximation to the sampling distribution using only one sample.

To perform this resampling with replacement virtually in Section \ref{resampling-simulation}, we used the \texttt{rep\_sample\_n()} function, making sure that the size of the resamples matched the original sample size of 50. In this section, we'll build off these ideas to construct confidence intervals using a new package: the \texttt{infer} package for ``tidy'' and transparent statistical inference.

\hypertarget{original-workflow}{%
\subsection{Original workflow}\label{original-workflow}}

Recall that in Section \ref{resampling-simulation}, we virtually performed bootstrap resampling with replacement to construct bootstrap distributions. Such distributions are approximations to the sampling distributions we saw in Chapter \ref{sampling}, but are constructed using only a single sample. Let's revisit the original workflow using the \texttt{\%\textgreater{}\%} pipe operator.

First, we used the \texttt{rep\_sample\_n()} function to resample \texttt{size\ =\ 50} pennies with replacement from the original sample of 50 pennies in \texttt{pennies\_sample} by setting \texttt{replace\ =\ TRUE}. Furthermore, we repeated this resampling 1000 times by setting \texttt{reps\ =\ 1000}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pennies\_sample }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rep\_sample\_n}\NormalTok{(}\AttributeTok{size =} \DecValTok{50}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{reps =} \DecValTok{1000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Second, since for each of our 1000 resamples of size 50, we wanted to compute a separate sample mean, we used the \texttt{dplyr} verb \texttt{group\_by()} to group observations/rows together by the \texttt{replicate} variable\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pennies\_sample }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rep\_sample\_n}\NormalTok{(}\AttributeTok{size =} \DecValTok{50}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{reps =} \DecValTok{1000}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(replicate) }
\end{Highlighting}
\end{Shaded}

\ldots{} followed by using \texttt{summarize()} to compute the sample \texttt{mean()} year for each \texttt{replicate} group:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pennies\_sample }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rep\_sample\_n}\NormalTok{(}\AttributeTok{size =} \DecValTok{50}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{reps =} \DecValTok{1000}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(replicate) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{mean\_year =} \FunctionTok{mean}\NormalTok{(year))}
\end{Highlighting}
\end{Shaded}

For this simple case, we can get by with using the \texttt{rep\_sample\_n()} function and a couple of \texttt{dplyr} verbs to construct the bootstrap distribution. However, using only \texttt{dplyr} verbs only provides us with a limited set of tools. For more complicated situations, we'll need a little more firepower. Let's repeat this using the \texttt{infer} package.

\hypertarget{infer-workflow}{%
\subsection{\texorpdfstring{\texttt{infer} package workflow}{infer package workflow}}\label{infer-workflow}}

The \texttt{infer} package is an R package for statistical inference. It makes efficient use of the \texttt{\%\textgreater{}\%} pipe operator we introduced in Section \ref{piping} to spell out the sequence of steps necessary to perform statistical inference in a ``tidy'' and transparent fashion.\index{operators!pipe} Furthermore, just as the \texttt{dplyr} package provides functions with verb-like names to perform data wrangling, the \texttt{infer} package provides functions with intuitive verb-like names to perform statistical inference.

Let's go back to our pennies. Previously, we computed the value of the sample mean \(\overline{x}\) using the \texttt{dplyr} function \texttt{summarize()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pennies\_sample }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{stat =} \FunctionTok{mean}\NormalTok{(year))}
\end{Highlighting}
\end{Shaded}

We'll see that we can also do this using \texttt{infer} functions \texttt{specify()} and \texttt{calculate()}: \index{infer!observed statistic shortcut}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pennies\_sample }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{response =}\NormalTok{ year) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"mean"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

You might be asking yourself: ``Isn't the \texttt{infer} code longer? Why would I use that code?''. While not immediately apparent, you'll see that there are three chief benefits to the \texttt{infer} workflow as opposed to the \texttt{dplyr} workflow.

First, the \texttt{infer} verb names better align with the overall resampling framework you need to understand to construct confidence intervals and to conduct hypothesis tests (in Chapter \ref{hypothesis-testing}). We'll see flowchart diagrams of this framework in the upcoming Figure \ref{fig:infer-workflow-ci} and in Chapter \ref{hypothesis-testing} with Figure \ref{fig:htdowney}.

Second, you can jump back and forth seamlessly between confidence intervals and hypothesis testing with minimal changes to your code. This will become apparent in Subsection \ref{comparing-infer-workflows} when we'll compare the \texttt{infer} code for both of these inferential methods.

Third, the \texttt{infer} workflow is much simpler for conducting inference when you have \emph{more than one variable}. We'll see two such situations. We'll first see situations of \emph{two-sample} inference\index{two-sample inference} where the sample data is collected from two groups, such as in Section \ref{case-study-two-prop-ci} where we study the contagiousness of yawning and in Section \ref{ht-activity} where we compare promotion rates of two groups at banks in the 1970s. Then in Section \ref{infer-regression}, we'll see situations of \emph{inference for regression} using the regression models you fit in Chapter \ref{regression}.

Let's now illustrate the sequence of verbs necessary to construct a confidence interval for \(\mu\), the population mean year of minting of all US pennies in 2019.

\hypertarget{specify-variables}{%
\subsubsection*{\texorpdfstring{1. \texttt{specify} variables}{1. specify variables}}\label{specify-variables}}


\begin{figure}

{\centering \includegraphics[width=0.2\linewidth,height=0.2\textheight]{images/flowcharts/infer/specify} 

}

\caption{Diagram of the specify() verb.}\label{fig:infer-specify}
\end{figure}

As shown in Figure \ref{fig:infer-specify}, the \texttt{specify()} \index{infer!specify()} function is used to choose which variables in a data frame will be the focus of our statistical inference. We do this by \texttt{specify}ing the \texttt{response} argument. For example, in our \texttt{pennies\_sample} data frame of the 50 pennies sampled from the bank, the variable of interest is \texttt{year}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pennies\_sample }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{response =}\NormalTok{ year)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Response: year (numeric)
# A tibble: 50 x 1
    year
   <dbl>
 1  2002
 2  1986
 3  2017
 4  1988
 5  2008
 6  1983
 7  2008
 8  1996
 9  2004
10  2000
# ... with 40 more rows
\end{verbatim}

Notice how the data itself doesn't change, but the \texttt{Response:\ year\ (numeric)} \emph{meta-data} does\index{meta-data}. This is similar to how the \texttt{group\_by()} verb from \texttt{dplyr} doesn't change the data, but only adds ``grouping'' meta-data, as we saw in Section \ref{groupby}.

We can also specify which variables will be the focus of our statistical inference using a \texttt{formula\ =\ y\ \textasciitilde{}\ x}. This is the same formula notation you saw in Chapters \ref{regression} and \ref{multiple-regression} on regression models: the response variable \texttt{y} is separated from the explanatory variable \texttt{x} by a \texttt{\textasciitilde{}} (``tilde''). The following use of \texttt{specify()} with the \texttt{formula} argument yields the same result seen previously:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pennies\_sample }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ year }\SpecialCharTok{\textasciitilde{}} \ConstantTok{NULL}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Since in the case of pennies we only have a response variable and no explanatory variable of interest, we set the \texttt{x} on the right-hand side of the \texttt{\textasciitilde{}} to be \texttt{NULL}.

While in the case of the pennies either specification works just fine, we'll see examples later on where the \texttt{formula} specification is simpler. In particular, this comes up in the upcoming Section \ref{case-study-two-prop-ci} on comparing two proportions and Section \ref{infer-regression} on inference for regression.

\hypertarget{generate-replicates}{%
\subsubsection*{\texorpdfstring{2. \texttt{generate} replicates}{2. generate replicates}}\label{generate-replicates}}


\begin{figure}

{\centering \includegraphics[width=0.6\linewidth,height=0.6\textheight]{images/flowcharts/infer/generate} 

}

\caption{Diagram of generate() replicates.}\label{fig:infer-generate}
\end{figure}

After we \texttt{specify()} the variables of interest, we pipe the results into the \texttt{generate()} function to generate replicates. Figure \ref{fig:infer-generate} shows how this is combined with \texttt{specify()} to start the pipeline. In other words, repeat the resampling process a large number of times. Recall in Sections \ref{bootstrap-35-replicates} and \ref{bootstrap-1000-replicates} we did this 35 and 1000 times.

The \texttt{generate()} \index{infer!generate()} function's first argument is \texttt{reps}, which sets the number of replicates we would like to generate. Since we want to resample the 50 pennies in \texttt{pennies\_sample} with replacement 1000 times, we set \texttt{reps\ =\ 1000}. The second argument \texttt{type} determines the type of computer simulation we'd like to perform. We set this to \texttt{type\ =\ "bootstrap"} indicating that we want to perform bootstrap resampling. You'll see different options for \texttt{type} in Chapter \ref{hypothesis-testing}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pennies\_sample }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{response =}\NormalTok{ year) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{, }\AttributeTok{type =} \StringTok{"bootstrap"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Response: year (numeric)
# A tibble: 50,000 x 2
# Groups:   replicate [1,000]
   replicate  year
       <int> <dbl>
 1         1  1981
 2         1  1988
 3         1  2006
 4         1  2016
 5         1  2002
 6         1  1985
 7         1  1979
 8         1  2000
 9         1  2006
10         1  2016
# ... with 49,990 more rows
\end{verbatim}

Observe that the resulting data frame has 50,000 rows. This is because we performed resampling of 50 pennies with replacement 1000 times and 50,000 = 50 \(\cdot\) 1000.

The variable \texttt{replicate} indicates which resample each row belongs to. So it has the value \texttt{1} 50 times, the value \texttt{2} 50 times, all the way through to the value \texttt{1000} 50 times. The default value of the \texttt{type} argument is \texttt{"bootstrap"} in this scenario, so if the last line was written as \texttt{generate(reps\ =\ 1000)}, we'd obtain the same results.

\textbf{Comparing with original workflow}: Note that the steps of the \texttt{infer} workflow so far produce the same results as the original workflow using the \texttt{rep\_sample\_n()} function we saw earlier. In other words, the following two code chunks produce similar results:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# infer workflow:                   \# Original workflow:}
\NormalTok{pennies\_sample }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{                  pennies\_sample }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{response =}\NormalTok{ year) }\SpecialCharTok{\%\textgreater{}\%}        \FunctionTok{rep\_sample\_n}\NormalTok{(}\AttributeTok{size =} \DecValTok{50}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{)                            }\AttributeTok{reps =} \DecValTok{1000}\NormalTok{)              }
\end{Highlighting}
\end{Shaded}

\hypertarget{calculate-summary-statistics}{%
\subsubsection*{\texorpdfstring{3. \texttt{calculate} summary statistics}{3. calculate summary statistics}}\label{calculate-summary-statistics}}


\begin{figure}

{\centering \includegraphics[width=0.8\linewidth,height=0.8\textheight]{images/flowcharts/infer/calculate} 

}

\caption{Diagram of calculate() summary statistics.}\label{fig:infer-calculate}
\end{figure}

After we \texttt{generate()} many replicates of bootstrap resampling with replacement, we next want to summarize each of the 1000 resamples of size 50 to a single sample statistic value. As seen in the diagram, the \texttt{calculate()} \index{infer!calculate()} function does this.

In our case, we want to calculate the mean \texttt{year} for each bootstrap resample of size 50. To do so, we set the \texttt{stat} argument to \texttt{"mean"}. You can also set the \texttt{stat} argument to a variety of other common summary statistics, like \texttt{"median"}, \texttt{"sum"}, \texttt{"sd"} (standard deviation), and \texttt{"prop"} (proportion). To see a list of all possible summary statistics you can use, type \texttt{?calculate} and read the help file.

Let's save the result in a data frame called \texttt{bootstrap\_distribution} and explore its contents:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bootstrap\_distribution }\OtherTok{\textless{}{-}}\NormalTok{ pennies\_sample }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{response =}\NormalTok{ year) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"mean"}\NormalTok{)}
\NormalTok{bootstrap\_distribution}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,000 x 2
   replicate    stat
       <int>   <dbl>
 1         1 1995.7 
 2         2 1994.04
 3         3 1993.62
 4         4 1994.5 
 5         5 1994.08
 6         6 1993.6 
 7         7 1995.26
 8         8 1996.64
 9         9 1994.3 
10        10 1995.94
# ... with 990 more rows
\end{verbatim}

Observe that the resulting data frame has 1000 rows and 2 columns corresponding to the 1000 \texttt{replicate} values. It also has the mean year for each bootstrap resample saved in the variable \texttt{stat}.

\textbf{Comparing with original workflow}: You may have recognized at this point that the \texttt{calculate()} step in the \texttt{infer} workflow produces the same output as the \texttt{group\_by()\ \%\textgreater{}\%\ summarize()} steps in the original workflow.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# infer workflow:                   \# Original workflow:}
\NormalTok{pennies\_sample }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{                  pennies\_sample }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{response =}\NormalTok{ year) }\SpecialCharTok{\%\textgreater{}\%}        \FunctionTok{rep\_sample\_n}\NormalTok{(}\AttributeTok{size =} \DecValTok{50}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}                        \AttributeTok{reps =} \DecValTok{1000}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}              
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"mean"}\NormalTok{)            }\FunctionTok{group\_by}\NormalTok{(replicate) }\SpecialCharTok{\%\textgreater{}\%} 
                                      \FunctionTok{summarize}\NormalTok{(}\AttributeTok{stat =} \FunctionTok{mean}\NormalTok{(year))}
\end{Highlighting}
\end{Shaded}

\hypertarget{visualize-the-results}{%
\subsubsection*{\texorpdfstring{4. \texttt{visualize} the results}{4. visualize the results}}\label{visualize-the-results}}


\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{images/flowcharts/infer/visualize} 

}

\caption{Diagram of visualize() results.}\label{fig:infer-visualize}
\end{figure}

The \texttt{visualize()} \index{infer!visualize()} verb provides a quick way to visualize the bootstrap distribution as a histogram of the numerical \texttt{stat} variable's values. The pipeline of the main \texttt{infer} verbs used for exploring bootstrap distribution results is shown in Figure \ref{fig:infer-visualize}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{visualize}\NormalTok{(bootstrap\_distribution)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/boostrap-distribution-infer-1} 

}

\caption{Bootstrap distribution.}\label{fig:boostrap-distribution-infer}
\end{figure}

\textbf{Comparing with original workflow}: In fact, \texttt{visualize()} is a \emph{wrapper function} for the \texttt{ggplot()} function that uses a \texttt{geom\_histogram()} layer. Recall that we illustrated the concept of a wrapper function in Figure \ref{fig:moderndive-figure-wrapper} in Subsection \ref{model1table}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# infer workflow:                    \# Original workflow:}
\FunctionTok{visualize}\NormalTok{(bootstrap\_distribution)    }\FunctionTok{ggplot}\NormalTok{(bootstrap\_distribution, }
                                            \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ stat)) }\SpecialCharTok{+}
                                       \FunctionTok{geom\_histogram}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

The \texttt{visualize()} function can take many other arguments which we'll see momentarily to customize the plot further. It also works with helper functions to do the shading of the histogram values corresponding to the confidence interval values.

Let's recap the steps of the \texttt{infer} workflow for constructing a bootstrap distribution and then visualizing it in Figure \ref{fig:infer-workflow-ci}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/flowcharts/infer/ci_diagram} 

}

\caption{infer package workflow for confidence intervals.}\label{fig:infer-workflow-ci}
\end{figure}

Recall how we introduced two different methods for constructing 95\% confidence intervals for an unknown population parameter in Section \ref{ci-build-up}: the \emph{percentile method} and the \emph{standard error method}. Let's now check out the \texttt{infer} package code that explicitly constructs these. There are also some additional neat functions to visualize the resulting confidence intervals built-in to the \texttt{infer} package!

\hypertarget{percentile-method-infer}{%
\subsection{\texorpdfstring{Percentile method with \texttt{infer}}{Percentile method with infer}}\label{percentile-method-infer}}

Recall the percentile method for constructing 95\% confidence intervals we introduced in Subsection \ref{percentile-method}. This method sets the lower endpoint of the confidence interval at the 2.5th percentile of the bootstrap distribution and similarly sets the upper endpoint at the 97.5th percentile. The resulting interval captures the middle 95\% of the values of the sample mean in the bootstrap distribution.

We can compute the 95\% confidence interval by piping \texttt{bootstrap\_distribution} into the \texttt{get\_confidence\_interval()} \index{infer!get\_confidence\_interval()} function from the \texttt{infer} package, with the confidence \texttt{level} set to 0.95 and the confidence interval \texttt{type} to be \texttt{"percentile"}. Let's save the results in \texttt{percentile\_ci}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{percentile\_ci }\OtherTok{\textless{}{-}}\NormalTok{ bootstrap\_distribution }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{get\_confidence\_interval}\NormalTok{(}\AttributeTok{level =} \FloatTok{0.95}\NormalTok{, }\AttributeTok{type =} \StringTok{"percentile"}\NormalTok{)}
\NormalTok{percentile\_ci}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 2
  lower_ci upper_ci
     <dbl>    <dbl>
1  1991.24  1999.42
\end{verbatim}

Alternatively, we can visualize the interval (1991.24, 1999.42) by piping the \texttt{bootstrap\_distribution} data frame into the \texttt{visualize()} function and adding a \texttt{shade\_confidence\_interval()} \index{infer!shade\_confidence\_interval()} layer. We set the \texttt{endpoints} argument to be \texttt{percentile\_ci}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{visualize}\NormalTok{(bootstrap\_distribution) }\SpecialCharTok{+} 
  \FunctionTok{shade\_confidence\_interval}\NormalTok{(}\AttributeTok{endpoints =}\NormalTok{ percentile\_ci)}
\end{Highlighting}
\end{Shaded}



\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/percentile-ci-viz-1} 

}

\caption{Percentile method 95\% confidence interval shaded corresponding to potential values.}\label{fig:percentile-ci-viz}
\end{figure}

Observe in Figure \ref{fig:percentile-ci-viz} that 95\% of the sample means stored in the \texttt{stat} variable in \texttt{bootstrap\_distribution} fall between the two endpoints marked with the darker lines, with 2.5\% of the sample means to the left of the shaded area and 2.5\% of the sample means to the right. You also have the option to change the colors of the shading using the \texttt{color} and \texttt{fill} arguments.

You can also use the shorter named function \texttt{shade\_ci()} and the results will be the same. This is for folks who don't want to type out all of \texttt{confidence\_interval} and prefer to type out \texttt{ci} instead. Try out the following code!

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{visualize}\NormalTok{(bootstrap\_distribution) }\SpecialCharTok{+} 
  \FunctionTok{shade\_ci}\NormalTok{(}\AttributeTok{endpoints =}\NormalTok{ percentile\_ci, }\AttributeTok{color =} \StringTok{"hotpink"}\NormalTok{, }\AttributeTok{fill =} \StringTok{"khaki"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{infer-se}{%
\subsection{\texorpdfstring{Standard error method with \texttt{infer}}{Standard error method with infer}}\label{infer-se}}

Recall the standard error method for constructing 95\% confidence intervals we introduced in Subsection \ref{se-method}. For any distribution that is normally shaped, roughly 95\% of the values lie within two standard deviations of the mean. In the case of the bootstrap distribution, the standard deviation has a special name: the \emph{standard error}.

So in our case, 95\% of values of the bootstrap distribution will lie within \(\pm 1.96\) standard errors of \(\overline{x}\). Thus, a 95\% confidence interval is

\[\overline{x} \pm 1.96 \cdot SE = (\overline{x} - 1.96 \cdot SE, \, \overline{x} + 1.96 \cdot SE).\]

Computation of the 95\% confidence interval can once again be done by piping the \texttt{bootstrap\_distribution} data frame we created into the \texttt{get\_confidence\_interval()} function. However, this time we set the first \texttt{type} argument to be \texttt{"se"}. Second, we must specify the \texttt{point\_estimate} argument in order to set the center of the confidence interval. We set this to be the sample mean of the original sample of 50 pennies of 1995.44 we saved in \texttt{x\_bar} earlier.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{standard\_error\_ci }\OtherTok{\textless{}{-}}\NormalTok{ bootstrap\_distribution }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{get\_confidence\_interval}\NormalTok{(}\AttributeTok{type =} \StringTok{"se"}\NormalTok{, }\AttributeTok{point\_estimate =}\NormalTok{ x\_bar)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Using `level = 0.95` to compute confidence interval.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{standard\_error\_ci}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 2
  lower_ci upper_ci
     <dbl>    <dbl>
1  1991.35  1999.53
\end{verbatim}

If we would like to visualize the interval (1991.35, 1999.53), we can once again pipe the \texttt{bootstrap\_distribution} data frame into the \texttt{visualize()} function and add a \texttt{shade\_confidence\_interval()} layer to our plot. We set the \texttt{endpoints} argument to be \texttt{standard\_error\_ci}. The resulting standard-error method based on a 95\% confidence interval for \(\mu\) can be seen in Figure \ref{fig:se-ci-viz}.



\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{visualize}\NormalTok{(bootstrap\_distribution) }\SpecialCharTok{+} 
  \FunctionTok{shade\_confidence\_interval}\NormalTok{(}\AttributeTok{endpoints =}\NormalTok{ standard\_error\_ci)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/se-ci-viz-1} 

}

\caption{Standard-error-method 95\% confidence interval.}\label{fig:se-ci-viz}
\end{figure}

As noted in Section \ref{ci-build-up}, both methods produce similar confidence intervals:

\begin{itemize}
\tightlist
\item
  Percentile method: (1991.24, 1999.42)
\item
  Standard error method: (1991.35, 1999.53)
\end{itemize}

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC8.5)} Construct a 95\% confidence interval for the \emph{median} year of minting of \emph{all} US pennies. Use the percentile method and, if appropriate, then use the standard-error method.

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{one-prop-ci}{%
\section{Interpreting confidence intervals}\label{one-prop-ci}}

Now that we've shown you how to construct confidence intervals using a sample drawn from a population, let's now focus on how to interpret their effectiveness. The effectiveness of a confidence interval is judged by whether or not it contains the true value of the population parameter. Going back to our fishing analogy in Section \ref{ci-build-up}, this is like asking, ``Did our net capture the fish?''.

So, for example, does our percentile-based confidence interval of (1991.24, 1999.42) ``capture'' the true mean year \(\mu\) of \emph{all} US pennies? Alas, we'll never know, because we don't know what the true value of \(\mu\) is. After all, we're sampling to estimate it!

In order to interpret a confidence interval's effectiveness, we need to \emph{know} what the value of the population parameter is. That way we can say whether or not a confidence interval ``captured'' this value.

Let's revisit our sampling bowl from Chapter \ref{sampling}. What proportion of the bowl's 2400 balls are red? Let's compute this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bowl }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{p\_red =} \FunctionTok{mean}\NormalTok{(color }\SpecialCharTok{==} \StringTok{"red"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 1
  p_red
  <dbl>
1 0.375
\end{verbatim}

In this case, we \emph{know} what the value of the population parameter is: we know that the population proportion \(p\) is 0.375. In other words, we know that 37.5\% of the bowl's balls are red.

As we stated in Subsection \ref{moral-of-the-story}, the sampling bowl exercise doesn't really reflect how sampling is done in real life, but rather was an \emph{idealized} activity. In real life, we won't know what the true value of the population parameter is, hence the need for estimation.

Let's now construct confidence intervals for \(p\) using our 33 groups of friends' samples from the bowl in Chapter \ref{sampling}. We'll then see if the confidence intervals ``captured'' the true value of \(p\), which we know to be 37.5\%. That is to say, ``Did the net capture the fish?''.

\hypertarget{ilyas-yohan}{%
\subsection{Did the net capture the fish?}\label{ilyas-yohan}}

Recall that we had 33 groups of friends each take samples of size 50 from the bowl and then compute the sample proportion of red balls \(\widehat{p}\). This resulted in 33 such estimates of \(p\). Let's focus on Ilyas and Yohan's sample, which is saved in the \texttt{bowl\_sample\_1} data frame in the \texttt{moderndive} package:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bowl\_sample\_1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 50 x 1
   color
   <chr>
 1 white
 2 white
 3 red  
 4 red  
 5 white
 6 white
 7 red  
 8 white
 9 white
10 white
# ... with 40 more rows
\end{verbatim}

They observed 21 red balls out of 50 and thus their sample proportion \(\widehat{p}\) was 21/50 = 0.42 = 42\%. Think of this as the ``spear'' from our fishing analogy.

Let's now follow the \texttt{infer} package workflow from Subsection \ref{infer-workflow} to create a percentile-method-based 95\% confidence interval for \(p\) using Ilyas and Yohan's sample. Think of this as the ``net.''

\hypertarget{specify-variables-1}{%
\subsubsection*{\texorpdfstring{1. \texttt{specify} variables}{1. specify variables}}\label{specify-variables-1}}


First, we \texttt{specify()} the \texttt{response} variable of interest \texttt{color}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bowl\_sample\_1 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{response =}\NormalTok{ color)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Error: A level of the response variable `color` needs to be specified for the `success`
argument in `specify()`.
\end{verbatim}

Whoops! We need to define which event is of interest! \texttt{red} or \texttt{white} balls? Since we are interested in the proportion red, let's set \texttt{success} to be \texttt{"red"}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bowl\_sample\_1 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{response =}\NormalTok{ color, }\AttributeTok{success =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Response: color (factor)
# A tibble: 50 x 1
   color
   <fct>
 1 white
 2 white
 3 red  
 4 red  
 5 white
 6 white
 7 red  
 8 white
 9 white
10 white
# ... with 40 more rows
\end{verbatim}

\hypertarget{generate-replicates-1}{%
\subsubsection*{\texorpdfstring{2. \texttt{generate} replicates}{2. generate replicates}}\label{generate-replicates-1}}


Second, we \texttt{generate()} 1000 replicates of \emph{bootstrap resampling with replacement} from \texttt{bowl\_sample\_1} by setting \texttt{reps\ =\ 1000} and \texttt{type\ =\ "bootstrap"}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bowl\_sample\_1 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{response =}\NormalTok{ color, }\AttributeTok{success =} \StringTok{"red"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{, }\AttributeTok{type =} \StringTok{"bootstrap"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Response: color (factor)
# A tibble: 50,000 x 2
# Groups:   replicate [1,000]
   replicate color
       <int> <fct>
 1         1 white
 2         1 white
 3         1 white
 4         1 white
 5         1 red  
 6         1 white
 7         1 white
 8         1 white
 9         1 white
10         1 red  
# ... with 49,990 more rows
\end{verbatim}

Observe that the resulting data frame has 50,000 rows. This is because we performed resampling of 50 balls with replacement 1000 times and thus 50,000 = 50 \(\cdot\) 1000. The variable \texttt{replicate} indicates which resample each row belongs to. So it has the value \texttt{1} 50 times, the value \texttt{2} 50 times, all the way through to the value \texttt{1000} 50 times.

\hypertarget{calculate-summary-statistics-1}{%
\subsubsection*{\texorpdfstring{3. \texttt{calculate} summary statistics}{3. calculate summary statistics}}\label{calculate-summary-statistics-1}}


Third, we summarize each of the 1000 resamples of size 50 with the proportion of \emph{successes}. In other words, the proportion of the balls that are \texttt{"red"}. We can set the summary statistic to be calculated as the proportion by setting the \texttt{stat} argument to be \texttt{"prop"}. Let's save the result as \texttt{sample\_1\_bootstrap}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample\_1\_bootstrap }\OtherTok{\textless{}{-}}\NormalTok{ bowl\_sample\_1 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{response =}\NormalTok{ color, }\AttributeTok{success =} \StringTok{"red"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{, }\AttributeTok{type =} \StringTok{"bootstrap"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"prop"}\NormalTok{)}
\NormalTok{sample\_1\_bootstrap}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,000 x 2
   replicate  stat
       <int> <dbl>
 1         1  0.32
 2         2  0.42
 3         3  0.44
 4         4  0.4 
 5         5  0.44
 6         6  0.52
 7         7  0.38
 8         8  0.44
 9         9  0.34
10        10  0.42
# ... with 990 more rows
\end{verbatim}

Observe there are 1000 rows in this data frame and thus 1000 values of the variable \texttt{stat}. These 1000 values of \texttt{stat} represent our 1000 replicated values of the proportion, each based on a different resample.

\hypertarget{visualize-the-results-1}{%
\subsubsection*{\texorpdfstring{4. \texttt{visualize} the results}{4. visualize the results}}\label{visualize-the-results-1}}


Fourth and lastly, let's compute the resulting 95\% confidence interval.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{percentile\_ci\_1 }\OtherTok{\textless{}{-}}\NormalTok{ sample\_1\_bootstrap }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{get\_confidence\_interval}\NormalTok{(}\AttributeTok{level =} \FloatTok{0.95}\NormalTok{, }\AttributeTok{type =} \StringTok{"percentile"}\NormalTok{)}
\NormalTok{percentile\_ci\_1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 2
  lower_ci upper_ci
     <dbl>    <dbl>
1      0.3     0.56
\end{verbatim}

Let's visualize the bootstrap distribution along with the \texttt{percentile\_ci\_1} percentile-based 95\% confidence interval for \(p\) in Figure \ref{fig:shovel-bootstrap-1-infer}. We'll adjust the number of bins to better see the resulting shape. Furthermore, we'll add a dashed vertical line at Ilyas and Yohan's observed \(\widehat{p}\) = 21/50 = 0.42 = 42\% using \texttt{geom\_vline()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample\_1\_bootstrap }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{visualize}\NormalTok{(}\AttributeTok{bins =} \DecValTok{15}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{shade\_confidence\_interval}\NormalTok{(}\AttributeTok{endpoints =}\NormalTok{ percentile\_ci\_1) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \FloatTok{0.42}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/shovel-bootstrap-1-infer-1} 

}

\caption{Bootstrap distribution.}\label{fig:shovel-bootstrap-1-infer}
\end{figure}

Did Ilyas and Yohan's net capture the fish? Did their 95\% confidence interval for \(p\) based on their sample contain the true value of \(p\) of 0.375? Yes! 0.375 is between the endpoints of their confidence interval (0.3, 0.56).

However, will \emph{every} 95\% confidence interval for \(p\) capture this value? In other words, if we had a different sample of 50 balls and constructed a different confidence interval, would it necessarily contain \(p\) = 0.375 as well? Let's see!

Let's first take a different sample from the bowl, this time using the computer as we did in Chapter \ref{sampling}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bowl\_sample\_2 }\OtherTok{\textless{}{-}}\NormalTok{ bowl }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{rep\_sample\_n}\NormalTok{(}\AttributeTok{size =} \DecValTok{50}\NormalTok{)}
\NormalTok{bowl\_sample\_2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 50 x 3
# Groups:   replicate [1]
   replicate ball_ID color
       <int>   <int> <chr>
 1         1    1665 red  
 2         1    1312 red  
 3         1    2105 red  
 4         1     810 white
 5         1     189 white
 6         1    1429 white
 7         1    2294 red  
 8         1    1233 white
 9         1    1951 white
10         1    2061 white
# ... with 40 more rows
\end{verbatim}

Let's reapply the same \texttt{infer} functions on \texttt{bowl\_sample\_2} to generate a different 95\% confidence interval for \(p\). First, we create the new bootstrap distribution and save the results in \texttt{sample\_2\_bootstrap}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample\_2\_bootstrap }\OtherTok{\textless{}{-}}\NormalTok{ bowl\_sample\_2 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{response =}\NormalTok{ color, }
          \AttributeTok{success =} \StringTok{"red"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{, }
           \AttributeTok{type =} \StringTok{"bootstrap"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"prop"}\NormalTok{)}
\NormalTok{sample\_2\_bootstrap}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,000 x 2
   replicate  stat
       <int> <dbl>
 1         1  0.48
 2         2  0.38
 3         3  0.32
 4         4  0.32
 5         5  0.34
 6         6  0.26
 7         7  0.3 
 8         8  0.36
 9         9  0.44
10        10  0.36
# ... with 990 more rows
\end{verbatim}

We once again compute a percentile-based 95\% confidence interval for \(p\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{percentile\_ci\_2 }\OtherTok{\textless{}{-}}\NormalTok{ sample\_2\_bootstrap }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{get\_confidence\_interval}\NormalTok{(}\AttributeTok{level =} \FloatTok{0.95}\NormalTok{, }\AttributeTok{type =} \StringTok{"percentile"}\NormalTok{)}
\NormalTok{percentile\_ci\_2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 2
  lower_ci upper_ci
     <dbl>    <dbl>
1      0.2     0.48
\end{verbatim}

Does this new net capture the fish? In other words, does the 95\% confidence interval for \(p\) based on the new sample contain the true value of \(p\) of 0.375? Yes again! 0.375 is between the endpoints of our confidence interval (0.2, 0.48).

Let's now repeat this process 100 more times: we take 100 virtual samples from the bowl and construct 100 95\% confidence intervals. Let's visualize the results in Figure \ref{fig:reliable-percentile} where:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We mark the true value of \(p = 0.375\) with a vertical line.
\item
  We mark each of the 100 95\% confidence intervals with horizontal lines. These are the ``nets.''
\item
  The horizontal line is colored grey if the confidence interval ``captures'' the true value of \(p\) marked with the vertical line. The horizontal line is colored black otherwise.
\end{enumerate}



\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/reliable-percentile-1} 

}

\caption{100 percentile-based 95\% confidence intervals for \(p\).}\label{fig:reliable-percentile}
\end{figure}

Of the 100 95\% confidence intervals, 95 of them captured the true value \(p = 0.375\), whereas 5 of them didn't. In other words, 95 of our nets caught the fish, whereas 5 of our nets didn't.

This is where the ``95\% confidence level'' we defined in Section \ref{ci-build-up} comes into play: for every 100 95\% confidence intervals, we \emph{expect} that 95 of them will capture \(p\) and that five of them won't.

Note that ``expect'' is a probabilistic statement referring to a long-run average. In other words, for every 100 confidence intervals, we will observe \emph{about} 95 confidence intervals that capture \(p\), but not necessarily exactly 95. In Figure \ref{fig:reliable-percentile} for example, 95 of the confidence intervals capture \(p\).

To further accentuate our point about confidence levels, let's generate a figure similar to Figure \ref{fig:reliable-percentile}, but this time constructing 80\% standard-error method based confidence intervals instead. Let's visualize the results in Figure \ref{fig:reliable-se} with the scale on the x-axis being the same as in Figure \ref{fig:reliable-percentile} to make comparison easy. Furthermore, since all standard-error method confidence intervals for \(p\) are centered at their respective point estimates \(\widehat{p}\), we mark this value on each line with dots.



\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/reliable-se-1} 

}

\caption{100 SE-based 80\% confidence intervals for \(p\) with point estimate center marked with dots.}\label{fig:reliable-se}
\end{figure}

Observe how the 80\% confidence intervals are narrower than the 95\% confidence intervals, reflecting our lower degree of confidence. Think of this as using a smaller ``net.'' We'll explore other determinants of confidence interval width in the upcoming Subsection \ref{ci-width}.

Furthermore, observe that of the 100 80\% confidence intervals, 82 of them captured the population proportion \(p\) = 0.375, whereas 18 of them did not. Since we lowered the confidence level from 95\% to 80\%, we now have a much larger number of confidence intervals that failed to ``catch the fish.''

\hypertarget{shorthand}{%
\subsection{Precise and shorthand interpretation}\label{shorthand}}

\index{confidence interval!interpretation}

Let's return our attention to 95\% confidence intervals. The precise and mathematically correct interpretation of a 95\% confidence interval is a little long-winded:

\begin{quote}
Precise interpretation: If we repeated our sampling procedure a large number of times, we expect about 95\% of the resulting confidence intervals to capture the value of the population parameter.
\end{quote}

This is what we observed in Figure \ref{fig:reliable-percentile}. Our confidence interval construction procedure is 95\% \emph{reliable}. That is to say, we can expect our confidence intervals to include the true population parameter about 95\% of the time.

A common but incorrect interpretation is: ``There is a 95\% probability that the confidence interval contains \(p\).'' Looking at Figure \ref{fig:reliable-percentile}, each of the confidence intervals either does or doesn't contain \(p\). In other words, the probability is either a 1 or a 0.

So if the 95\% confidence level only relates to the reliability of the confidence interval construction procedure and not to a given confidence interval itself, what insight can be derived from a given confidence interval? For example, going back to the pennies example, we found that the percentile method 95\% confidence interval for \(\mu\) was (1991.24, 1999.42), whereas the standard error method 95\% confidence interval was (1991.35, 1999.53). What can be said about these two intervals?

Loosely speaking, we can think of these intervals as our ``best guess'' of a plausible range of values for the mean year \(\mu\) of \emph{all} US pennies. For the rest of this book, we'll use the following shorthand summary of the precise interpretation.

\begin{quote}
Short-hand interpretation: We are 95\% ``confident'' that a 95\% confidence interval captures the value of the population parameter.
\end{quote}

We use quotation marks around ``confident'' to emphasize that while 95\% relates to the reliability of our confidence interval construction procedure, ultimately a constructed confidence interval is our best guess of an interval that contains the population parameter. In other words, it's our best net.

So returning to our pennies example and focusing on the percentile method, we are 95\% ``confident'' that the true mean year of pennies in circulation in 2019 is somewhere between 1991.24 and 1999.42.

\hypertarget{ci-width}{%
\subsection{Width of confidence intervals}\label{ci-width}}

Now that we know how to interpret confidence intervals, let's go over some factors that determine their width.

\hypertarget{impact-of-confidence-level}{%
\subsubsection*{Impact of confidence level}\label{impact-of-confidence-level}}


One factor that determines confidence interval widths is the pre-specified confidence level. For example, in Figures \ref{fig:reliable-percentile} and \ref{fig:reliable-se}, we compared the widths of 95\% and 80\% confidence intervals and observed that the 95\% confidence intervals were wider. The quantification of the confidence level should match what many expect of the word ``confident.'' In order to be more confident in our best guess of a range of values, we need to widen the range of values.

To elaborate on this, imagine we want to guess the forecasted high temperature in Seoul, South Korea on August 15th. Given Seoul's temperate climate with four distinct seasons, we could say somewhat confidently that the high temperature would be between 50°F - 95°F (10°C - 35°C). However, if we wanted a temperature range we were \emph{absolutely} confident about, we would need to widen it.

We need this wider range to allow for the possibility of anomalous weather, like a freak cold spell or an extreme heat wave. So a range of temperatures we could be near certain about would be between 32°F - 110°F (0°C - 43°C). On the other hand, if we could tolerate being a little less confident, we could narrow this range to between 70°F - 85°F (21°C - 30°C).

Let's revisit our sampling bowl from Chapter \ref{sampling}. Let's compare \(10 \cdot 3 = 30\) confidence intervals for \(p\) based on three different confidence levels: 80\%, 95\%, and 99\%.

Specifically, we'll first take 30 different random samples of size \(n\) = 50 balls from the bowl. Then we'll construct 10 percentile-based confidence intervals using each of the three different confidence levels.

Finally, we'll compare the widths of these intervals. We visualize the resulting confidence intervals in Figure \ref{fig:reliable-percentile-80-95-99} along with a vertical line marking the true value of \(p\) = 0.375.



\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/reliable-percentile-80-95-99-1} 

}

\caption{Ten 80, 95, and 99\% confidence intervals for \(p\) based on \(n = 50\).}\label{fig:reliable-percentile-80-95-99}
\end{figure}

Observe that as the confidence level increases from 80\% to 95\% to 99\%, the confidence intervals tend to get wider as seen in Table \ref{tab:perc-cis-average-width} where we compare their average widths.

\begingroup\fontsize{10}{12}\selectfont

\begin{longtable}[t]{lr}
\caption{\label{tab:perc-cis-average-width}Average width of 80, 95, and 99\% confidence intervals}\\
\toprule
Confidence level & Mean width\\
\midrule
\endfirsthead
\caption[]{\label{tab:perc-cis-average-width}Average width of 80, 95, and 99\% confidence intervals \textit{(continued)}}\\
\toprule
Confidence level & Mean width\\
\midrule
\endhead

\endfoot
\bottomrule
\endlastfoot
80\% & 0.162\\
95\% & 0.262\\
99\% & 0.338\\*
\end{longtable}
\endgroup{}

So in order to have a higher confidence level, our confidence intervals must be wider. Ideally, we would have both a high confidence level and narrow confidence intervals. However, we cannot have it both ways. If we want to \emph{be more confident}, we need to allow for wider intervals. Conversely, if we would like a narrow interval, we must tolerate a lower confidence level.

The moral of the story is: \index{confidence interval!impact of confidence level on interval width} \textbf{Higher confidence levels tend to produce wider confidence intervals.} When looking at Figure \ref{fig:reliable-percentile-80-95-99} it is important to keep in mind that we kept the sample size fixed at \(n\) = 50. Thus, all \(10 \cdot 3 = 30\) random samples from the \texttt{bowl} had the same sample size. What happens if instead we took samples of different sizes? Recall that we did this in Subsection \ref{different-shovels} using virtual shovels with 25, 50, and 100 slots.

\hypertarget{impact-of-sample-size}{%
\subsubsection*{Impact of sample size}\label{impact-of-sample-size}}


This time, let's fix the confidence level at 95\%, but consider three different sample sizes for \(n\): 25, 50, and 100. Specifically, we'll first take 10 different random samples of size 25, 10 different random samples of size 50, and 10 different random samples of size 100. We'll then construct 95\% percentile-based confidence intervals for each sample. Finally, we'll compare the widths of these intervals. We visualize the resulting 30 confidence intervals in Figure \ref{fig:reliable-percentile-n-25-50-100}. Note also the vertical line marking the true value of \(p\) = 0.375.



\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/reliable-percentile-n-25-50-100-1} 

}

\caption{Ten 95\% confidence intervals for \(p\) with \(n = 25, 50,\) and \(100\).}\label{fig:reliable-percentile-n-25-50-100}
\end{figure}

Observe that as the confidence intervals are constructed from larger and larger sample sizes, they tend to get narrower. Let's compare the average widths in Table \ref{tab:perc-cis-average-width-2}.

\begingroup\fontsize{10}{12}\selectfont

\begin{longtable}[t]{lr}
\caption{\label{tab:perc-cis-average-width-2}Average width of 95\% confidence intervals based on $n = 25$, $50$, and $100$}\\
\toprule
Sample size & Mean width\\
\midrule
\endfirsthead
\caption[]{\label{tab:perc-cis-average-width-2}Average width of 95\% confidence intervals based on $n = 25$, $50$, and $100$ \textit{(continued)}}\\
\toprule
Sample size & Mean width\\
\midrule
\endhead

\endfoot
\bottomrule
\endlastfoot
n = 25 & 0.380\\
n = 50 & 0.268\\
n = 100 & 0.189\\*
\end{longtable}
\endgroup{}

The moral of the story is: \index{confidence interval!impact of sample size on interval width} \textbf{Larger sample sizes tend to produce narrower confidence intervals.} Recall that this was a key message in Subsection \ref{moral-of-the-story}. As we used larger and larger shovels for our samples, the sample proportions red \(\widehat{p}\) tended to vary less. In other words, our estimates got more and more \emph{precise}.

Recall that we visualized these results in Figure \ref{fig:comparing-sampling-distributions-3}, where we compared the \emph{sampling distributions} for \(\widehat{p}\) based on samples of size \(n\) equal 25, 50, and 100. We also quantified the sampling variation of these sampling distributions using their standard deviation, which has that special name: the \emph{standard error}. So as the sample size increases, the standard error decreases.

In fact, the standard error is another related factor in determining confidence interval width. We'll explore this fact in Subsection \ref{theory-ci} when we discuss theory-based methods for constructing confidence intervals using mathematical formulas. Such methods are an alternative to the computer-based methods we've been using so far.

\hypertarget{case-study-two-prop-ci}{%
\section{Case study: Is yawning contagious?}\label{case-study-two-prop-ci}}

Let's apply our knowledge of confidence intervals to answer the question: ``Is yawning contagious?''. If you see someone else yawn, are you more likely to yawn? In an episode of the US show \href{http://www.discovery.com/tv-shows/mythbusters/mythbusters-database/yawning-contagious/}{\emph{Mythbusters}}, the hosts conducted an experiment to answer this question. The episode is available to view in the United States on the Discovery Network website \href{https://www.discovery.com/tv-shows/mythbusters/videos/is-yawning-contagious}{here} and more information about the episode is also available on \href{https://www.imdb.com/title/tt0768479/}{IMDb}.

\hypertarget{mythbusters-study-data}{%
\subsection{\texorpdfstring{\emph{Mythbusters} study data}{Mythbusters study data}}\label{mythbusters-study-data}}

Fifty adult participants who thought they were being considered for an appearance on the show were interviewed by a show recruiter. In the interview, the recruiter either yawned or did not. Participants then sat by themselves in a large van and were asked to wait. While in the van, the \emph{Mythbusters} team watched the participants using a hidden camera to see if they yawned. The data frame containing the results of their experiment is available in the \texttt{mythbusters\_yawn} data frame included in the \texttt{moderndive} package: \index{moderndive!mythbusters\_yawn}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mythbusters\_yawn}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 50 x 3
    subj group   yawn 
   <int> <chr>   <chr>
 1     1 seed    yes  
 2     2 control yes  
 3     3 seed    no   
 4     4 seed    yes  
 5     5 seed    no   
 6     6 control no   
 7     7 seed    yes  
 8     8 control no   
 9     9 control no   
10    10 seed    no   
# ... with 40 more rows
\end{verbatim}

The variables are:

\begin{itemize}
\tightlist
\item
  \texttt{subj}: The participant ID with values 1 through 50.
\item
  \texttt{group}: A binary \emph{treatment} variable indicating whether the participant was exposed to yawning. \texttt{"seed"} indicates the participant was exposed to yawning while \texttt{"control"} indicates the participant was not.
\item
  \texttt{yawn}: A binary \emph{response} variable indicating whether the participant ultimately yawned.
\end{itemize}

Recall that you learned about treatment and response variables in Subsection \ref{correlation-is-not-causation} in our discussion on confounding variables. \index{variables!treatment}\index{variables!response}

Let's use some data wrangling to obtain counts of the four possible outcomes:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mythbusters\_yawn }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(group, yawn) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{count =} \FunctionTok{n}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 3
# Groups:   group [2]
  group   yawn  count
  <chr>   <chr> <int>
1 control no       12
2 control yes       4
3 seed    no       24
4 seed    yes      10
\end{verbatim}

Let's first focus on the \texttt{"control"} group participants who were not exposed to yawning. 12 such participants did not yawn, while 4 such participants did. So out of the 16 people who were not exposed to yawning, 4/16 = 0.25 = 25\% did yawn.

Let's now focus on the \texttt{"seed"} group participants who were exposed to yawning where 24 such participants did not yawn, while 10 such participants did yawn. So out of the 34 people who were exposed to yawning, 10/34 = 0.294 = 29.4\% did yawn. Comparing these two percentages, the participants who were exposed to yawning yawned 29.4\% - 25\% = 4.4\% more often than those who were not.

\hypertarget{sampling-scenario}{%
\subsection{Sampling scenario}\label{sampling-scenario}}

Let's review the terminology and notation related to sampling we studied in Subsection \ref{terminology-and-notation}. In Chapter \ref{sampling} our \emph{study population} was the bowl of \(N\) = 2400 balls. Our \emph{population parameter} of interest was the \emph{population proportion} of these balls that were red, denoted mathematically by \(p\). In order to estimate \(p\), we extracted a sample of 50 balls using the shovel and computed the relevant \emph{point estimate}: the \emph{sample proportion} that were red, denoted mathematically by \(\widehat{p}\).

Who is the study population here? All humans? All the people who watch the show \emph{Mythbusters}? It's hard to say! This question can only be answered if we know how the show's hosts recruited participants! In other words, what was the \emph{sampling methodology}\index{sampling methodology} used by the \emph{Mythbusters} to recruit participants? We alas are not provided with this information. Only for the purposes of this case study, however, we'll \emph{assume} that the 50 participants are a representative sample of all Americans given the popularity of this show. Thus, we'll be assuming that any results of this experiment will generalize to all \(N\) = 327 million Americans (2018 population).

Just like with our sampling bowl, the population parameter here will involve proportions. However, in this case it will be the \emph{difference in population proportions} \(p_{seed} - p_{control}\), where \(p_{seed}\) is the proportion of \emph{all} Americans who if exposed to yawning will yawn themselves, and \(p_{control}\) is the proportion of \emph{all} Americans who if not exposed to yawning still yawn themselves. Correspondingly, the point estimate/sample statistic based the \emph{Mythbusters}' sample of participants will be the \emph{difference in sample proportions} \(\widehat{p}_{seed} - \widehat{p}_{control}\). Let's extend Table \ref{tab:table-ch8} of scenarios of sampling for inference to include our latest scenario.

\begin{table}[!h]

\caption{\label{tab:table-ch8-c}Scenarios of sampling for inference}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{>{\raggedleft\arraybackslash}p{0.5in}>{\raggedright\arraybackslash}p{1.5in}>{\raggedright\arraybackslash}p{0.65in}>{\raggedright\arraybackslash}p{1.6in}>{\raggedright\arraybackslash}p{0.65in}}
\toprule
Scenario & Population parameter & Notation & Point estimate & Symbol(s)\\
\midrule
1 & Population proportion & $p$ & Sample proportion & $\widehat{p}$\\
2 & Population mean & $\mu$ & Sample mean & $\overline{x}$ or $\widehat{\mu}$\\
3 & Difference in population proportions & $p_1 - p_2$ & Difference in sample proportions & $\widehat{p}_1 - \widehat{p}_2$\\
\bottomrule
\end{tabular}
\end{table}

This is known as a \emph{two-sample} inference\index{two-sample inference} situation since we have two separate samples. Based on their two-samples of size \(n_{seed}\) = 34 and \(n_{control}\) = 16, the point estimate is

\[
\widehat{p}_{seed} - \widehat{p}_{control} = \frac{24}{34} - \frac{12}{16} = 0.04411765 \approx 4.4\%
\]

However, say the \emph{Mythbusters} repeated this experiment. In other words, say they recruited 50 new participants and exposed 34 of them to yawning and 16 not. Would they obtain the exact same estimated difference of 4.4\%? Probably not, again, because of \emph{sampling variation}.

How does this sampling variation affect their estimate of 4.4\%? In other words, what would be a plausible range of values for this difference that accounts for this sampling variation? We can answer this question with confidence intervals! Furthermore, since the \emph{Mythbusters} only have a single two-sample of 50 participants, they would have to construct a 95\% confidence interval for \(p_{seed} - p_{control}\) using \emph{bootstrap resampling with replacement}.

We make a couple of important notes. First, for the comparison between the \texttt{"seed"} and \texttt{"control"} groups to make sense, however, both groups need to be \emph{independent} from each other. Otherwise, they could influence each other's results. This means that a participant being selected for the \texttt{"seed"} or \texttt{"control"} group has no influence on another participant being assigned to one of the two groups. As an example, if there were a mother and her child as participants in the study, they wouldn't necessarily be in the same group. They would each be assigned randomly to one of the two groups of the explanatory variable.

Second, the order of the subtraction in the difference doesn't matter so long as you are consistent and tailor your interpretations accordingly. In other words, using a point estimate of \(\widehat{p}_{seed} - \widehat{p}_{control}\) or \(\widehat{p}_{control} - \widehat{p}_{seed}\) does not make a material difference, you just need to stay consistent and interpret your results accordingly.

\hypertarget{ci-build}{%
\subsection{Constructing the confidence interval}\label{ci-build}}

As we did in Subsection \ref{infer-workflow}, let's first construct the bootstrap distribution for \(\widehat{p}_{seed} - \widehat{p}_{control}\) and then use this to construct 95\% confidence intervals for \(p_{seed} - p_{control}\). We'll do this using the \texttt{infer} workflow again. However, since the difference in proportions is a new scenario for inference, we'll need to use some new arguments in the \texttt{infer} functions along the way.

\hypertarget{specify-variables-2}{%
\subsubsection*{\texorpdfstring{1. \texttt{specify} variables}{1. specify variables}}\label{specify-variables-2}}


Let's take our \texttt{mythbusters\_yawn} data frame and \texttt{specify()} which variables are of interest using the \texttt{y\ \textasciitilde{}\ x} formula interface where:

\begin{itemize}
\tightlist
\item
  Our response variable is \texttt{yawn}: whether or not a participant yawned. It has levels \texttt{"yes"} and \texttt{"no"}.
\item
  The explanatory variable is \texttt{group}: whether or not a participant was exposed to yawning. It has levels \texttt{"seed"} (exposed to yawning) and \texttt{"control"} (not exposed to yawning).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mythbusters\_yawn }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ yawn }\SpecialCharTok{\textasciitilde{}}\NormalTok{ group)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Error: A level of the response variable `yawn` needs to be 
specified for the `success` argument in `specify()`.
\end{verbatim}

Alas, we got an error message similar to the one from Subsection \ref{ilyas-yohan}: \texttt{infer} is telling us that one of the levels of the categorical variable \texttt{yawn} needs to be defined as the \texttt{success}. Recall that we define \texttt{success} to be the event of interest we are trying to count and compute proportions of. Are we interested in those participants who \texttt{"yes"} yawned or those who \texttt{"no"} didn't yawn? This isn't clear to R or someone just picking up the code and results for the first time, so we need to set the \texttt{success} argument to \texttt{"yes"} as follows to improve the transparency of the code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mythbusters\_yawn }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ yawn }\SpecialCharTok{\textasciitilde{}}\NormalTok{ group, }\AttributeTok{success =} \StringTok{"yes"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Response: yawn (factor)
Explanatory: group (factor)
# A tibble: 50 x 2
   yawn  group  
   <fct> <fct>  
 1 yes   seed   
 2 yes   control
 3 no    seed   
 4 yes   seed   
 5 no    seed   
 6 no    control
 7 yes   seed   
 8 no    control
 9 no    control
10 no    seed   
# ... with 40 more rows
\end{verbatim}

\hypertarget{generate-replicates-2}{%
\subsubsection*{\texorpdfstring{2. \texttt{generate} replicates}{2. generate replicates}}\label{generate-replicates-2}}


Our next step is to perform \emph{bootstrap resampling with replacement} like we did with the slips of paper in our pennies activity in Section \ref{resampling-tactile}. We saw how it works with both a single variable in computing bootstrap means in Section \ref{bootstrap-process} and in computing bootstrap proportions in Section \ref{one-prop-ci}, but we haven't yet worked with bootstrapping involving multiple variables.

In the \texttt{infer} package, bootstrapping with multiple variables means that each \emph{row} is potentially resampled. Let's investigate this by focusing only on the first six rows of \texttt{mythbusters\_yawn}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{first\_six\_rows }\OtherTok{\textless{}{-}} \FunctionTok{head}\NormalTok{(mythbusters\_yawn)}
\NormalTok{first\_six\_rows}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 3
   subj group   yawn 
  <int> <chr>   <chr>
1     1 seed    yes  
2     2 control yes  
3     3 seed    no   
4     4 seed    yes  
5     5 seed    no   
6     6 control no   
\end{verbatim}

When we bootstrap this data, we are potentially pulling the subject's readings multiple times. Thus, we could see the entries of \texttt{"seed"} for \texttt{group} and \texttt{"no"} for \texttt{yawn} together in a new row in a bootstrap sample. This is further seen by exploring the \texttt{sample\_n()} function in \texttt{dplyr} on this smaller 6-row data frame comprised of \texttt{head(mythbusters\_yawn)}. The \texttt{sample\_n()} function can perform this bootstrapping procedure and is similar to the \texttt{rep\_sample\_n()} function in \texttt{infer}, except that it is not repeated, but rather only performs one sample with or without replacement.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{first\_six\_rows }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{sample\_n}\NormalTok{(}\AttributeTok{size =} \DecValTok{6}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 3
   subj group   yawn 
  <int> <chr>   <chr>
1     1 seed    yes  
2     6 control no   
3     1 seed    yes  
4     5 seed    no   
5     4 seed    yes  
6     4 seed    yes  
\end{verbatim}

We can see that in this bootstrap sample generated from the first six rows of \texttt{mythbusters\_yawn}, we have some rows repeated. The same is true when we perform the \texttt{generate()} step in \texttt{infer} as done in what follows. Using this fact, we \texttt{generate} 1000 replicates, or, in other words, we bootstrap resample the 50 participants with replacement 1000 times.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mythbusters\_yawn }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ yawn }\SpecialCharTok{\textasciitilde{}}\NormalTok{ group, }\AttributeTok{success =} \StringTok{"yes"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{, }\AttributeTok{type =} \StringTok{"bootstrap"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Response: yawn (factor)
Explanatory: group (factor)
# A tibble: 50,000 x 3
# Groups:   replicate [1,000]
   replicate yawn  group  
       <int> <fct> <fct>  
 1         1 yes   seed   
 2         1 yes   control
 3         1 no    control
 4         1 no    control
 5         1 yes   seed   
 6         1 yes   seed   
 7         1 yes   seed   
 8         1 yes   seed   
 9         1 no    seed   
10         1 yes   seed   
# ... with 49,990 more rows
\end{verbatim}

Observe that the resulting data frame has 50,000 rows. This is because we performed resampling of 50 participants with replacement 1000 times and 50,000 = 1000 \(\cdot\) 50. The variable \texttt{replicate} indicates which resample each row belongs to. So it has the value \texttt{1} 50 times, the value \texttt{2} 50 times, all the way through to the value \texttt{1000} 50 times.

\hypertarget{calculate-summary-statistics-2}{%
\subsubsection*{\texorpdfstring{3. \texttt{calculate} summary statistics}{3. calculate summary statistics}}\label{calculate-summary-statistics-2}}


After we \texttt{generate()} many replicates of bootstrap resampling with replacement, we next want to summarize the bootstrap resamples of size 50 with a single summary statistic, the difference in proportions. We do this by setting the \texttt{stat} argument to \texttt{"diff\ in\ props"}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mythbusters\_yawn }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ yawn }\SpecialCharTok{\textasciitilde{}}\NormalTok{ group, }\AttributeTok{success =} \StringTok{"yes"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{, }\AttributeTok{type =} \StringTok{"bootstrap"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"diff in props"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Error: Statistic is based on a difference; specify the `order` in which to
subtract the levels of the explanatory variable.
\end{verbatim}

We see another error here. We need to specify the order of the subtraction. Is it \(\widehat{p}_{seed} - \widehat{p}_{control}\) or \(\widehat{p}_{control} - \widehat{p}_{seed}\). We specify it to be \(\widehat{p}_{seed} - \widehat{p}_{control}\) by setting \texttt{order\ =\ c("seed",\ "control")}. Note that you could've also set \texttt{order\ =\ c("control",\ "seed")}. As we stated earlier, the order of the subtraction does not matter, so long as you stay consistent throughout your analysis and tailor your interpretations accordingly.

Let's save the output in a data frame \texttt{bootstrap\_distribution\_yawning}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bootstrap\_distribution\_yawning }\OtherTok{\textless{}{-}}\NormalTok{ mythbusters\_yawn }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ yawn }\SpecialCharTok{\textasciitilde{}}\NormalTok{ group, }\AttributeTok{success =} \StringTok{"yes"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{, }\AttributeTok{type =} \StringTok{"bootstrap"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"diff in props"}\NormalTok{, }\AttributeTok{order =} \FunctionTok{c}\NormalTok{(}\StringTok{"seed"}\NormalTok{, }\StringTok{"control"}\NormalTok{))}
\NormalTok{bootstrap\_distribution\_yawning}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,000 x 2
   replicate        stat
       <int>       <dbl>
 1         1  0.0357143 
 2         2  0.229167  
 3         3  0.00952381
 4         4  0.0106952 
 5         5  0.00483092
 6         6  0.00793651
 7         7 -0.0845588 
 8         8 -0.00466200
 9         9  0.164686  
10        10  0.124777  
# ... with 990 more rows
\end{verbatim}

Observe that the resulting data frame has 1000 rows and 2 columns corresponding to the 1000 \texttt{replicate} ID's and the 1000 differences in proportions for each bootstrap resample in \texttt{stat}.

\hypertarget{visualize-the-results-2}{%
\subsubsection*{\texorpdfstring{4. \texttt{visualize} the results}{4. visualize the results}}\label{visualize-the-results-2}}


In Figure \ref{fig:bootstrap-distribution-mythbusters} we \texttt{visualize()} the resulting bootstrap resampling distribution. Let's also add a vertical line at 0 by adding a \texttt{geom\_vline()} layer.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{visualize}\NormalTok{(bootstrap\_distribution\_yawning) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/bootstrap-distribution-mythbusters-1} 

}

\caption{Bootstrap distribution.}\label{fig:bootstrap-distribution-mythbusters}
\end{figure}

First, let's compute the 95\% confidence interval for \(p_{seed} - p_{control}\) using the percentile method, in other words, by identifying the 2.5th and 97.5th percentiles which include the middle 95\% of values. Recall that this method does not require the bootstrap distribution to be normally shaped.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bootstrap\_distribution\_yawning }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{get\_confidence\_interval}\NormalTok{(}\AttributeTok{type =} \StringTok{"percentile"}\NormalTok{, }\AttributeTok{level =} \FloatTok{0.95}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 2
   lower_ci upper_ci
      <dbl>    <dbl>
1 -0.238276 0.302464
\end{verbatim}

Second, since the bootstrap distribution is roughly bell-shaped, we can construct a confidence interval using the standard error method as well. Recall that to construct a confidence interval using the standard error method, we need to specify the center of the interval using the \texttt{point\_estimate} argument. In our case, we need to set it to be the difference in sample proportions of 4.4\% that the \emph{Mythbusters} observed.

We can also use the \texttt{infer} workflow to compute this value by excluding the \texttt{generate()} 1000 bootstrap replicates step. In other words, do not generate replicates, but rather use only the original sample data. We can achieve this by commenting out the \texttt{generate()} line, telling R to ignore it:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{obs\_diff\_in\_props }\OtherTok{\textless{}{-}}\NormalTok{ mythbusters\_yawn }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ yawn }\SpecialCharTok{\textasciitilde{}}\NormalTok{ group, }\AttributeTok{success =} \StringTok{"yes"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \CommentTok{\# generate(reps = 1000, type = "bootstrap") \%\textgreater{}\% }
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"diff in props"}\NormalTok{, }\AttributeTok{order =} \FunctionTok{c}\NormalTok{(}\StringTok{"seed"}\NormalTok{, }\StringTok{"control"}\NormalTok{))}
\NormalTok{obs\_diff\_in\_props}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 1
       stat
      <dbl>
1 0.0441176
\end{verbatim}

We thus plug this value in as the \texttt{point\_estimate} argument.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{myth\_ci\_se }\OtherTok{\textless{}{-}}\NormalTok{ bootstrap\_distribution\_yawning }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{get\_confidence\_interval}\NormalTok{(}\AttributeTok{type =} \StringTok{"se"}\NormalTok{, }\AttributeTok{point\_estimate =}\NormalTok{ obs\_diff\_in\_props)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Using `level = 0.95` to compute confidence interval.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{myth\_ci\_se}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 2
   lower_ci upper_ci
      <dbl>    <dbl>
1 -0.227291 0.315526
\end{verbatim}

Let's visualize both confidence intervals in Figure \ref{fig:bootstrap-distribution-mythbusters-CI}, with the percentile-method interval marked with black lines and the standard-error-method marked with grey lines. Observe that they are both similar to each other.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/bootstrap-distribution-mythbusters-CI-1} 

}

\caption{Two 95\% confidence intervals: percentile method (black) and standard error method (grey).}\label{fig:bootstrap-distribution-mythbusters-CI}
\end{figure}

\hypertarget{interpreting-the-confidence-interval}{%
\subsection{Interpreting the confidence interval}\label{interpreting-the-confidence-interval}}

Given that both confidence intervals are quite similar, let's focus our interpretation to only the percentile-method confidence interval of (-0.238, 0.302). Recall from Subsection \ref{shorthand} that the precise statistical interpretation of a 95\% confidence interval is: if this construction procedure is repeated 100 times, then we expect about 95 of the confidence intervals to capture the true value of \(p_{seed} - p_{control}\). In other words, if we gathered 100 samples of \(n\) = 50 participants from a similar pool of people and constructed 100 confidence intervals each based on each of the 100 samples, about 95 of them will contain the true value of \(p_{seed} - p_{control}\) while about five won't. Given that this is a little long winded, we use the shorthand interpretation: we're 95\% ``confident'' that the true difference in proportions \(p_{seed} - p_{control}\) is between (-0.238, 0.302).

There is one value of particular interest that this 95\% confidence interval contains: zero. If \(p_{seed} - p_{control}\) were equal to 0, then there would be no difference in proportion yawning between the two groups. This would suggest that there is no associated effect of being exposed to a yawning recruiter on whether you yawn yourself.

In our case, since the 95\% confidence interval includes 0, we cannot conclusively say if either proportion is larger. Of our 1000 bootstrap resamples with replacement, sometimes \(\widehat{p}_{seed}\) was higher and thus those exposed to yawning yawned themselves more often. At other times, the reverse happened.

Say, on the other hand, the 95\% confidence interval was entirely above zero. This would suggest that \(p_{seed} - p_{control} > 0\), or, in other words \(p_{seed} > p_{control}\), and thus we'd have evidence suggesting those exposed to yawning do yawn more often.

\hypertarget{ci-conclusion}{%
\section{Conclusion}\label{ci-conclusion}}

\hypertarget{bootstrap-vs-sampling}{%
\subsection{Comparing bootstrap and sampling distributions}\label{bootstrap-vs-sampling}}

Let's talk more about the relationship between \emph{sampling distributions} and \emph{bootstrap distributions}.\index{bootstrap!distribution}\index{sampling distributions}

Recall back in Subsection \ref{shovel-1000-times}, we took 1000 virtual samples from the \texttt{bowl} using a virtual shovel, computed 1000 values of the sample proportion red \(\widehat{p}\), then visualized their distribution in a histogram. Recall that this distribution is called the \emph{sampling distribution of} \(\widehat{p}\). Furthermore, the standard deviation of the sampling distribution has a special name: the \emph{standard error}.

We also mentioned that this sampling activity does not reflect how sampling is done in real life. Rather, it was an \emph{idealized version} of sampling so that we could study the effects of sampling variation on estimates, like the proportion of the shovel's balls that are red. In real life, however, one would take a single sample that's as large as possible, much like in the Obama poll we saw in Section \ref{sampling-case-study}. But how can we get a sense of the effect of sampling variation on estimates if we only have one sample and thus only one estimate? Don't we need many samples and hence many estimates?

The workaround to having a \emph{single} sample was to perform \emph{bootstrap resampling with replacement} from the single sample. We did this in the resampling activity in Section \ref{resampling-tactile} where we focused on the mean year of minting of pennies. We used pieces of paper representing the original sample of 50 pennies from the bank and resampled them with replacement from a hat. We had 35 of our friends perform this activity and visualized the resulting 35 sample means \(\overline{x}\) in a histogram in Figure \ref{fig:tactile-resampling-6}.

This distribution was called the \emph{bootstrap distribution} of \(\overline{x}\). We stated at the time that the bootstrap distribution is an \emph{approximation} to the sampling distribution of \(\overline{x}\) in the sense that both distributions will have a similar shape and similar spread. \index{bootstrap!distribution!approximation of sampling distribution} Thus the \emph{standard error} of the bootstrap distribution can be used as an approximation to the \emph{standard error} of the sampling distribution.

Let's show you that this is the case by now comparing these two types of distributions. Specifically, we'll compare

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  the sampling distribution of \(\widehat{p}\) based on 1000 virtual samples from the \texttt{bowl} from Subsection \ref{shovel-1000-times} to
\item
  the bootstrap distribution of \(\widehat{p}\) based on 1000 virtual resamples with replacement from Ilyas and Yohan's single sample \texttt{bowl\_sample\_1} from Subsection \ref{ilyas-yohan}.
\end{enumerate}

\hypertarget{sampling-distribution}{%
\subsubsection*{Sampling distribution}\label{sampling-distribution}}


Here is the code you saw in Subsection \ref{shovel-1000-times} to construct the sampling distribution of \(\widehat{p}\) shown again in Figure \ref{fig:sampling-distribution-part-deux}, with some changes to incorporate the statistical terminology relating to sampling from Subsection \ref{terminology-and-notation}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Take 1000 virtual samples of size 50 from the bowl:}
\NormalTok{virtual\_samples }\OtherTok{\textless{}{-}}\NormalTok{ bowl }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rep\_sample\_n}\NormalTok{(}\AttributeTok{size =} \DecValTok{50}\NormalTok{, }\AttributeTok{reps =} \DecValTok{1000}\NormalTok{)}
\CommentTok{\# Compute the sampling distribution of 1000 values of p{-}hat}
\NormalTok{sampling\_distribution }\OtherTok{\textless{}{-}}\NormalTok{ virtual\_samples }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(replicate) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{red =} \FunctionTok{sum}\NormalTok{(color }\SpecialCharTok{==} \StringTok{"red"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{prop\_red =}\NormalTok{ red }\SpecialCharTok{/} \DecValTok{50}\NormalTok{)}
\CommentTok{\# Visualize sampling distribution of p{-}hat}
\FunctionTok{ggplot}\NormalTok{(sampling\_distribution, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ prop\_red)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \FloatTok{0.05}\NormalTok{, }\AttributeTok{boundary =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{color =} \StringTok{"white"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Proportion of 50 balls that were red"}\NormalTok{, }
       \AttributeTok{title =} \StringTok{"Sampling distribution"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/sampling-distribution-part-deux-1} 

}

\caption{Previously seen sampling distribution of sample proportion red for $n = 1000$.}\label{fig:sampling-distribution-part-deux}
\end{figure}

An important thing to keep in mind is the default value for \texttt{replace} is \texttt{FALSE} when using \texttt{rep\_sample\_n()}. This is because when sampling 50 balls with a shovel, we are extracting 50 balls one-by-one \emph{without} replacing them. This is in contrast to bootstrap resampling \emph{with} replacement, where we resample a ball and put it back, and repeat this process 50 times.

Let's quantify the variability in this sampling distribution by calculating the standard deviation of the \texttt{prop\_red} variable representing 1000 values of the sample proportion \(\widehat{p}\). Remember that the standard deviation of the sampling distribution is the \emph{standard error}, frequently denoted as \texttt{se}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sampling\_distribution }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summarize}\NormalTok{(}\AttributeTok{se =} \FunctionTok{sd}\NormalTok{(prop\_red))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 1
         se
      <dbl>
1 0.0673987
\end{verbatim}

\hypertarget{bootstrap-distribution}{%
\subsubsection*{Bootstrap distribution}\label{bootstrap-distribution}}


Here is the code you previously saw in Subsection \ref{ilyas-yohan} to construct the bootstrap distribution of \(\widehat{p}\) based on Ilyas and Yohan's original sample of 50 balls saved in \texttt{bowl\_sample\_1}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bootstrap\_distribution }\OtherTok{\textless{}{-}}\NormalTok{ bowl\_sample\_1 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{response =}\NormalTok{ color, }\AttributeTok{success =} \StringTok{"red"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{, }\AttributeTok{type =} \StringTok{"bootstrap"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"prop"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/bootstrap-distribution-part-deux-1} 

}

\caption{Bootstrap distribution of proportion red for $n = 1000$.}\label{fig:bootstrap-distribution-part-deux}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bootstrap\_distribution }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summarize}\NormalTok{(}\AttributeTok{se =} \FunctionTok{sd}\NormalTok{(stat))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 1
         se
      <dbl>
1 0.0712212
\end{verbatim}

\hypertarget{comparison}{%
\subsubsection*{Comparison}\label{comparison}}


Now that we have computed both the sampling distribution and the bootstrap distributions, let's compare them side-by-side in Figure \ref{fig:side-by-side}. We'll make both histograms have matching scales on the x- and y-axes to make them more comparable. Furthermore, we'll add:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  To the sampling distribution on the top: a solid line denoting the proportion of the bowl's balls that are red \(p\) = 0.375.
\item
  To the bootstrap distribution on the bottom: a dashed line at the sample proportion \(\widehat{p}\) = 21/50 = 0.42 = 42\% that Ilyas and Yohan observed.
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/side-by-side-1} 

}

\caption{Comparing the sampling and bootstrap distributions of $\widehat{p}$.}\label{fig:side-by-side}
\end{figure}

There is a lot going on in Figure \ref{fig:side-by-side}, so let's break down all the comparisons slowly. First, observe how the sampling distribution on top is centered at \(p\) = 0.375. This is because the sampling is done at random and in an unbiased fashion. So the estimates \(\widehat{p}\) are centered at the true value of \(p\).

However, this is not the case with the following bootstrap distribution. The bootstrap distribution is centered at 0.42, which is the proportion red of Ilyas and Yohan's 50 sampled balls. This is because we are resampling from the same sample over and over again. Since the bootstrap distribution is centered at the original sample's proportion, it doesn't necessarily provide a better estimate of \(p\) = 0.375. This leads us to our first lesson about bootstrapping:

\begin{quote}
The bootstrap distribution will likely not have the same center as the sampling distribution. In other words, bootstrapping cannot improve the quality of an estimate.
\end{quote}

Second, let's now compare the spread of the two distributions: they are somewhat similar. In the previous code, we computed the standard deviations of both distributions as well. Recall that such standard deviations have a special name: \emph{standard errors}. Let's compare them in Table \ref{tab:comparing-se}.

\begin{table}[!h]

\caption{\label{tab:comparing-se}Comparing standard errors}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{lr}
\toprule
Distribution type & Standard error\\
\midrule
Sampling distribution & 0.067\\
Bootstrap distribution & 0.071\\
\bottomrule
\end{tabular}
\end{table}

Notice that the bootstrap distribution's standard error is a rather good \emph{approximation} to the sampling distribution's standard error. This leads us to our second lesson about bootstrapping:

\begin{quote}
Even if the bootstrap distribution might not have the same center as the sampling distribution, it will likely have very similar shape and spread. In other words, bootstrapping will give you a good estimate of the \emph{standard error}.
\end{quote}

Thus, using the fact that the bootstrap distribution and sampling distributions have similar spreads, we can build confidence intervals using bootstrapping as we've done all throughout this chapter!

\hypertarget{theory-ci}{%
\subsection{Theory-based confidence intervals}\label{theory-ci}}

So far in this chapter, we've constructed confidence intervals using two methods: the percentile method and the standard error method. Recall also from Subsection \ref{se-method} that we can only use the standard-error method if the bootstrap distribution is bell-shaped (i.e., normally distributed).

In a similar vein, if the sampling distribution is normally shaped, there is another method for constructing confidence intervals that does not involve using your computer. You can use a \emph{theory-based method} involving mathematical formulas!

The formula uses the rule of thumb we saw in Appendix \ref{appendix-normal-curve} that 95\% of values in a normal distribution are within \(\pm 1.96\) standard deviations of the mean. In the case of sampling and bootstrap distributions, recall that the standard deviation has a special name: the \emph{standard error}.

\hypertarget{theory-based-method-for-computing-standard-errors}{%
\subsubsection*{Theory-based method for computing standard errors}\label{theory-based-method-for-computing-standard-errors}}


There exists in many cases a formula that approximates the standard error! In the case of our \texttt{bowl} where we used the sample proportion red \(\widehat{p}\) to estimate the proportion of the bowl's balls that are red, the formula that approximates the standard error is:

\[\text{SE}_{\widehat{p}} \approx \sqrt{\frac{\widehat{p}(1-\widehat{p})}{n}}\]

For example, recall from \texttt{bowl\_sample\_1} that Yohan and Ilyas sampled \(n = 50\) balls and observed a sample proportion \(\widehat{p}\) of 21/50 = 0.42. So, using the formula, an approximation of the standard error of \(\widehat{p}\) is

\[\text{SE}_{\widehat{p}} \approx \sqrt{\frac{0.42(1-0.42)}{50}} = \sqrt{0.004872} = 0.0698 \approx 0.070\]

The key observation to make here is that there is an \(n\) in the denominator. So as the sample size \(n\) increases, the standard error decreases. We've demonstrated this fact using our virtual shovels in Subsection \ref{moral-of-the-story}. If you don't recall this demonstration, we highly recommend you go back and read that subsection.

Let's compare this theory-based standard error to the standard error of the sampling and bootstrap distributions you computed previously in Subsection \ref{bootstrap-vs-sampling} in Table \ref{tab:comparing-se-2}. Notice how they are all similar!

\begin{table}[!h]

\caption{\label{tab:comparing-se-2}Comparing standard errors}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{lr}
\toprule
Distribution type & Standard error\\
\midrule
Sampling distribution & 0.067\\
Bootstrap distribution & 0.071\\
Formula approximation & 0.070\\
\bottomrule
\end{tabular}
\end{table}

Going back to Yohan and Ilyas' sample proportion of \(\widehat{p}\) of 21/50 = 0.42, say this were based on a sample of size \(n\) = 100 instead of 50. Then the standard error would be:

\[\text{SE}_{\widehat{p}} \approx \sqrt{\frac{0.42(1-0.42)}{100}} = \sqrt{0.002436} = 0.0494\]

Observe that the standard error has gone down from 0.0698 to 0.0494. In other words, the ``typical'' error of our estimates using \(n\) = 100 will go down and hence be more \emph{precise}. Recall that we illustrated the difference between accuracy and precision of estimates in Figure \ref{fig:accuracy-vs-precision}.

Why is this formula true? Unfortunately, we don't have the tools at this point to prove this; you'll need to take a more advanced course in probability and statistics. (It is related to the concepts of Bernoulli and Binomial Distributions. You can read more about its derivation \href{http://onlinestatbook.com/2/sampling_distributions/samp_dist_p.html}{here} if you like.)

\hypertarget{theory-based-method-for-constructing-confidence-intervals}{%
\subsubsection*{Theory-based method for constructing confidence intervals}\label{theory-based-method-for-constructing-confidence-intervals}}


Using these theory-based standard errors, let's present a theory-based method for constructing 95\% confidence intervals that does not involve using a computer, but rather mathematical formulas. Note that this theory-based method only holds if the sampling distribution is normally shaped, so that we can use the 95\% rule of thumb about normal distributions discussed in Appendix \ref{appendix-normal-curve}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Collect a single representative sample of size \(n\) that's as large as possible.
\item
  Compute the \emph{point estimate}: the \emph{sample proportion} \(\widehat{p}\). Think of this as the center of your ``net.''
\item
  Compute the approximation to the standard error
\end{enumerate}

\[\text{SE}_{\widehat{p}} \approx \sqrt{\frac{\widehat{p}(1-\widehat{p})}{n}}\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Compute a quantity known as the \emph{margin of error} (more on this later after we list the five steps):
\end{enumerate}

\[\text{MoE}_{\widehat{p}} = 1.96 \cdot \text{SE}_{\widehat{p}} =  1.96 \cdot \sqrt{\frac{\widehat{p}(1-\widehat{p})}{n}}\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Compute both endpoints of the confidence interval.

  \begin{itemize}
  \item
    The lower end-point. Think of this as the left end-point of the net:
    \[\widehat{p} - \text{MoE}_{\widehat{p}} = \widehat{p} - 1.96 \cdot \text{SE}_{\widehat{p}} = \widehat{p} - 1.96 \cdot \sqrt{\frac{\widehat{p}(1-\widehat{p})}{n}}\]
  \item
    The upper endpoint. Think of this as the right end-point of the net:
    \[\widehat{p} + \text{MoE}_{\widehat{p}} = \widehat{p} + 1.96 \cdot \text{SE}_{\widehat{p}} = \widehat{p} + 1.96 \cdot \sqrt{\frac{\widehat{p}(1-\widehat{p})}{n}}\]
  \item
    Alternatively, you can succinctly summarize a 95\% confidence interval for \(p\) using the \(\pm\) symbol:
  \end{itemize}

  \[\widehat{p} \pm \text{MoE}_{\widehat{p}} = \widehat{p} \pm (1.96 \cdot \text{SE}_{\widehat{p}}) = \widehat{p} \pm \left( 1.96 \cdot \sqrt{\frac{\widehat{p}(1-\widehat{p})}{n}} \right)\]
\end{enumerate}

So going back to Yohan and Ilyas' sample of \(n = 50\) balls that had 21 red balls, the 95\% confidence interval for \(p\) is

\[
\begin{aligned}
0.41 \pm 1.96 \cdot 0.0698 &= 0.41 \, \pm \, 0.137 \\ &= (0.41 - 0.137, \, 0.41 + 0.137) \\ &= (0.273, \, 0.547).
\end{aligned}
\]

Yohan and Ilyas are 95\% ``confident'' that the true proportion red of the bowl's balls is between 28.3\% and 55.7\%. Given that the true population proportion \(p\) was 0.375, in this case they successfully captured the fish.

In Step 4, we defined a statistical quantity known as the \emph{margin of error}.\index{margin of error} You can think of this quantity as how much the net extends to the left and to the right of the center of our net. The 1.96 multiplier is rooted in the 95\% rule of thumb we introduced earlier and the fact that we want the confidence level to be 95\%. The value of the margin of error entirely determines the width of the confidence interval. Recall from Subsection \ref{ci-width} that confidence interval widths are determined by an interplay of the confidence level, the sample size \(n\), and the standard error.

Let's revisit the poll of President Obama's approval rating among young Americans aged 18-29 which we introduced in Section \ref{sampling-case-study}. Pollsters found that based on a representative sample of \(n\) = 2089 young Americans, \(\widehat{p}\) = 0.41 = 41\% supported President Obama.

If you look towards the end of the article, it also states: ``The poll's margin of error was plus or minus 2.1 percentage points.'' This is precisely the \(\text{MoE}\):

\[
\begin{aligned}
\text{MoE} &= 1.96 \cdot \text{SE} =  1.96 \cdot \sqrt{\frac{\widehat{p}(1-\widehat{p})}{n}} = 1.96 \cdot \sqrt{\frac{0.41(1-0.41)}{2089}} \\
&= 1.96 \cdot 0.0108 = 0.021 = 2.1\%
\end{aligned}
\]

Their poll results are based on a confidence level of 95\% and the resulting 95\% confidence interval for the proportion of all young Americans who support Obama is:

\[\widehat{p} \pm \text{MoE} = 0.41 \pm 0.021 = (0.389, \, 0.431) = (38.9\%, \, 43.1\%).\]

\hypertarget{confidence-intervals-based-on-33-tactile-samples}{%
\subsubsection*{Confidence intervals based on 33 tactile samples}\label{confidence-intervals-based-on-33-tactile-samples}}


Let's revisit our 33 friends' samples from the \texttt{bowl} from Subsection \ref{student-shovels}. We'll use their 33 samples to construct 33 theory-based 95\% confidence intervals for \(p\). Recall this data was saved in the \texttt{tactile\_prop\_red} data frame included in the \texttt{moderndive} package:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{rename()} the variable \texttt{prop\_red} to \texttt{p\_hat}, the statistical name of the sample proportion \(\widehat{p}\).
\item
  \texttt{mutate()} a new variable \texttt{n} making explicit the sample size of 50.
\item
  \texttt{mutate()} other new variables computing:

  \begin{itemize}
  \tightlist
  \item
    The standard error \texttt{SE} for \(\widehat{p}\) using the previous formula.
  \item
    The margin of error \texttt{MoE} by multiplying the \texttt{SE} by 1.96
  \item
    The left endpoint of the confidence interval \texttt{lower\_ci}
  \item
    The right endpoint of the confidence interval \texttt{upper\_ci}
  \end{itemize}
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{conf\_ints }\OtherTok{\textless{}{-}}\NormalTok{ tactile\_prop\_red }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{p\_hat =}\NormalTok{ prop\_red) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{n =} \DecValTok{50}\NormalTok{,}
    \AttributeTok{SE =} \FunctionTok{sqrt}\NormalTok{(p\_hat }\SpecialCharTok{*}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ p\_hat) }\SpecialCharTok{/}\NormalTok{ n),}
    \AttributeTok{MoE =} \FloatTok{1.96} \SpecialCharTok{*}\NormalTok{ SE,}
    \AttributeTok{lower\_ci =}\NormalTok{ p\_hat }\SpecialCharTok{{-}}\NormalTok{ MoE,}
    \AttributeTok{upper\_ci =}\NormalTok{ p\_hat }\SpecialCharTok{+}\NormalTok{ MoE}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

In Figure \ref{fig:tactile-conf-int}, let's plot the 33 confidence intervals for \(p\) saved in \texttt{conf\_ints} along with a vertical line at \(p\) = 0.375 indicating the true proportion of the \texttt{bowl}'s balls that are red. Furthermore, let's mark the sample proportions \(\widehat{p}\) with dots since they represent the centers of these confidence intervals.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/tactile-conf-int-1} 

}

\caption{33 confidence intervals at the 95\% level based on 33 tactile samples of size $n = 50$.}\label{fig:tactile-conf-int}
\end{figure}

Observe that 31 of the 33 confidence intervals ``captured'' the true value of \(p\), for a success rate of 31 / 33 = 93.94\%. While this is not quite 95\%, recall that we \emph{expect} about 95\% of such confidence intervals to capture \(p\). The actual observed success rate will vary slightly.

Theory-based methods like this have largely been used in the past because we didn't have the computing power to perform simulation-based methods such as bootstrapping. They are still commonly used, however, and if the sampling distribution is normally distributed, we have access to an alternative method for constructing confidence intervals as well as performing hypothesis tests as we will see in Chapter \ref{hypothesis-testing}.

The kind of computer-based statistical inference we've seen so far has a particular name in the field of statistics: \emph{simulation-based inference}. This is because we are performing statistical inference using computer simulations.\index{simulation-based inference} In our opinion, two large benefits of simulation-based methods over theory-based methods are that (1) they are easier for people new to statistical inference to understand and (2) they also work in situations where theory-based methods and mathematical formulas don't exist.

\hypertarget{additional-resources-6}{%
\subsection{Additional resources}\label{additional-resources-6}}

Solutions to all \emph{Learning checks} can be found online in \href{https://moderndive.com/D-appendixD.html}{Appendix D}.

An R script file of all R code used in this chapter is available at \url{https://www.moderndive.com/scripts/08-confidence-intervals.R}.

If you want more examples of the \texttt{infer} workflow to construct confidence intervals, we suggest you check out the \texttt{infer} package homepage, in particular, a series of example analyses available at \url{https://infer.netlify.app/articles/}.

\hypertarget{whats-to-come-7}{%
\subsection{What's to come?}\label{whats-to-come-7}}

Now that we've equipped ourselves with confidence intervals, in Chapter \ref{hypothesis-testing} we'll cover the other common tool for statistical inference: hypothesis testing. Just like confidence intervals, hypothesis tests are used to infer about a population using a sample. However, we'll see that the framework for making such inferences is slightly different.

\hypertarget{hypothesis-testing}{%
\chapter{Hypothesis Testing}\label{hypothesis-testing}}

Now that we've studied confidence intervals in Chapter \ref{confidence-intervals}, let's study another commonly used method for statistical inference: hypothesis testing. Hypothesis tests allow us to take a sample of data from a population and infer about the plausibility of competing hypotheses. For example, in the upcoming ``promotions'' activity in Section \ref{ht-activity}, you'll study the data collected from a psychology study in the 1970s to investigate whether gender-based discrimination in promotion rates existed in the banking industry at the time of the study.

The good news is we've already covered many of the necessary concepts to understand hypothesis testing in Chapters \ref{sampling} and \ref{confidence-intervals}. We will expand further on these ideas here and also provide a general framework for understanding hypothesis tests. By understanding this general framework, you'll be able to adapt it to many different scenarios.

The same can be said for confidence intervals. There was one general framework that applies to \emph{all} confidence intervals and the \texttt{infer} package was designed around this framework. While the specifics may change slightly for different types of confidence intervals, the general framework stays the same.

We believe that this approach is much better for long-term learning than focusing on specific details for specific confidence intervals using theory-based approaches. As you'll now see, we prefer this general framework for hypothesis tests as well.

\hypertarget{nhst-packages}{%
\subsection*{Needed packages}\label{nhst-packages}}


Let's load all the packages needed for this chapter (this assumes you've already installed them). Recall from our discussion in Section \ref{tidyverse-package} that loading the \texttt{tidyverse} package by running \texttt{library(tidyverse)} loads the following commonly used data science packages all at once:

\begin{itemize}
\tightlist
\item
  \texttt{ggplot2} for data visualization
\item
  \texttt{dplyr} for data wrangling
\item
  \texttt{tidyr} for converting data to ``tidy'' format
\item
  \texttt{readr} for importing spreadsheet data into R
\item
  As well as the more advanced \texttt{purrr}, \texttt{tibble}, \texttt{stringr}, and \texttt{forcats} packages
\end{itemize}

If needed, read Section \ref{packages} for information on how to install and load R packages.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(infer)}
\FunctionTok{library}\NormalTok{(moderndive)}
\FunctionTok{library}\NormalTok{(nycflights13)}
\FunctionTok{library}\NormalTok{(ggplot2movies)}
\end{Highlighting}
\end{Shaded}

\hypertarget{ht-activity}{%
\section{Promotions activity}\label{ht-activity}}

Let's start with an activity studying the effect of gender on promotions at a bank.

\hypertarget{does-gender-affect-promotions-at-a-bank}{%
\subsection{Does gender affect promotions at a bank?}\label{does-gender-affect-promotions-at-a-bank}}

Say you are working at a bank in the 1970s and you are submitting your résumé to apply for a promotion. Will your gender affect your chances of getting promoted? To answer this question, we'll focus on data from a study published in the \emph{Journal of Applied Psychology} in 1974. This data is also used in the \href{https://www.openintro.org/}{\emph{OpenIntro}} series of statistics textbooks.

To begin the study, 48 bank supervisors were asked to assume the role of a hypothetical director of a bank with multiple branches. Every one of the bank supervisors was given a résumé and asked whether or not the candidate on the résumé was fit to be promoted to a new position in one of their branches.

However, each of these 48 résumés were identical in all respects except one: the name of the applicant at the top of the résumé. Of the supervisors, 24 were randomly given résumés with stereotypically ``male'' names, while 24 of the supervisors were randomly given résumés with stereotypically ``female'' names. Since only (binary) gender varied from résumé to résumé, researchers could isolate the effect of this variable in promotion rates.

While many people today (including us, the authors) disagree with such binary views of gender, it is important to remember that this study was conducted at a time where more nuanced views of gender were not as prevalent. Despite this imperfection, we decided to still use this example as we feel it presents ideas still relevant today about how we could study discrimination in the workplace.

The \texttt{moderndive} package contains the data on the 48 applicants in the \texttt{promotions} data frame. Let's explore this data by looking at six randomly selected rows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{promotions }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{sample\_n}\NormalTok{(}\AttributeTok{size =} \DecValTok{6}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(id)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 3
     id decision gender
  <int> <fct>    <fct> 
1    11 promoted male  
2    26 promoted female
3    28 promoted female
4    36 not      male  
5    37 not      male  
6    46 not      female
\end{verbatim}

The variable \texttt{id} acts as an identification variable for all 48 rows, the \texttt{decision} variable indicates whether the applicant was selected for promotion or not, while the \texttt{gender} variable indicates the gender of the name used on the résumé. Recall that this data does not pertain to 24 actual men and 24 actual women, but rather 48 identical résumés of which 24 were assigned stereotypically ``male'' names and 24 were assigned stereotypically ``female'' names.

Let's perform an exploratory data analysis of the relationship between the two categorical variables \texttt{decision} and \texttt{gender}. Recall that we saw in Subsection \ref{two-categ-barplot} that one way we can visualize such a relationship is by using a stacked barplot.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(promotions, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gender, }\AttributeTok{fill =}\NormalTok{ decision)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Gender of name on résumé"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/promotions-barplot-1} 

}

\caption{Barplot relating gender to promotion decision.}\label{fig:promotions-barplot}
\end{figure}

Observe in Figure \ref{fig:promotions-barplot} that it appears that résumés with female names were much less likely to be accepted for promotion. Let's quantify these promotion rates by computing the proportion of résumés accepted for promotion for each group using the \texttt{dplyr} package for data wrangling. Note the use of the \texttt{tally()} function here which is a shortcut for \texttt{summarize(n\ =\ n())} to get counts.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{promotions }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(gender, decision) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{tally}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 3
# Groups:   gender [2]
  gender decision     n
  <fct>  <fct>    <int>
1 male   not          3
2 male   promoted    21
3 female not         10
4 female promoted    14
\end{verbatim}

So of the 24 résumés with male names, 21 were selected for promotion, for a proportion of 21/24 = 0.875 = 87.5\%. On the other hand, of the 24 résumés with female names, 14 were selected for promotion, for a proportion of 14/24 = 0.583 = 58.3\%. Comparing these two rates of promotion, it appears that résumés with male names were selected for promotion at a rate 0.875 - 0.583 = 0.292 = 29.2\% higher than résumés with female names. This is suggestive of an advantage for résumés with a male name on it.

The question is, however, does this provide \emph{conclusive} evidence that there is gender discrimination in promotions at banks? Could a difference in promotion rates of 29.2\% still occur by chance, even in a hypothetical world where no gender-based discrimination existed? In other words, what is the role of \emph{sampling variation} in this hypothesized world? To answer this question, we'll again rely on a computer to run \emph{simulations}.

\hypertarget{shuffling-once}{%
\subsection{Shuffling once}\label{shuffling-once}}

First, try to imagine a hypothetical universe where no gender discrimination in promotions existed. In such a hypothetical universe, the gender of an applicant would have no bearing on their chances of promotion. Bringing things back to our \texttt{promotions} data frame, the \texttt{gender} variable would thus be an irrelevant label. If these \texttt{gender} labels were irrelevant, then we could randomly reassign them by ``shuffling'' them to no consequence!

To illustrate this idea, let's narrow our focus to 6 arbitrarily chosen résumés of the 48 in Table \ref{tab:compare-six}. The \texttt{decision} column shows that 3 résumés resulted in promotion while 3 didn't. The \texttt{gender} column shows what the original gender of the résumé name was.

However, in our hypothesized universe of no gender discrimination, gender is irrelevant and thus it is of no consequence to randomly ``shuffle'' the values of \texttt{gender}. The \texttt{shuffled\_gender} column shows one such possible random shuffling. Observe in the fourth column how the number of male and female names remains the same at 3 each, but they are now listed in a different order.

\begingroup\fontsize{10}{12}\selectfont

\begin{longtable}[t]{rlll}
\caption{\label{tab:compare-six}One example of shuffling gender variable}\\
\toprule
résumé number & decision & gender & shuffled gender\\
\midrule
\endfirsthead
\caption[]{\label{tab:compare-six}One example of shuffling gender variable \textit{(continued)}}\\
\toprule
résumé number & decision & gender & shuffled gender\\
\midrule
\endhead

\endfoot
\bottomrule
\endlastfoot
1 & not & male & male\\
2 & not & female & male\\
3 & not & female & female\\
4 & promoted & male & female\\
5 & promoted & male & female\\
6 & promoted & female & male\\*
\end{longtable}
\endgroup{}

Again, such random shuffling of the gender label only makes sense in our hypothesized universe of no gender discrimination. How could we extend this shuffling of the gender variable to all 48 résumés by hand? One way would be by using standard deck of 52 playing cards, which we display in Figure \ref{fig:deck-of-cards}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/shutterstock/shutterstock_670789453} 

}

\caption{Standard deck of 52 playing cards.}\label{fig:deck-of-cards}
\end{figure}

Since half the cards are red (diamonds and hearts) and the other half are black (spades and clubs), by removing two red cards and two black cards, we would end up with 24 red cards and 24 black cards. After shuffling these 48 cards as seen in Figure \ref{fig:shuffling}, we can flip the cards over one-by-one, assigning ``male'' for each red card and ``female'' for each black card.

\begin{figure}

{\centering \includegraphics[width=1\linewidth,height=1\textheight]{images/shutterstock/shutterstock_128283971} 

}

\caption{Shuffling a deck of cards.}\label{fig:shuffling}
\end{figure}

We've saved one such shuffling in the \texttt{promotions\_shuffled} data frame of the \texttt{moderndive} package. If you compare the original \texttt{promotions} and the shuffled \texttt{promotions\_shuffled} data frames, you'll see that while the \texttt{decision} variable is identical, the \texttt{gender} variable has changed.

Let's repeat the same exploratory data analysis we did for the original \texttt{promotions} data on our \texttt{promotions\_shuffled} data frame. Let's create a barplot visualizing the relationship between \texttt{decision} and the new shuffled \texttt{gender} variable and compare this to the original unshuffled version in Figure \ref{fig:promotions-barplot-permuted}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(promotions\_shuffled, }
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ gender, }\AttributeTok{fill =}\NormalTok{ decision)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Gender of résumé name"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/promotions-barplot-permuted-1} 

}

\caption{Barplots of relationship of promotion with gender (left) and shuffled gender (right).}\label{fig:promotions-barplot-permuted}
\end{figure}

It appears the difference in ``male names'' versus ``female names'' promotion rates is now different. Compared to the original data in the left barplot, the new ``shuffled'' data in the right barplot has promotion rates that are much more similar.

Let's also compute the proportion of résumés accepted for promotion for each group:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{promotions\_shuffled }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(gender, decision) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{tally}\NormalTok{() }\CommentTok{\# Same as summarize(n = n())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 3
# Groups:   gender [2]
  gender decision     n
  <fct>  <fct>    <int>
1 male   not          6
2 male   promoted    18
3 female not          7
4 female promoted    17
\end{verbatim}

So in this hypothetical universe of no discrimination, \(18/24 = 0.75 = 75\%\) of ``male'' résumés were selected for promotion. On the other hand, \(17/24 = 0.708 = 70.8\%\) of ``female'' résumés were selected for promotion.

Let's next compare these two values. It appears that résumés with stereotypically male names were selected for promotion at a rate that was \(0.75 - 0.708 = 0.042 = 4.2\%\) different than résumés with stereotypically female names.

Observe how this difference in rates is not the same as the difference in rates of 0.292 = 29.2\% we originally observed. This is once again due to \emph{sampling variation}. How can we better understand the effect of this sampling variation? By repeating this shuffling several times!

\hypertarget{shuffling-16-times}{%
\subsection{Shuffling 16 times}\label{shuffling-16-times}}

We recruited 16 groups of our friends to repeat this shuffling exercise. They recorded these values in a \href{https://docs.google.com/spreadsheets/d/1Q-ENy3o5IrpJshJ7gn3hJ5A0TOWV2AZrKNHMsshQtiE/}{shared spreadsheet}; we display a snapshot of the first 10 rows and 5 columns in Figure \ref{fig:tactile-shuffling}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{images/sampling/promotions/shared_spreadsheet} 

}

\caption{Snapshot of shared spreadsheet of shuffling results (m for male, f for female).}\label{fig:tactile-shuffling}
\end{figure}

For each of these 16 columns of \emph{shuffles}, we computed the difference in promotion rates, and in Figure \ref{fig:null-distribution-1} we display their distribution in a histogram. We also mark the observed difference in promotion rate that occurred in real life of 0.292 = 29.2\% with a dark line.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/null-distribution-1-1} 

}

\caption{Distribution of shuffled differences in promotions.}\label{fig:null-distribution-1}
\end{figure}

Before we discuss the distribution of the histogram, we emphasize the key thing to remember: this histogram represents differences in promotion rates that one would observe in our \emph{hypothesized universe} of no gender discrimination.

Observe first that the histogram is roughly centered at 0. Saying that the difference in promotion rates is 0 is equivalent to saying that both genders had the same promotion rate. In other words, the center of these 16 values is consistent with what we would expect in our hypothesized universe of no gender discrimination.

However, while the values are centered at 0, there is variation about 0. This is because even in a hypothesized universe of no gender discrimination, you will still likely observe small differences in promotion rates because of chance \emph{sampling variation}. Looking at the histogram in Figure \ref{fig:null-distribution-1}, such differences could even be as extreme as -0.292 or 0.208.

Turning our attention to what we observed in real life: the difference of 0.292 = 29.2\% is marked with a vertical dark line. Ask yourself: in a hypothesized world of no gender discrimination, how likely would it be that we observe this difference? While opinions here may differ, in our opinion not often! Now ask yourself: what do these results say about our hypothesized universe of no gender discrimination?

\hypertarget{what-did-we-just-do-2}{%
\subsection{What did we just do?}\label{what-did-we-just-do-2}}

What we just demonstrated in this activity is the statistical procedure known as \emph{hypothesis testing} using a \emph{permutation test}. The term ``permutation'' \index{permutation} is the mathematical term for ``shuffling'': taking a series of values and reordering them randomly, as you did with the playing cards.

In fact, permutations are another form of \emph{resampling}, like the bootstrap method you performed in Chapter \ref{confidence-intervals}. While the bootstrap method involves resampling \emph{with} replacement, permutation methods involve resampling \emph{without} replacement.

Think of our exercise involving the slips of paper representing pennies and the hat in Section \ref{resampling-tactile}: after sampling a penny, you put it back in the hat. Now think of our deck of cards. After drawing a card, you laid it out in front of you, recorded the color, and then you \emph{did not} put it back in the deck.

In our previous example, we tested the validity of the hypothesized universe of no gender discrimination. The evidence contained in our observed sample of 48 résumés was somewhat inconsistent with our hypothesized universe. Thus, we would be inclined to \emph{reject} this hypothesized universe and declare that the evidence suggests there is gender discrimination.

Recall our case study on whether yawning is contagious from Section \ref{case-study-two-prop-ci}. The previous example involves inference about an unknown difference of population proportions as well. This time, it will be \(p_{m} - p_{f}\), where \(p_{m}\) is the population proportion of résumés with male names being recommended for promotion and \(p_{f}\) is the equivalent for résumés with female names. Recall that this is one of the scenarios for inference we've seen so far in Table \ref{tab:table-diff-prop}.

\begin{table}[!h]

\caption{\label{tab:table-diff-prop}Scenarios of sampling for inference}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{>{\raggedleft\arraybackslash}p{0.5in}>{\raggedright\arraybackslash}p{0.7in}>{\raggedright\arraybackslash}p{1in}>{\raggedright\arraybackslash}p{1.1in}>{\raggedright\arraybackslash}p{1in}}
\toprule
Scenario & Population parameter & Notation & Point estimate & Symbol(s)\\
\midrule
1 & Population proportion & $p$ & Sample proportion & $\widehat{p}$\\
2 & Population mean & $\mu$ & Sample mean & $\overline{x}$ or $\widehat{\mu}$\\
3 & Difference in population proportions & $p_1 - p_2$ & Difference in sample proportions & $\widehat{p}_1 - \widehat{p}_2$\\
\bottomrule
\end{tabular}
\end{table}

So, based on our sample of \(n_m\) = 24 ``male'' applicants and \(n_w\) = 24 ``female'' applicants, the \emph{point estimate} for \(p_{m} - p_{f}\) is the \emph{difference in sample proportions} \(\widehat{p}_{m} -\widehat{p}_{f}\) = 0.875 - 0.583 = 0.292 = 29.2\%. This difference in favor of ``male'' résumés of 0.292 is greater than 0, suggesting discrimination in favor of men.

However, the question we asked ourselves was ``is this difference meaningfully greater than 0?''. In other words, is that difference indicative of true discrimination, or can we just attribute it to \emph{sampling variation}? Hypothesis testing allows us to make such distinctions.

\hypertarget{understanding-ht}{%
\section{Understanding hypothesis tests}\label{understanding-ht}}

Much like the terminology, notation, and definitions relating to sampling you saw in Section \ref{sampling-framework}, there are a lot of terminology, notation, and definitions related to hypothesis testing as well. Learning these may seem like a very daunting task at first. However, with practice, practice, and more practice, anyone can master them.

First, a \textbf{hypothesis} \index{hypothesis testing!hypothesis} is a statement about the value of an unknown population parameter. In our résumé activity, our population parameter of interest is the difference in population proportions \(p_{m} - p_{f}\). Hypothesis tests can involve any of the population parameters in Table \ref{tab:table-ch8} of the five inference scenarios we'll cover in this book and also more advanced types we won't cover here.

Second, a \textbf{hypothesis test} \index{hypothesis testing} consists of a test between two competing hypotheses: (1) a \textbf{null hypothesis} \(H_0\) (pronounced ``H-naught'') versus (2) an \textbf{alternative hypothesis} \(H_A\) (also denoted \(H_1\)).

Generally the null hypothesis \index{hypothesis testing!null hypothesis} is a claim that there is ``no effect'' or ``no difference of interest.'' In many cases, the null hypothesis represents the status quo or a situation that nothing interesting is happening. Furthermore, generally the alternative hypothesis \index{hypothesis testing!alternative hypothesis} is the claim the experimenter or researcher wants to establish or find evidence to support. It is viewed as a ``challenger'' hypothesis to the null hypothesis \(H_0\). In our résumé activity, an appropriate hypothesis test would be:

\[
\begin{aligned}
H_0 &: \text{men and women are promoted at the same rate}\\
\text{vs } H_A &: \text{men are promoted at a higher rate than women}
\end{aligned}
\]

Note some of the choices we have made. First, we set the null hypothesis \(H_0\) to be that there is no difference in promotion rate and the ``challenger'' alternative hypothesis \(H_A\) to be that there is a difference. While it would not be wrong in principle to reverse the two, it is a convention in statistical inference that the null hypothesis is set to reflect a ``null'' situation where ``nothing is going on.'' As we discussed earlier, in this case, \(H_0\) corresponds to there being no difference in promotion rates. Furthermore, we set \(H_A\) to be that men are promoted at a \emph{higher} rate, a subjective choice reflecting a prior suspicion we have that this is the case. We call such alternative hypotheses \index{hypothesis testing!one-sided alternative} \emph{one-sided alternatives}. If someone else however does not share such suspicions and only wants to investigate that there is a difference, whether higher or lower, they would set what is known as a \index{hypothesis testing!two-sided alternative} \emph{two-sided alternative}.

We can re-express the formulation of our hypothesis test using the mathematical notation for our population parameter of interest, the difference in population proportions \(p_{m} - p_{f}\):

\[
\begin{aligned}
H_0 &: p_{m} - p_{f} = 0\\
\text{vs } H_A&: p_{m} - p_{f} > 0
\end{aligned}
\]

Observe how the alternative hypothesis \(H_A\) is one-sided with \(p_{m} - p_{f} > 0\). Had we opted for a two-sided alternative, we would have set \(p_{m} - p_{f} \neq 0\). To keep things simple for now, we'll stick with the simpler one-sided alternative. We'll present an example of a two-sided alternative in Section \ref{ht-case-study}.

Third, a \textbf{test statistic} \index{hypothesis testing!test statistic} is a \emph{point estimate/sample statistic} formula used for hypothesis testing. Note that a sample statistic is merely a summary statistic based on a sample of observations. Recall we saw in Section \ref{summarize} that a summary statistic takes in many values and returns only one. Here, the samples would be the \(n_m\) = 24 résumés with male names and the \(n_f\) = 24 résumés with female names. Hence, the point estimate of interest is the difference in sample proportions \(\widehat{p}_{m} - \widehat{p}_{f}\).

Fourth, the \textbf{observed test statistic} \index{hypothesis testing!observed test statistic} is the value of the test statistic that we observed in real life. In our case, we computed this value using the data saved in the \texttt{promotions} data frame. It was the observed difference of \(\widehat{p}_{m} -\widehat{p}_{f} = 0.875 - 0.583 = 0.292 = 29.2\%\) in favor of résumés with male names.

Fifth, the \textbf{null distribution} \index{hypothesis testing!null distribution} is the sampling distribution of the test statistic \emph{assuming the null hypothesis \(H_0\) is true}. Ooof! That's a long one! Let's unpack it slowly. The key to understanding the null distribution is that the null hypothesis \(H_0\) is \emph{assumed} to be true. We're not saying that \(H_0\) is true at this point, we're only assuming it to be true for hypothesis testing purposes. In our case, this corresponds to our hypothesized universe of no gender discrimination in promotion rates. Assuming the null hypothesis \(H_0\), also stated as ``Under \(H_0\),'' how does the test statistic vary due to sampling variation? In our case, how will the difference in sample proportions \(\widehat{p}_{m} - \widehat{p}_{f}\) vary due to sampling under \(H_0\)? Recall from Subsection \ref{sampling-definitions} that distributions displaying how point estimates vary due to sampling variation are called \emph{sampling distributions}. The only additional thing to keep in mind about null distributions is that they are sampling distributions \emph{assuming the null hypothesis \(H_0\) is true}.

In our case, we previously visualized a null distribution in Figure \ref{fig:null-distribution-1}, which we re-display in Figure \ref{fig:null-distribution-2} using our new notation and terminology. It is the distribution of the 16 differences in sample proportions our friends computed \emph{assuming} a hypothetical universe of no gender discrimination. We also mark the value of the observed test statistic of 0.292 with a vertical line.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/null-distribution-2-1} 

}

\caption{Null distribution and observed test statistic.}\label{fig:null-distribution-2}
\end{figure}

Sixth, the \textbf{\(p\)-value} \index{hypothesis testing!p-value} is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic \emph{assuming the null hypothesis \(H_0\) is true}. Double ooof! Let's unpack this slowly as well. You can think of the \(p\)-value as a quantification of ``surprise'': assuming \(H_0\) is true, how surprised are we with what we observed? Or in our case, in our hypothesized universe of no gender discrimination, how surprised are we that we observed a difference in promotion rates of 0.292 from our collected samples assuming \(H_0\) is true? Very surprised? Somewhat surprised?

The \(p\)-value quantifies this probability, or in the case of our 16 differences in sample proportions in Figure \ref{fig:null-distribution-2}, what proportion had a more ``extreme'' result? Here, extreme is defined in terms of the alternative hypothesis \(H_A\) that ``male'' applicants are promoted at a higher rate than ``female'' applicants. In other words, how often was the discrimination in favor of men \emph{even more} pronounced than \(0.875 - 0.583 = 0.292 = 29.2\%\)?

In this case, 0 times out of 16, we obtained a difference in proportion greater than or equal to the observed difference of 0.292 = 29.2\%. A very rare (in fact, not occurring) outcome! Given the rarity of such a pronounced difference in promotion rates in our hypothesized universe of no gender discrimination, we're inclined to \emph{reject} \index{hypothesis testing!reject the null hypothesis} our hypothesized universe. Instead, we favor the hypothesis stating there is discrimination in favor of the ``male'' applicants. In other words, we reject \(H_0\) in favor of \(H_A\).

Seventh and lastly, in many hypothesis testing procedures, it is commonly recommended to set the \textbf{significance level} \index{hypothesis testing!significance level} of the test beforehand. It is denoted by the Greek letter \(\alpha\) (pronounced ``alpha''). This value acts as a cutoff on the \(p\)-value, where if the \(p\)-value falls below \(\alpha\), we would ``reject the null hypothesis \(H_0\).''

Alternatively, if the \(p\)-value does not fall below \(\alpha\), we would ``fail to reject \(H_0\).'' Note the latter statement is not quite the same as saying we ``accept \(H_0\).'' This distinction is rather subtle and not immediately obvious. So we'll revisit it later in Section \ref{ht-interpretation}.

While different fields tend to use different values of \(\alpha\), some commonly used values for \(\alpha\) are 0.1, 0.01, and 0.05; with 0.05 being the choice people often make without putting much thought into it. We'll talk more about \(\alpha\) significance levels in Section \ref{ht-interpretation}, but first let's fully conduct the hypothesis test corresponding to our promotions activity using the \texttt{infer} package.

\hypertarget{ht-infer}{%
\section{Conducting hypothesis tests}\label{ht-infer}}

In Section \ref{bootstrap-process}, we showed you how to construct confidence intervals. We first illustrated how to do this using \texttt{dplyr} data wrangling verbs and the \texttt{rep\_sample\_n()} function from Subsection \ref{shovel-1000-times} which we used as a virtual shovel. In particular, we constructed confidence intervals by resampling with replacement by setting the \texttt{replace\ =\ TRUE} argument to the \texttt{rep\_sample\_n()} function.

We then showed you how to perform the same task using the \texttt{infer} package workflow. While both workflows resulted in the same bootstrap distribution from which we can construct confidence intervals, the \texttt{infer} package workflow emphasizes each of the steps in the overall process in Figure \ref{fig:infer-ci}. It does so using function names that are intuitively named with verbs:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{specify()} the variables of interest in your data frame.
\item
  \texttt{generate()} replicates of bootstrap resamples with replacement.
\item
  \texttt{calculate()} the summary statistic of interest.
\item
  \texttt{visualize()} the resulting bootstrap distribution and confidence interval.
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth,height=0.9\textheight]{images/flowcharts/infer/visualize} 

}

\caption{Confidence intervals with the infer package.}\label{fig:infer-ci}
\end{figure}

In this section, we'll now show you how to seamlessly modify the previously seen \texttt{infer} code for constructing confidence intervals to conduct hypothesis tests. You'll notice that the basic outline of the workflow is almost identical, except for an additional \texttt{hypothesize()} step between the \texttt{specify()} and \texttt{generate()} steps, as can be seen in Figure \ref{fig:inferht}.

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth,height=0.9\textheight]{images/flowcharts/infer/ht} 

}

\caption{Hypothesis testing with the infer package.}\label{fig:inferht}
\end{figure}

Furthermore, we'll use a pre-specified significance level \(\alpha\) = 0.05 for this hypothesis test. Let's leave discussion on the choice of this \(\alpha\) value until later on in Section \ref{ht-interpretation}.

\hypertarget{infer-workflow-ht}{%
\subsection{\texorpdfstring{\texttt{infer} package workflow}{infer package workflow}}\label{infer-workflow-ht}}

\hypertarget{specify-variables-3}{%
\subsubsection*{\texorpdfstring{1. \texttt{specify} variables}{1. specify variables}}\label{specify-variables-3}}


Recall that we use the \texttt{specify()} \index{infer!specify()} verb to specify the response variable and, if needed, any explanatory variables for our study. In this case, since we are interested in any potential effects of gender on promotion decisions, we set \texttt{decision} as the response variable and \texttt{gender} as the explanatory variable. We do so using \texttt{formula\ =\ response\ \textasciitilde{}\ explanatory} where \texttt{response} is the name of the response variable in the data frame and \texttt{explanatory} is the name of the explanatory variable. So in our case it is \texttt{decision\ \textasciitilde{}\ gender}.

Furthermore, since we are interested in the proportion of résumés \texttt{"promoted"}, and not the proportion of résumés \texttt{not} promoted, we set the argument \texttt{success} to \texttt{"promoted"}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{promotions }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ decision }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gender, }\AttributeTok{success =} \StringTok{"promoted"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Response: decision (factor)
Explanatory: gender (factor)
# A tibble: 48 x 2
   decision gender
   <fct>    <fct> 
 1 promoted male  
 2 promoted male  
 3 promoted male  
 4 promoted male  
 5 promoted male  
 6 promoted male  
 7 promoted male  
 8 promoted male  
 9 promoted male  
10 promoted male  
# ... with 38 more rows
\end{verbatim}

Again, notice how the \texttt{promotions} data itself doesn't change, but the \texttt{Response:\ decision\ (factor)} and \texttt{Explanatory:\ gender\ (factor)} \emph{meta-data} do. This is similar to how the \texttt{group\_by()} verb from \texttt{dplyr} doesn't change the data, but only adds ``grouping'' meta-data, as we saw in Section \ref{groupby}.

\hypertarget{hypothesize-the-null}{%
\subsubsection*{\texorpdfstring{2. \texttt{hypothesize} the null}{2. hypothesize the null}}\label{hypothesize-the-null}}


In order to conduct hypothesis tests using the \texttt{infer} workflow, we need a new step not present for confidence intervals: \index{infer!hypothesize()} \texttt{hypothesize()}. Recall from Section \ref{understanding-ht} that our hypothesis test was

\[
\begin{aligned}
H_0 &: p_{m} - p_{f} = 0\\
\text{vs. } H_A&: p_{m} - p_{f} > 0
\end{aligned}
\]

In other words, the null hypothesis \(H_0\) corresponding to our ``hypothesized universe'' stated that there was no difference in gender-based discrimination rates. We set this null hypothesis \(H_0\) in our \texttt{infer} workflow using the \texttt{null} argument of the \texttt{hypothesize()} function to either:

\begin{itemize}
\tightlist
\item
  \texttt{"point"} for hypotheses involving a single sample or
\item
  \texttt{"independence"} for hypotheses involving two samples.
\end{itemize}

In our case, since we have two samples (the résumés with ``male'' and ``female'' names), we set \texttt{null\ =\ "independence"}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{promotions }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ decision }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gender, }\AttributeTok{success =} \StringTok{"promoted"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{hypothesize}\NormalTok{(}\AttributeTok{null =} \StringTok{"independence"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Response: decision (factor)
Explanatory: gender (factor)
Null Hypothesis: independence
# A tibble: 48 x 2
   decision gender
   <fct>    <fct> 
 1 promoted male  
 2 promoted male  
 3 promoted male  
 4 promoted male  
 5 promoted male  
 6 promoted male  
 7 promoted male  
 8 promoted male  
 9 promoted male  
10 promoted male  
# ... with 38 more rows
\end{verbatim}

Again, the data has not changed yet. This will occur at the upcoming \texttt{generate()} step; we're merely setting meta-data for now.

Where do the terms \texttt{"point"} and \texttt{"independence"} come from? These are two technical statistical terms. The term ``point'' relates from the fact that for a single group of observations, you will test the value of a single point. Going back to the pennies example from Chapter \ref{confidence-intervals}, say we wanted to test if the mean year of all US pennies was equal to 1993 or not. We would be testing the value of a ``point'' \(\mu\), the mean year of \emph{all} US pennies, as follows

\[
\begin{aligned}
H_0 &: \mu = 1993\\
\text{vs } H_A&: \mu \neq 1993
\end{aligned}
\]

The term ``independence'' relates to the fact that for two groups of observations, you are testing whether or not the response variable is \emph{independent} of the explanatory variable that assigns the groups. In our case, we are testing whether the \texttt{decision} response variable is ``independent'' of the explanatory variable \texttt{gender} that assigns each résumé to either of the two groups.

\hypertarget{generate-replicates-3}{%
\subsubsection*{\texorpdfstring{3. \texttt{generate} replicates}{3. generate replicates}}\label{generate-replicates-3}}


After we \texttt{hypothesize()} the null hypothesis, we \texttt{generate()} replicates of ``shuffled'' datasets assuming the null hypothesis is true. We do this by repeating the shuffling exercise you performed in Section \ref{ht-activity} several times. Instead of merely doing it 16 times as our groups of friends did, let's use the computer to repeat this 1000 times by setting \texttt{reps\ =\ 1000} in the \texttt{generate()} \index{infer!generate()} function. However, unlike for confidence intervals where we generated replicates using \texttt{type\ =\ "bootstrap"} resampling with replacement, we'll now perform shuffles/permutations by setting \texttt{type\ =\ "permute"}. Recall that shuffles/permutations are a kind of resampling, but unlike the bootstrap method, they involve resampling \emph{without} replacement.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{promotions\_generate }\OtherTok{\textless{}{-}}\NormalTok{ promotions }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ decision }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gender, }\AttributeTok{success =} \StringTok{"promoted"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{hypothesize}\NormalTok{(}\AttributeTok{null =} \StringTok{"independence"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{, }\AttributeTok{type =} \StringTok{"permute"}\NormalTok{)}
\FunctionTok{nrow}\NormalTok{(promotions\_generate)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 48000
\end{verbatim}

Observe that the resulting data frame has 48,000 rows. This is because we performed shuffles/permutations for each of the 48 rows 1000 times and \(48,000 = 1000 \cdot 48\). If you explore the \texttt{promotions\_generate} data frame with \texttt{View()}, you'll notice that the variable \texttt{replicate} indicates which resample each row belongs to. So it has the value \texttt{1} 48 times, the value \texttt{2} 48 times, all the way through to the value \texttt{1000} 48 times.

\hypertarget{calculate-summary-statistics-3}{%
\subsubsection*{\texorpdfstring{4. \texttt{calculate} summary statistics}{4. calculate summary statistics}}\label{calculate-summary-statistics-3}}


Now that we have generated 1000 replicates of ``shuffles'' assuming the null hypothesis is true, let's \texttt{calculate()} \index{infer!calculate()} the appropriate summary statistic for each of our 1000 shuffles. From Section \ref{understanding-ht}, point estimates related to hypothesis testing have a specific name: \emph{test statistics}. Since the unknown population parameter of interest is the difference in population proportions \(p_{m} - p_{f}\), the test statistic here is the difference in sample proportions \(\widehat{p}_{m} - \widehat{p}_{f}\).

For each of our 1000 shuffles, we can calculate this test statistic by setting \texttt{stat\ =\ "diff\ in\ props"}. Furthermore, since we are interested in \(\widehat{p}_{m} - \widehat{p}_{f}\) we set \texttt{order\ =\ c("male",\ "female")}. As we stated earlier, the order of the subtraction does not matter, so long as you stay consistent throughout your analysis and tailor your interpretations accordingly.

Let's save the result in a data frame called \texttt{null\_distribution}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{null\_distribution }\OtherTok{\textless{}{-}}\NormalTok{ promotions }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ decision }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gender, }\AttributeTok{success =} \StringTok{"promoted"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{hypothesize}\NormalTok{(}\AttributeTok{null =} \StringTok{"independence"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{, }\AttributeTok{type =} \StringTok{"permute"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"diff in props"}\NormalTok{, }\AttributeTok{order =} \FunctionTok{c}\NormalTok{(}\StringTok{"male"}\NormalTok{, }\StringTok{"female"}\NormalTok{))}
\NormalTok{null\_distribution}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,000 x 2
   replicate       stat
       <int>      <dbl>
 1         1 -0.0416667
 2         2 -0.125    
 3         3 -0.125    
 4         4 -0.0416667
 5         5 -0.0416667
 6         6 -0.125    
 7         7 -0.125    
 8         8 -0.125    
 9         9 -0.0416667
10        10 -0.0416667
# ... with 990 more rows
\end{verbatim}

Observe that we have 1000 values of \texttt{stat}, each representing one instance of \(\widehat{p}_{m} - \widehat{p}_{f}\) in a hypothesized world of no gender discrimination. Observe as well that we chose the name of this data frame carefully: \texttt{null\_distribution}. Recall once again from Section \ref{understanding-ht} that sampling distributions when the null hypothesis \(H_0\) is assumed to be true have a special name: the \emph{null distribution}.

What was the \emph{observed} difference in promotion rates? In other words, what was the \emph{observed test statistic} \(\widehat{p}_{m} - \widehat{p}_{f}\)? Recall from Section \ref{ht-activity} that we computed this observed difference by hand to be 0.875 - 0.583 = 0.292 = 29.2\%. We can also compute this value using the previous \texttt{infer} code but with the \texttt{hypothesize()} and \texttt{generate()} steps removed. Let's save this in \texttt{obs\_diff\_prop}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{obs\_diff\_prop }\OtherTok{\textless{}{-}}\NormalTok{ promotions }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(decision }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gender, }\AttributeTok{success =} \StringTok{"promoted"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"diff in props"}\NormalTok{, }\AttributeTok{order =} \FunctionTok{c}\NormalTok{(}\StringTok{"male"}\NormalTok{, }\StringTok{"female"}\NormalTok{))}
\NormalTok{obs\_diff\_prop}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 1
      stat
     <dbl>
1 0.291667
\end{verbatim}

\hypertarget{visualize-the-p-value}{%
\subsubsection*{\texorpdfstring{5. \texttt{visualize} the p-value}{5. visualize the p-value}}\label{visualize-the-p-value}}


The final step is to measure how surprised we are by a promotion difference of 29.2\% in a hypothesized universe of no gender discrimination. If the observed difference of 0.292 is highly unlikely, then we would be inclined to reject the validity of our hypothesized universe.

We start by visualizing the \emph{null distribution} of our 1000 values of \(\widehat{p}_{m} - \widehat{p}_{f}\) using \texttt{visualize()} \index{infer!visualize()} in Figure \ref{fig:null-distribution-infer}. Recall that these are values of the difference in promotion rates assuming \(H_0\) is true. This corresponds to being in our hypothesized universe of no gender discrimination.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{visualize}\NormalTok{(null\_distribution, }\AttributeTok{bins =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/null-distribution-infer-1} 

}

\caption{Null distribution.}\label{fig:null-distribution-infer}
\end{figure}

Let's now add what happened in real life to Figure \ref{fig:null-distribution-infer}, the observed difference in promotion rates of 0.875 - 0.583 = 0.292 = 29.2\%. However, instead of merely adding a vertical line using \texttt{geom\_vline()}, let's use the \index{infer!shade\_p\_value()} \texttt{shade\_p\_value()} function with \texttt{obs\_stat} set to the observed test statistic value we saved in \texttt{obs\_diff\_prop}.

Furthermore, we'll set the \texttt{direction\ =\ "right"} reflecting our alternative hypothesis \(H_A: p_{m} - p_{f} > 0\). Recall our alternative hypothesis \(H_A\) is that \(p_{m} - p_{f} > 0\), stating that there is a difference in promotion rates in favor of résumés with male names. ``More extreme'' here corresponds to differences that are ``bigger'' or ``more positive'' or ``more to the right.'' Hence we set the \texttt{direction} argument of \texttt{shade\_p\_value()} to be \texttt{"right"}.

On the other hand, had our alternative hypothesis \(H_A\) been the other possible one-sided alternative \(p_{m} - p_{f} < 0\), suggesting discrimination in favor of résumés with female names, we would've set \texttt{direction\ =\ "left"}. Had our alternative hypothesis \(H_A\) been two-sided \(p_{m} - p_{f} \neq 0\), suggesting discrimination in either direction, we would've set \texttt{direction\ =\ "both"}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{visualize}\NormalTok{(null\_distribution, }\AttributeTok{bins =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{shade\_p\_value}\NormalTok{(}\AttributeTok{obs\_stat =}\NormalTok{ obs\_diff\_prop, }\AttributeTok{direction =} \StringTok{"right"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/null-distribution-infer-2-1} 

}

\caption{Shaded histogram to show $p$-value.}\label{fig:null-distribution-infer-2}
\end{figure}

In the resulting Figure \ref{fig:null-distribution-infer-2}, the solid dark line marks 0.292 = 29.2\%. However, what does the shaded-region correspond to? This is the \emph{\(p\)-value}. Recall the definition of the \(p\)-value from Section \ref{understanding-ht}:

\begin{quote}
A \(p\)-value is the probability of obtaining a test statistic just as or more extreme than the observed test statistic \emph{assuming the null hypothesis \(H_0\) is true}.
\end{quote}

So judging by the shaded region in Figure \ref{fig:null-distribution-infer-2}, it seems we would somewhat rarely observe differences in promotion rates of 0.292 = 29.2\% or more in a hypothesized universe of no gender discrimination. In other words, the \(p\)-value is somewhat small. Hence, we would be inclined to reject this hypothesized universe, or using statistical language we would ``reject \(H_0\).''

What fraction of the null distribution is shaded? In other words, what is the exact value of the \(p\)-value? We can compute it using the \texttt{get\_p\_value()} \index{infer!get\_p\_value()} function with the same arguments as the previous \texttt{shade\_p\_value()} code:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{null\_distribution }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{get\_p\_value}\NormalTok{(}\AttributeTok{obs\_stat =}\NormalTok{ obs\_diff\_prop, }\AttributeTok{direction =} \StringTok{"right"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 1
  p_value
    <dbl>
1   0.027
\end{verbatim}

Keeping the definition of a \(p\)-value in mind, the probability of observing a difference in promotion rates as large as 0.292 = 29.2\% due to sampling variation alone in the null distribution is 0.027 = 2.7\%. Since this \(p\)-value is smaller than our pre-specified significance level \(\alpha\) = 0.05, we reject the null hypothesis \(H_0: p_{m} - p_{f} = 0\). In other words, this \(p\)-value is sufficiently small to reject our hypothesized universe of no gender discrimination. We instead have enough evidence to change our mind in favor of gender discrimination being a likely culprit here. Observe that whether we reject the null hypothesis \(H_0\) or not depends in large part on our choice of significance level \(\alpha\). We'll discuss this more in Subsection \ref{choosing-alpha}.

\hypertarget{comparing-infer-workflows}{%
\subsection{Comparison with confidence intervals}\label{comparing-infer-workflows}}

One of the great things about the \texttt{infer} package is that we can jump seamlessly between conducting hypothesis tests and constructing confidence intervals with minimal changes! Recall the code from the previous section that creates the null distribution, which in turn is needed to compute the \(p\)-value:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{null\_distribution }\OtherTok{\textless{}{-}}\NormalTok{ promotions }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ decision }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gender, }\AttributeTok{success =} \StringTok{"promoted"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{hypothesize}\NormalTok{(}\AttributeTok{null =} \StringTok{"independence"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{, }\AttributeTok{type =} \StringTok{"permute"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"diff in props"}\NormalTok{, }\AttributeTok{order =} \FunctionTok{c}\NormalTok{(}\StringTok{"male"}\NormalTok{, }\StringTok{"female"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

To create the corresponding bootstrap distribution needed to construct a 95\% confidence interval for \(p_{m} - p_{f}\), we only need to make two changes. \index{infer!switching between tests and confidence intervals} First, we remove the \texttt{hypothesize()} step since we are no longer assuming a null hypothesis \(H_0\) is true. We can do this by deleting or commenting out the \texttt{hypothesize()} line of code. Second, we switch the \texttt{type} of resampling in the \texttt{generate()} step to be \texttt{"bootstrap"} instead of \texttt{"permute"}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bootstrap\_distribution }\OtherTok{\textless{}{-}}\NormalTok{ promotions }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ decision }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gender, }\AttributeTok{success =} \StringTok{"promoted"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \CommentTok{\# Change 1 {-} Remove hypothesize():}
  \CommentTok{\# hypothesize(null = "independence") \%\textgreater{}\% }
  \CommentTok{\# Change 2 {-} Switch type from "permute" to "bootstrap":}
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{, }\AttributeTok{type =} \StringTok{"bootstrap"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"diff in props"}\NormalTok{, }\AttributeTok{order =} \FunctionTok{c}\NormalTok{(}\StringTok{"male"}\NormalTok{, }\StringTok{"female"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Using this \texttt{bootstrap\_distribution}, let's first compute the percentile-based confidence intervals, as we did in Section \ref{bootstrap-process}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{percentile\_ci }\OtherTok{\textless{}{-}}\NormalTok{ bootstrap\_distribution }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{get\_confidence\_interval}\NormalTok{(}\AttributeTok{level =} \FloatTok{0.95}\NormalTok{, }\AttributeTok{type =} \StringTok{"percentile"}\NormalTok{)}
\NormalTok{percentile\_ci}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 2
   lower_ci upper_ci
      <dbl>    <dbl>
1 0.0444444 0.538542
\end{verbatim}

Using our shorthand interpretation for 95\% confidence intervals from Subsection \ref{shorthand}, we are 95\% ``confident'' that the true difference in population proportions \(p_{m} - p_{f}\) is between (0.044, 0.539). Let's visualize \texttt{bootstrap\_distribution} and this percentile-based 95\% confidence interval for \(p_{m} - p_{f}\) in Figure \ref{fig:bootstrap-distribution-two-prop-percentile}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{visualize}\NormalTok{(bootstrap\_distribution) }\SpecialCharTok{+} 
  \FunctionTok{shade\_confidence\_interval}\NormalTok{(}\AttributeTok{endpoints =}\NormalTok{ percentile\_ci)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/bootstrap-distribution-two-prop-percentile-1} 

}

\caption{Percentile-based 95\% confidence interval.}\label{fig:bootstrap-distribution-two-prop-percentile}
\end{figure}

Notice a key value that is not included in the 95\% confidence interval for \(p_{m} - p_{f}\): the value 0. In other words, a difference of 0 is not included in our net, suggesting that \(p_{m}\) and \(p_{f}\) are truly different! Furthermore, observe how the entirety of the 95\% confidence interval for \(p_{m} - p_{f}\) lies above 0, suggesting that this difference is in favor of men.

Since the bootstrap distribution appears to be roughly normally shaped, we can also use the standard error method as we did in Section \ref{bootstrap-process}. In this case, we must specify the \texttt{point\_estimate} argument as the observed difference in promotion rates 0.292 = 29.2\% saved in \texttt{obs\_diff\_prop}. This value acts as the center of the confidence interval.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{se\_ci }\OtherTok{\textless{}{-}}\NormalTok{ bootstrap\_distribution }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{get\_confidence\_interval}\NormalTok{(}\AttributeTok{level =} \FloatTok{0.95}\NormalTok{, }\AttributeTok{type =} \StringTok{"se"}\NormalTok{, }
                          \AttributeTok{point\_estimate =}\NormalTok{ obs\_diff\_prop)}
\NormalTok{se\_ci}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 2
   lower_ci upper_ci
      <dbl>    <dbl>
1 0.0514129 0.531920
\end{verbatim}

Let's visualize \texttt{bootstrap\_distribution} again, but now the standard error based 95\% confidence interval for \(p_{m} - p_{f}\) in Figure \ref{fig:bootstrap-distribution-two-prop-se}. Again, notice how the value 0 is not included in our confidence interval, again suggesting that \(p_{m}\) and \(p_{f}\) are truly different!

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{visualize}\NormalTok{(bootstrap\_distribution) }\SpecialCharTok{+} 
  \FunctionTok{shade\_confidence\_interval}\NormalTok{(}\AttributeTok{endpoints =}\NormalTok{ se\_ci)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/bootstrap-distribution-two-prop-se-1} 

}

\caption{Standard error-based 95\% confidence interval.}\label{fig:bootstrap-distribution-two-prop-se}
\end{figure}

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC9.1)} Conduct the same hypothesis test and confidence interval analysis comparing male and female promotion rates using the median rating instead of the mean rating. What was different and what was the same?

\textbf{(LC9.2)} Why are we relatively confident that the distributions of the sample proportions will be good approximations of the population distributions of promotion proportions for the two genders?

\textbf{(LC9.3)} Using the definition of \emph{p-value}, write in words what the \(p\)-value represents for the hypothesis test comparing the promotion rates for males and females.

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{only-one-test}{%
\subsection{``There is only one test''}\label{only-one-test}}

Let's recap the steps necessary to conduct a hypothesis test using the terminology, notation, and definitions related to sampling you saw in Section \ref{understanding-ht} and the \texttt{infer} workflow from Subsection \ref{infer-workflow-ht}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{specify()} the variables of interest in your data frame.
\item
  \texttt{hypothesize()} the null hypothesis \(H_0\). In other words, set a ``model for the universe'' assuming \(H_0\) is true.
\item
  \texttt{generate()} shuffles assuming \(H_0\) is true. In other words, \emph{simulate} data assuming \(H_0\) is true.
\item
  \texttt{calculate()} the \emph{test statistic} of interest, both for the observed data and your \emph{simulated} data.
\item
  \texttt{visualize()} the resulting \emph{null distribution} and compute the \emph{\(p\)-value} by comparing the null distribution to the observed test statistic.
\end{enumerate}

While this is a lot to digest, especially the first time you encounter hypothesis testing, the nice thing is that once you understand this general framework, then you can understand \emph{any} hypothesis test. In a famous blog post, computer scientist Allen Downey called this the \href{http://allendowney.blogspot.com/2016/06/there-is-still-only-one-test.html}{``There is only one test''} framework, for which he created the flowchart displayed in Figure \ref{fig:htdowney}.

\begin{figure}

{\centering \includegraphics[width=1.1\linewidth]{images/copyright/there_is_only_one_test} 

}

\caption{Allen Downey's hypothesis testing framework.}\label{fig:htdowney}
\end{figure}

Notice its similarity with the ``hypothesis testing with \texttt{infer}'' diagram you saw in Figure \ref{fig:inferht}. That's because the \texttt{infer} package was explicitly designed to match the ``There is only one test'' framework. So if you can understand the framework, you can easily generalize these ideas for all hypothesis testing scenarios. Whether for population proportions \(p\), population means \(\mu\), differences in population proportions \(p_1 - p_2\), differences in population means \(\mu_1 - \mu_2\), and as you'll see in Chapter \ref{inference-for-regression} on inference for regression, population regression slopes \(\beta_1\) as well. In fact, it applies more generally even than just these examples to more complicated hypothesis tests and test statistics as well.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC9.4)} Describe in a paragraph how we used Allen Downey's diagram to conclude if a statistical difference existed between the promotion rate of males and females using this study.

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{ht-interpretation}{%
\section{Interpreting hypothesis tests}\label{ht-interpretation}}

Interpreting the results of hypothesis tests is one of the more challenging aspects of this method for statistical inference. In this section, we'll focus on ways to help with deciphering the process and address some common misconceptions.

\hypertarget{trial}{%
\subsection{Two possible outcomes}\label{trial}}

In Section \ref{understanding-ht}, we mentioned that given a pre-specified significance level \(\alpha\) there are two possible outcomes of a hypothesis test:

\begin{itemize}
\tightlist
\item
  If the \(p\)-value is less than \(\alpha\), then we \emph{reject} the null hypothesis \(H_0\) in favor of \(H_A\).
\item
  If the \(p\)-value is greater than or equal to \(\alpha\), we \emph{fail to reject} the null hypothesis \(H_0\).
\end{itemize}

Unfortunately, the latter result is often misinterpreted as ``accepting the null hypothesis \(H_0\).'' While at first glance it may seem that the statements ``failing to reject \(H_0\)'' and ``accepting \(H_0\)'' are equivalent, there actually is a subtle difference. Saying that we ``accept the null hypothesis \(H_0\)'' is equivalent to stating that ``we think the null hypothesis \(H_0\) is true.'' However, saying that we ``fail to reject the null hypothesis \(H_0\)'' is saying something else: ``While \(H_0\) might still be false, we don't have enough evidence to say so.'' In other words, there is an absence of enough proof. However, the absence of proof is not proof of absence.

To further shed light on this distinction, \index{hypothesis testing!US criminal trial analogy} let's use the United States criminal justice system as an analogy. A criminal trial in the United States is a similar situation to hypothesis tests whereby a choice between two contradictory claims must be made about a defendant who is on trial:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The defendant is truly either ``innocent'' or ``guilty.''
\item
  The defendant is presumed ``innocent until proven guilty.''
\item
  The defendant is found guilty only if there is \emph{strong evidence} that the defendant is guilty. The phrase ``beyond a reasonable doubt'' is often used as a guideline for determining a cutoff for when enough evidence exists to find the defendant guilty.
\item
  The defendant is found to be either ``not guilty'' or ``guilty'' in the ultimate verdict.
\end{enumerate}

In other words, \emph{not guilty} verdicts are not suggesting the defendant is \emph{innocent}, but instead that ``while the defendant may still actually be guilty, there wasn't enough evidence to prove this fact.'' Now let's make the connection with hypothesis tests:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Either the null hypothesis \(H_0\) or the alternative hypothesis \(H_A\) is true.
\item
  Hypothesis tests are conducted assuming the null hypothesis \(H_0\) is true.
\item
  We reject the null hypothesis \(H_0\) in favor of \(H_A\) only if the evidence found in the sample suggests that \(H_A\) is true. The significance level \(\alpha\) is used as a guideline to set the threshold on just how strong of evidence we require.
\item
  We ultimately decide to either ``fail to reject \(H_0\)'' or ``reject \(H_0\).''
\end{enumerate}

So while gut instinct may suggest ``failing to reject \(H_0\)'' and ``accepting \(H_0\)'' are equivalent statements, they are not. ``Accepting \(H_0\)'' is equivalent to finding a defendant innocent. However, courts do not find defendants ``innocent,'' but rather they find them ``not guilty.'' Putting things differently, defense attorneys do not need to prove that their clients are innocent, rather they only need to prove that clients are not ``guilty beyond a reasonable doubt''.

So going back to our résumés activity in Section \ref{ht-infer}, recall that our hypothesis test was \(H_0: p_{m} - p_{f} = 0\) versus \(H_A: p_{m} - p_{f} > 0\) and that we used a pre-specified significance level of \(\alpha\) = 0.05. We found a \(p\)-value of 0.027. Since the \(p\)-value was smaller than \(\alpha\) = 0.05, we rejected \(H_0\). In other words, we found needed levels of evidence in this particular sample to say that \(H_0\) is false at the \(\alpha\) = 0.05 significance level. We also state this conclusion using non-statistical language: we found enough evidence in this data to suggest that there was gender discrimination at play.

\hypertarget{types-of-errors}{%
\subsection{Types of errors}\label{types-of-errors}}

Unfortunately, there is some chance a jury or a judge can make an incorrect decision in a criminal trial by reaching the wrong verdict. For example, finding a truly innocent defendant ``guilty''. Or on the other hand, finding a truly guilty defendant ``not guilty.'' This can often stem from the fact that prosecutors don't have access to all the relevant evidence, but instead are limited to whatever evidence the police can find.

The same holds for hypothesis tests. We can make incorrect decisions about a population parameter because we only have a sample of data from the population and thus sampling variation can lead us to incorrect conclusions.

There are two possible erroneous conclusions in a criminal trial: either (1) a truly innocent person is found guilty or (2) a truly guilty person is found not guilty. Similarly, there are two possible errors in a hypothesis test: either (1) rejecting \(H_0\) when in fact \(H_0\) is true, called a \textbf{Type I error} \index{hypothesis testing!Type I error} or (2) failing to reject \(H_0\) when in fact \(H_0\) is false, called a \index{hypothesis testing!Type II error} \textbf{Type II error}. Another term used for ``Type I error'' is ``false positive,'' while another term for ``Type II error'' is ``false negative.''

This risk of error is the price researchers pay for basing inference on a sample instead of performing a census on the entire population. But as we've seen in our numerous examples and activities so far, censuses are often very expensive and other times impossible, and thus researchers have no choice but to use a sample. Thus in any hypothesis test based on a sample, we have no choice but to tolerate some chance that a Type I error will be made and some chance that a Type II error will occur.

To help understand the concepts of Type I error and Type II errors, we apply these terms to our criminal justice analogy in Figure \ref{fig:trial-errors-table}.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{images/gt_error_table} 

}

\caption{Type I and Type II errors in criminal trials.}\label{fig:trial-errors-table}
\end{figure}

Thus a Type I error corresponds to incorrectly putting a truly innocent person in jail, whereas a Type II error corresponds to letting a truly guilty person go free. Let's show the corresponding table in Figure \ref{fig:trial-errors-table-ht} for hypothesis tests.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{images/gt_error_table_ht} 

}

\caption{Type I and Type II errors in hypothesis tests.}\label{fig:trial-errors-table-ht}
\end{figure}

\hypertarget{choosing-alpha}{%
\subsection{How do we choose alpha?}\label{choosing-alpha}}

If we are using a sample to make inferences about a population, we run the risk of making errors. For confidence intervals, a corresponding ``error'' would be constructing a confidence interval that does not contain the true value of the population parameter. For hypothesis tests, this would be making either a Type I or Type II error. Obviously, we want to minimize the probability of either error; we want a small probability of making an incorrect conclusion:

\begin{itemize}
\tightlist
\item
  The probability of a Type I Error occurring is denoted by \(\alpha\). The value of \(\alpha\) is called the \emph{significance level} of the hypothesis test, which we defined in Section \ref{understanding-ht}.
\item
  The probability of a Type II Error is denoted by \(\beta\). The value of \(1-\beta\) is known as the \emph{power} of the hypothesis test.
\end{itemize}

In other words, \(\alpha\) corresponds to the probability of incorrectly rejecting \(H_0\) when in fact \(H_0\) is true. On the other hand, \(\beta\) corresponds to the probability of incorrectly failing to reject \(H_0\) when in fact \(H_0\) is false.

Ideally, we want \(\alpha = 0\) and \(\beta = 0\), meaning that the chance of making either error is 0. However, this can never be the case in any situation where we are sampling for inference. There will always be the possibility of making either error when we use sample data. Furthermore, these two error probabilities are inversely related. As the probability of a Type I error goes down, the probability of a Type II error goes up.

What is typically done in practice is to fix the probability of a Type I error by pre-specifying a significance level \(\alpha\) and then try to minimize \(\beta\). In other words, we will tolerate a certain fraction of incorrect rejections of the null hypothesis \(H_0\), and then try to minimize the fraction of incorrect non-rejections of \(H_0\).

So for example if we used \(\alpha\) = 0.01, we would be using a hypothesis testing procedure that in the long run would incorrectly reject the null hypothesis \(H_0\) one percent of the time. This is analogous to setting the confidence level of a confidence interval.

So what value should you use for \(\alpha\)? \index{hypothesis testing!tradeoff between alpha and beta} Different fields have different conventions, but some commonly used values include 0.10, 0.05, 0.01, and 0.001. However, it is important to keep in mind that if you use a relatively small value of \(\alpha\), then all things being equal, \(p\)-values will have a harder time being less than \(\alpha\). Thus we would reject the null hypothesis less often. In other words, we would reject the null hypothesis \(H_0\) only if we have \emph{very strong} evidence to do so. This is known as a ``conservative'' test.

On the other hand, if we used a relatively large value of \(\alpha\), then all things being equal, \(p\)-values will have an easier time being less than \(\alpha\). Thus we would reject the null hypothesis more often. In other words, we would reject the null hypothesis \(H_0\) even if we only have \emph{mild} evidence to do so. This is known as a ``liberal'' test.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC9.5)} What is wrong about saying, ``The defendant is innocent.'' based on the US system of criminal trials?

\textbf{(LC9.6)} What is the purpose of hypothesis testing?

\textbf{(LC9.7)} What are some flaws with hypothesis testing? How could we alleviate them?

\textbf{(LC9.8)} Consider two \(\alpha\) significance levels of 0.1 and 0.01. Of the two, which would lead to a more \emph{liberal} hypothesis testing procedure? In other words, one that will, all things being equal, lead to more rejections of the null hypothesis \(H_0\).

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{ht-case-study}{%
\section{Case study: Are action or romance movies rated higher?}\label{ht-case-study}}

Let's apply our knowledge of hypothesis testing to answer the question: ``Are action or romance movies rated higher on IMDb?''. \href{https://www.imdb.com/}{IMDb} is a database on the internet providing information on movie and television show casts, plot summaries, trivia, and ratings. We'll investigate if, on average, action or romance movies get higher ratings on IMDb.

\hypertarget{imdb-data}{%
\subsection{IMDb ratings data}\label{imdb-data}}

The \texttt{movies} dataset in the \texttt{ggplot2movies} package contains information on 58,788 movies that have been rated by users of IMDb.com.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{movies}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 58,788 x 24
   title  year length budget rating votes    r1    r2    r3    r4    r5
   <chr> <int>  <int>  <int>  <dbl> <int> <dbl> <dbl> <dbl> <dbl> <dbl>
 1 $      1971    121     NA  6.4     348   4.5   4.5   4.5   4.5  14.5
 2 $100~  1939     71     NA  6        20   0    14.5   4.5  24.5  14.5
 3 $21 ~  1941      7     NA  8.200     5   0     0     0     0     0  
 4 $40,~  1996     70     NA  8.200     6  14.5   0     0     0     0  
 5 $50,~  1975     71     NA  3.4      17  24.5   4.5   0    14.5  14.5
 6 $pent  2000     91     NA  4.3      45   4.5   4.5   4.5  14.5  14.5
 7 $win~  2002     93     NA  5.3     200   4.5   0     4.5   4.5  24.5
 8 '15'   2002     25     NA  6.7      24   4.5   4.5   4.5   4.5   4.5
 9 '38    1987     97     NA  6.6      18   4.5   4.5   4.5   0     0  
10 '49-~  1917     61     NA  6        51   4.5   0     4.5   4.5   4.5
# ... with 58,778 more rows, and 13 more variables: r6 <dbl>, r7 <dbl>,
#   r8 <dbl>, r9 <dbl>, r10 <dbl>, mpaa <chr>, Action <int>,
#   Animation <int>, Comedy <int>, Drama <int>, Documentary <int>,
#   Romance <int>, Short <int>
\end{verbatim}

We'll focus on a random sample of 68 movies that are classified as either ``action'' or ``romance'' movies but not both. We disregard movies that are classified as both so that we can assign all 68 movies into either category. Furthermore, since the original \texttt{movies} dataset was a little messy, we provide a pre-wrangled version of our data in the \texttt{movies\_sample} data frame included in the \texttt{moderndive} package. If you're curious, you can look at the necessary data wrangling code to do this on \href{https://github.com/moderndive/moderndive/blob/master/data-raw/process_data_sets.R}{GitHub}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{movies\_sample}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 68 x 4
   title                     year rating genre  
   <chr>                    <int>  <dbl> <chr>  
 1 Underworld                1985    3.1 Action 
 2 Love Affair               1932    6.3 Romance
 3 Junglee                   1961    6.8 Romance
 4 Eversmile, New Jersey     1989    5   Romance
 5 Search and Destroy        1979    4   Action 
 6 Secreto de Romelia, El    1988    4.9 Romance
 7 Amants du Pont-Neuf, Les  1991    7.4 Romance
 8 Illicit Dreams            1995    3.5 Action 
 9 Kabhi Kabhie              1976    7.7 Romance
10 Electric Horseman, The    1979    5.8 Romance
# ... with 58 more rows
\end{verbatim}

The variables include the \texttt{title} and \texttt{year} the movie was filmed. Furthermore, we have a numerical variable \texttt{rating}, which is the IMDb rating out of 10 stars, and a binary categorical variable \texttt{genre} indicating if the movie was an \texttt{Action} or \texttt{Romance} movie. We are interested in whether \texttt{Action} or \texttt{Romance} movies got a higher \texttt{rating} on average.

Let's perform an exploratory data analysis of this data. Recall from Subsection \ref{geomboxplot} that a boxplot is a visualization we can use to show the relationship between a numerical and a categorical variable. Another option you saw in Section \ref{facets} would be to use a faceted histogram. However, in the interest of brevity, let's only present the boxplot in Figure \ref{fig:action-romance-boxplot}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ movies\_sample, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ genre, }\AttributeTok{y =}\NormalTok{ rating)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{"IMDb rating"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/action-romance-boxplot-1} 

}

\caption{Boxplot of IMDb rating vs. genre.}\label{fig:action-romance-boxplot}
\end{figure}

Eyeballing Figure \ref{fig:action-romance-boxplot}, romance movies have a higher median rating. Do we have reason to believe, however, that there is a \emph{significant} difference between the mean \texttt{rating} for action movies compared to romance movies? It's hard to say just based on this plot. The boxplot does show that the median sample rating is higher for romance movies.

However, there is a large amount of overlap between the boxes. Recall that the median isn't necessarily the same as the mean either, depending on whether the distribution is skewed.

Let's calculate some summary statistics split by the binary categorical variable \texttt{genre}: the number of movies, the mean rating, and the standard deviation split by \texttt{genre}. We'll do this using \texttt{dplyr} data wrangling verbs. Notice in particular how we count the number of each type of movie using the \texttt{n()} summary function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{movies\_sample }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(genre) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{n =} \FunctionTok{n}\NormalTok{(), }\AttributeTok{mean\_rating =} \FunctionTok{mean}\NormalTok{(rating), }\AttributeTok{std\_dev =} \FunctionTok{sd}\NormalTok{(rating))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 4
  genre       n mean_rating std_dev
  <chr>   <int>       <dbl>   <dbl>
1 Action     32     5.275   1.36121
2 Romance    36     6.32222 1.60963
\end{verbatim}

Observe that we have 36 movies with an average rating of 6.322 stars and 32 movies with an average rating of 5.275 stars. The difference in these average ratings is thus 6.322 - 5.275 = 1.047. So there appears to be an edge of 1.047 stars in favor of romance movies. The question is, however, are these results indicative of a true difference for \emph{all} romance and action movies? Or could we attribute this difference to chance \emph{sampling variation}?

\hypertarget{sampling-scenario-1}{%
\subsection{Sampling scenario}\label{sampling-scenario-1}}

Let's now revisit this study in terms of terminology and notation related to sampling we studied in Subsection \ref{terminology-and-notation}. The \emph{study population} is all movies in the IMDb database that are either action or romance (but not both). The \emph{sample} from this population is the 68 movies included in the \texttt{movies\_sample} dataset.

Since this sample was randomly taken from the population \texttt{movies}, it is representative of all romance and action movies on IMDb. Thus, any analysis and results based on \texttt{movies\_sample} can generalize to the entire population. What are the relevant \emph{population parameter} and \emph{point estimates}? We introduce the fourth sampling scenario in Table \ref{tab:summarytable-ch10}.

\begin{table}[!h]

\caption{\label{tab:summarytable-ch10}Scenarios of sampling for inference}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{>{\raggedleft\arraybackslash}p{0.5in}>{\raggedright\arraybackslash}p{0.7in}>{\raggedright\arraybackslash}p{1in}>{\raggedright\arraybackslash}p{1.1in}>{\raggedright\arraybackslash}p{1in}}
\toprule
Scenario & Population parameter & Notation & Point estimate & Symbol(s)\\
\midrule
1 & Population proportion & $p$ & Sample proportion & $\widehat{p}$\\
2 & Population mean & $\mu$ & Sample mean & $\overline{x}$ or $\widehat{\mu}$\\
3 & Difference in population proportions & $p_1 - p_2$ & Difference in sample proportions & $\widehat{p}_1 - \widehat{p}_2$\\
4 & Difference in population means & $\mu_1 - \mu_2$ & Difference in sample means & $\overline{x}_1 - \overline{x}_2$\\
\bottomrule
\end{tabular}
\end{table}

So, whereas the sampling bowl exercise in Section \ref{sampling-activity} concerned \emph{proportions}, the pennies exercise in Section \ref{resampling-tactile} concerned \emph{means}, the case study on whether yawning is contagious in Section \ref{case-study-two-prop-ci} and the promotions activity in Section \ref{ht-activity} concerned \emph{differences in proportions}, we are now concerned with \emph{differences in means}.

In other words, the population parameter of interest is the difference in population mean ratings \(\mu_a - \mu_r\), where \(\mu_a\) is the mean rating of all action movies on IMDb and similarly \(\mu_r\) is the mean rating of all romance movies. Additionally the point estimate/sample statistic of interest is the difference in sample means \(\overline{x}_a - \overline{x}_r\), where \(\overline{x}_a\) is the mean rating of the \(n_a\) = 32 movies in our sample and \(\overline{x}_r\) is the mean rating of the \(n_r\) = 36 in our sample. Based on our earlier exploratory data analysis, our estimate \(\overline{x}_a - \overline{x}_r\) is \(5.275 - 6.322 = -1.047\).

So there appears to be a slight difference of -1.047 in favor of romance movies. The question is, however, could this difference of -1.047 be merely due to chance and sampling variation? Or are these results indicative of a true difference in mean ratings for \emph{all} romance and action movies on IMDb? To answer this question, we'll use hypothesis testing.

\hypertarget{conducting-the-hypothesis-test}{%
\subsection{Conducting the hypothesis test}\label{conducting-the-hypothesis-test}}

We'll be testing:

\[
\begin{aligned}
H_0 &: \mu_a - \mu_r = 0\\
\text{vs } H_A&: \mu_a - \mu_r \neq 0
\end{aligned}
\]

In other words, the null hypothesis \(H_0\) suggests that both romance and action movies have the same mean rating. This is the ``hypothesized universe'' we'll \emph{assume} is true. On the other hand, the alternative hypothesis \(H_A\) suggests that there is a difference. Unlike the one-sided alternative we used in the promotions exercise \(H_A: p_m - p_f > 0\), we are now considering a two-sided alternative of \(H_A: \mu_a - \mu_r \neq 0\).

Furthermore, we'll pre-specify a low significance level of \(\alpha\) = 0.001. By setting this value low, all things being equal, there is a lower chance that the \(p\)-value will be less than \(\alpha\). Thus, there is a lower chance that we'll reject the null hypothesis \(H_0\) in favor of the alternative hypothesis \(H_A\). In other words, we'll reject the hypothesis that there is no difference in mean ratings for all action and romance movies, only if we have quite strong evidence. This is known as a ``conservative'' hypothesis testing procedure.

\hypertarget{specify-variables-4}{%
\subsubsection*{\texorpdfstring{1. \texttt{specify} variables}{1. specify variables}}\label{specify-variables-4}}


Let's now perform all the steps of the \texttt{infer} workflow. We first \texttt{specify()} the variables of interest in the \texttt{movies\_sample} data frame using the formula \texttt{rating\ \textasciitilde{}\ genre}. This tells \texttt{infer} that the numerical variable \texttt{rating} is the outcome variable, while the binary variable \texttt{genre} is the explanatory variable. Note that unlike previously when we were interested in proportions, since we are now interested in the mean of a numerical variable, we do not need to set the \texttt{success} argument.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{movies\_sample }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ rating }\SpecialCharTok{\textasciitilde{}}\NormalTok{ genre)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Response: rating (numeric)
Explanatory: genre (factor)
# A tibble: 68 x 2
   rating genre  
    <dbl> <fct>  
 1    3.1 Action 
 2    6.3 Romance
 3    6.8 Romance
 4    5   Romance
 5    4   Action 
 6    4.9 Romance
 7    7.4 Romance
 8    3.5 Action 
 9    7.7 Romance
10    5.8 Romance
# ... with 58 more rows
\end{verbatim}

Observe at this point that the data in \texttt{movies\_sample} has not changed. The only change so far is the newly defined \texttt{Response:\ rating\ (numeric)} and \texttt{Explanatory:\ genre\ (factor)} \emph{meta-data}.

\hypertarget{hypothesize-the-null-1}{%
\subsubsection*{\texorpdfstring{2. \texttt{hypothesize} the null}{2. hypothesize the null}}\label{hypothesize-the-null-1}}


We set the null hypothesis \(H_0: \mu_a - \mu_r = 0\) by using the \texttt{hypothesize()} function. Since we have two samples, action and romance movies, we set \texttt{null} to be \texttt{"independence"} as we described in Section \ref{ht-infer}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{movies\_sample }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ rating }\SpecialCharTok{\textasciitilde{}}\NormalTok{ genre) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{hypothesize}\NormalTok{(}\AttributeTok{null =} \StringTok{"independence"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Response: rating (numeric)
Explanatory: genre (factor)
Null Hypothesis: independence
# A tibble: 68 x 2
   rating genre  
    <dbl> <fct>  
 1    3.1 Action 
 2    6.3 Romance
 3    6.8 Romance
 4    5   Romance
 5    4   Action 
 6    4.9 Romance
 7    7.4 Romance
 8    3.5 Action 
 9    7.7 Romance
10    5.8 Romance
# ... with 58 more rows
\end{verbatim}

\hypertarget{generate-replicates-4}{%
\subsubsection*{\texorpdfstring{3. \texttt{generate} replicates}{3. generate replicates}}\label{generate-replicates-4}}


After we have set the null hypothesis, we generate ``shuffled'' replicates assuming the null hypothesis is true by repeating the shuffling/permutation exercise you performed in Section \ref{ht-activity}.

We'll repeat this resampling without replacement of \texttt{type\ =\ "permute"} a total of \texttt{reps\ =\ 1000} times. Feel free to run the code below to check out what the \texttt{generate()} step produces.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{movies\_sample }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ rating }\SpecialCharTok{\textasciitilde{}}\NormalTok{ genre) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{hypothesize}\NormalTok{(}\AttributeTok{null =} \StringTok{"independence"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{, }\AttributeTok{type =} \StringTok{"permute"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{View}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\hypertarget{calculate-summary-statistics-4}{%
\subsubsection*{\texorpdfstring{4. \texttt{calculate} summary statistics}{4. calculate summary statistics}}\label{calculate-summary-statistics-4}}


Now that we have 1000 replicated ``shuffles'' assuming the null hypothesis \(H_0\) that both \texttt{Action} and \texttt{Romance} movies on average have the same ratings on IMDb, let's \texttt{calculate()} the appropriate summary statistic for these 1000 replicated shuffles. From Section \ref{understanding-ht}, summary statistics relating to hypothesis testing have a specific name: \emph{test statistics}. Since the unknown population parameter of interest is the difference in population means \(\mu_{a} - \mu_{r}\), the test statistic of interest here is the difference in sample means \(\overline{x}_{a} - \overline{x}_{r}\).

For each of our 1000 shuffles, we can calculate this test statistic by setting \texttt{stat\ =\ "diff\ in\ means"}. Furthermore, since we are interested in \(\overline{x}_{a} - \overline{x}_{r}\), we set \texttt{order\ =\ c("Action",\ "Romance")}. Let's save the results in a data frame called \texttt{null\_distribution\_movies}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{null\_distribution\_movies }\OtherTok{\textless{}{-}}\NormalTok{ movies\_sample }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ rating }\SpecialCharTok{\textasciitilde{}}\NormalTok{ genre) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{hypothesize}\NormalTok{(}\AttributeTok{null =} \StringTok{"independence"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{, }\AttributeTok{type =} \StringTok{"permute"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"diff in means"}\NormalTok{, }\AttributeTok{order =} \FunctionTok{c}\NormalTok{(}\StringTok{"Action"}\NormalTok{, }\StringTok{"Romance"}\NormalTok{))}
\NormalTok{null\_distribution\_movies}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,000 x 2
   replicate      stat
       <int>     <dbl>
 1         1  0.511111
 2         2  0.345833
 3         3 -0.327083
 4         4 -0.209028
 5         5 -0.433333
 6         6 -0.102778
 7         7  0.387153
 8         8  0.16875 
 9         9  0.257292
10        10  0.334028
# ... with 990 more rows
\end{verbatim}

Observe that we have 1000 values of \texttt{stat}, each representing one instance of \(\overline{x}_{a} - \overline{x}_{r}\). The 1000 values form the \emph{null distribution}, which is the technical term for the sampling distribution of the difference in sample means \(\overline{x}_{a} - \overline{x}_{r}\) assuming \(H_0\) is true. What happened in real life? What was the observed difference in promotion rates? What was the \emph{observed test statistic} \(\overline{x}_{a} - \overline{x}_{r}\)? Recall from our earlier data wrangling, this observed difference in means was \(5.275 - 6.322 = -1.047\). We can also achieve this using the code that constructed the null distribution \texttt{null\_distribution\_movies} but with the \texttt{hypothesize()} and \texttt{generate()} steps removed. Let's save this in \texttt{obs\_diff\_means}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{obs\_diff\_means }\OtherTok{\textless{}{-}}\NormalTok{ movies\_sample }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ rating }\SpecialCharTok{\textasciitilde{}}\NormalTok{ genre) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"diff in means"}\NormalTok{, }\AttributeTok{order =} \FunctionTok{c}\NormalTok{(}\StringTok{"Action"}\NormalTok{, }\StringTok{"Romance"}\NormalTok{))}
\NormalTok{obs\_diff\_means}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 1
      stat
     <dbl>
1 -1.04722
\end{verbatim}

\hypertarget{visualize-the-p-value-1}{%
\subsubsection*{\texorpdfstring{5. \texttt{visualize} the p-value}{5. visualize the p-value}}\label{visualize-the-p-value-1}}


Lastly, in order to compute the \(p\)-value, we have to assess how ``extreme'' the observed difference in means of -1.047 is. We do this by comparing -1.047 to our null distribution, which was constructed in a hypothesized universe of no true difference in movie ratings. Let's visualize both the null distribution and the \(p\)-value in Figure \ref{fig:null-distribution-movies-2}. Unlike our example in Subsection \ref{infer-workflow-ht} involving promotions, since we have a two-sided \(H_A: \mu_a - \mu_r \neq 0\), we have to allow for both possibilities for \emph{more extreme}, so we set \texttt{direction\ =\ "both"}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{visualize}\NormalTok{(null\_distribution\_movies, }\AttributeTok{bins =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{shade\_p\_value}\NormalTok{(}\AttributeTok{obs\_stat =}\NormalTok{ obs\_diff\_means, }\AttributeTok{direction =} \StringTok{"both"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/null-distribution-movies-2-1} 

}

\caption{Null distribution, observed test statistic, and $p$-value.}\label{fig:null-distribution-movies-2}
\end{figure}

Let's go over the elements of this plot. First, the histogram is the \emph{null distribution}. Second, the solid line is the \emph{observed test statistic}, or the difference in sample means we observed in real life of \(5.275 - 6.322 = -1.047\). Third, the two shaded areas of the histogram form the \emph{\(p\)-value}, or the probability of obtaining a test statistic just as or more extreme than the observed test statistic \emph{assuming the null hypothesis \(H_0\) is true}.

What proportion of the null distribution is shaded? In other words, what is the numerical value of the \(p\)-value? We use the \texttt{get\_p\_value()} function to compute this value:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{null\_distribution\_movies }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{get\_p\_value}\NormalTok{(}\AttributeTok{obs\_stat =}\NormalTok{ obs\_diff\_means, }\AttributeTok{direction =} \StringTok{"both"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 1
  p_value
    <dbl>
1   0.004
\end{verbatim}

This \(p\)-value of 0.004 is very small. In other words, there is a very small chance that we'd observe a difference of 5.275 - 6.322 = -1.047 in a hypothesized universe where there was truly no difference in ratings.

But this \(p\)-value is larger than our (even smaller) pre-specified \(\alpha\) significance level of 0.001. Thus, we are inclined to fail to reject the null hypothesis \(H_0: \mu_a - \mu_r = 0\). In non-statistical language, the conclusion is: we do not have the evidence needed in this sample of data to suggest that we should reject the hypothesis that there is no difference in mean IMDb ratings between romance and action movies. We, thus, cannot say that a difference exists in romance and action movie ratings, on average, for all IMDb movies.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC9.9)} Conduct the same analysis comparing action movies versus romantic movies using the median rating instead of the mean rating. What was different and what was the same?

\textbf{(LC9.10)} What conclusions can you make from viewing the faceted histogram looking at \texttt{rating} versus \texttt{genre} that you couldn't see when looking at the boxplot?

\textbf{(LC9.11)} Describe in a paragraph how we used Allen Downey's diagram to conclude if a statistical difference existed between mean movie ratings for action and romance movies.

\textbf{(LC9.12)} Why are we relatively confident that the distributions of the sample ratings will be good approximations of the population distributions of ratings for the two genres?

\textbf{(LC9.13)} Using the definition of \(p\)-value, write in words what the \(p\)-value represents for the hypothesis test comparing the mean rating of romance to action movies.

\textbf{(LC9.14)} What is the value of the \(p\)-value for the hypothesis test comparing the mean rating of romance to action movies?

\textbf{(LC9.15)} Test your data wrangling knowledge and EDA skills:

\begin{itemize}
\tightlist
\item
  Use \texttt{dplyr} and \texttt{tidyr} to create the necessary data frame focused on only action and romance movies (but not both) from the \texttt{movies} data frame in the \texttt{ggplot2movies} package.
\item
  Make a boxplot and a faceted histogram of this population data comparing ratings of action and romance movies from IMDb.
\item
  Discuss how these plots compare to the similar plots produced for the \texttt{movies\_sample} data.
\end{itemize}

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{nhst-conclusion}{%
\section{Conclusion}\label{nhst-conclusion}}

\hypertarget{theory-hypo}{%
\subsection{Theory-based hypothesis tests}\label{theory-hypo}}

Much as we did in Subsection \ref{theory-ci} when we showed you a theory-based method for constructing confidence intervals that involved mathematical formulas, we now present an example of a traditional theory-based method to conduct hypothesis tests. This method relies on probability models, probability distributions, and a few assumptions to construct the null distribution. This is in contrast to the approach we've been using throughout this book where we relied on computer simulations to construct the null distribution.

These traditional theory-based methods have been used for decades mostly because researchers didn't have access to computers that could run thousands of calculations quickly and efficiently. Now that computing power is much cheaper and more accessible, simulation-based methods are much more feasible. However, researchers in many fields continue to use theory-based methods. Hence, we make it a point to include an example here.

As we'll show in this section, any theory-based method is ultimately an approximation to the simulation-based method. The theory-based method we'll focus on is known as the \emph{two-sample \(t\)-test} for testing differences in sample means. However, the test statistic we'll use won't be the difference in sample means \(\overline{x}_1 - \overline{x}_2\), but rather the related \emph{two-sample \(t\)-statistic}. The data we'll use will once again be the \texttt{movies\_sample} data of action and romance movies from Section \ref{ht-case-study}.

\hypertarget{two-sample-t-statistic}{%
\subsubsection*{Two-sample t-statistic}\label{two-sample-t-statistic}}


A common task in statistics is the process of ``standardizing a variable.'' By standardizing different variables, we make them more comparable. For example, say you are interested in studying the distribution of temperature recordings from Portland, Oregon, USA and comparing it to that of the temperature recordings in Montreal, Quebec, Canada. Given that US temperatures are generally recorded in degrees Fahrenheit and Canadian temperatures are generally recorded in degrees Celsius, how can we make them comparable? One approach would be to convert degrees Fahrenheit into Celsius, or vice versa. Another approach would be to convert them both to a common ``standardized'' scale, like Kelvin units of temperature.

One common method for standardizing a variable from probability and statistics theory is to compute the \index{z-score} \(z\)-score:

\[z = \frac{x - \mu}{\sigma}\]

where \(x\) represents one value of a variable, \(\mu\) represents the mean of that variable, and \(\sigma\) represents the standard deviation of that variable. You first subtract the mean \(\mu\) from each value of \(x\) and then divide \(x - \mu\) by the standard deviation \(\sigma\). These operations will have the effect of \emph{re-centering} your variable around 0 and \emph{re-scaling} your variable \(x\) so that they have what are known as ``standard units.'' Thus for every value that your variable can take, it has a corresponding \(z\)-score that gives how many standard units away that value is from the mean \(\mu\). \(z\)-scores are normally distributed with mean 0 and standard deviation 1. This curve is called a ``\(z\)-distribution'' or ``standard normal'' curve and has the common, bell-shaped pattern from Figure \ref{fig:zcurve} discussed in Appendix \ref{appendix-normal-curve}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{HR_Analytics.live_files/figure-latex/zcurve-1} 

}

\caption{Standard normal z curve.}\label{fig:zcurve}
\end{figure}

Bringing these back to the difference of sample mean ratings \(\overline{x}_a - \overline{x}_r\) of action versus romance movies, how would we standardize this variable? By once again subtracting its mean and dividing by its standard deviation. Recall two facts from Subsection \ref{moral-of-the-story}. First, if the sampling was done in a representative fashion, then the sampling distribution of \(\overline{x}_a - \overline{x}_r\) will be centered at the true population parameter \(\mu_a - \mu_r\). Second, the standard deviation of point estimates like \(\overline{x}_a - \overline{x}_r\) has a special name: the standard error.

Applying these ideas, we present the \emph{two-sample \(t\)-statistic}\index{two-sample t-statistic}:

\[t = \dfrac{ (\bar{x}_a - \bar{x}_r) - (\mu_a - \mu_r)}{ \text{SE}_{\bar{x}_a - \bar{x}_r} } = \dfrac{ (\bar{x}_a - \bar{x}_r) - (\mu_a - \mu_r)}{ \sqrt{\dfrac{{s_a}^2}{n_a} + \dfrac{{s_r}^2}{n_r}}  }\]

Oofda! There is a lot to try to unpack here! Let's go slowly. In the numerator, \(\bar{x}_a-\bar{x}_r\) is the difference in sample means, while \(\mu_a - \mu_r\) is the difference in population means. In the denominator, \(s_a\) and \(s_r\) are the \emph{sample standard deviations} of the action and romance movies in our sample \texttt{movies\_sample}. Lastly, \(n_a\) and \(n_r\) are the sample sizes of the action and romance movies. Putting this together under the square root gives us the standard error \(\text{SE}_{\bar{x}_a - \bar{x}_r}\).

Observe that the formula for \(\text{SE}_{\bar{x}_a - \bar{x}_r}\) has the sample sizes \(n_a\) and \(n_r\) in them. So as the sample sizes increase, the standard error goes down. We've seen this concept numerous times now, in particular in our simulations using the three virtual shovels with \(n\) = 25, 50, and 100 slots in Figure \ref{fig:comparing-sampling-distributions-3} and in Subsection \ref{ci-width} where we studied the effect of using larger sample sizes on the widths of confidence intervals.

So how can we use the two-sample \(t\)-statistic as a test statistic in our hypothesis test? First, assuming the null hypothesis \(H_0: \mu_a - \mu_r = 0\) is true, the right-hand side of the numerator (to the right of the \(-\) sign), \(\mu_a - \mu_r\), becomes 0.

Second, similarly to how the Central Limit Theorem from Subsection \ref{sampling-conclusion-central-limit-theorem} states that sample means follow a normal distribution, it can be mathematically proven that the two-sample \(t\)-statistic follows a \emph{\(t\) distribution with degrees of freedom} ``roughly equal'' to \(df = n_a + n_r - 2\). To better understand this concept of \emph{degrees of freedom}, we next display three examples of \(t\)-distributions in Figure \ref{fig:t-distributions} along with the standard normal \(z\) curve.

\begin{figure}

{\centering \includegraphics[width=1\linewidth]{HR_Analytics.live_files/figure-latex/t-distributions-1} 

}

\caption{Examples of t-distributions and the z curve.}\label{fig:t-distributions}
\end{figure}

Begin by looking at the center of the plot at 0 on the horizontal axis. As you move up from the value of 0, follow along with the labels and note that the bottom curve corresponds to 1 degree of freedom, the curve above it is for 3 degrees of freedom, the curve above that is for 10 degrees of freedom, and lastly the dotted curve is the standard normal \(z\) curve.

Observe that all four curves have a bell shape, are centered at 0, and that as the degrees of freedom increase, the \(t\)-distribution more and more resembles the standard normal \(z\) curve. The ``degrees of freedom'' \index{degrees of freedom} measures how different the \(t\) distribution will be from a normal distribution. \(t\)-distributions tend to have more values in the tails of their distributions than the standard normal \(z\) curve.

This ``roughly equal'' statement indicates that the equation \(df = n_a + n_r - 2\) is a ``good enough'' approximation to the true degrees of freedom. The true \href{https://en.wikipedia.org/wiki/Student\%27s_t-test\#Equal_or_unequal_sample_sizes,_unequal_variances}{formula} is a bit more complicated than this simple expression, but we've found the formula to be beyond the reach of those new to statistical inference and it does little to build the intuition of the \(t\)-test.

The message to retain, however, is that small sample sizes lead to small degrees of freedom and thus small sample sizes lead to \(t\)-distributions that are different than the \(z\) curve. On the other hand, large sample sizes correspond to large degrees of freedom and thus produce \(t\) distributions that closely align with the standard normal \(z\)-curve.

So, assuming the null hypothesis \(H_0\) is true, our formula for the test statistic simplifies a bit:

\[t = \dfrac{ (\bar{x}_a - \bar{x}_r) - 0}{ \sqrt{\dfrac{{s_a}^2}{n_a} + \dfrac{{s_r}^2}{n_r}}  } = \dfrac{ \bar{x}_a - \bar{x}_r}{ \sqrt{\dfrac{{s_a}^2}{n_a} + \dfrac{{s_r}^2}{n_r}}  }\]

Let's compute the values necessary for this two-sample \(t\)-statistic. Recall the summary statistics we computed during our exploratory data analysis in Section \ref{imdb-data}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{movies\_sample }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(genre) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{n =} \FunctionTok{n}\NormalTok{(), }\AttributeTok{mean\_rating =} \FunctionTok{mean}\NormalTok{(rating), }\AttributeTok{std\_dev =} \FunctionTok{sd}\NormalTok{(rating))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 4
  genre       n mean_rating std_dev
  <chr>   <int>       <dbl>   <dbl>
1 Action     32     5.275   1.36121
2 Romance    36     6.32222 1.60963
\end{verbatim}

Using these values, the observed two-sample \(t\)-test statistic is

\[
\dfrac{ \bar{x}_a - \bar{x}_r}{ \sqrt{\dfrac{{s_a}^2}{n_a} + \dfrac{{s_r}^2}{n_r}}  } = 
\dfrac{5.28 - 6.32}{ \sqrt{\dfrac{{1.36}^2}{32} + \dfrac{{1.61}^2}{36}}  } = 
-2.906
\]

Great! How can we compute the \(p\)-value using this theory-based test statistic? We need to compare it to a null distribution, which we construct next.

\hypertarget{null-distribution}{%
\subsubsection*{Null distribution}\label{null-distribution}}


Let's revisit the null distribution for the test statistic \(\bar{x}_a - \bar{x}_r\) we constructed in Section \ref{ht-case-study}. Let's visualize this in the left-hand plot of Figure \ref{fig:comparing-diff-means-t-stat}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Construct null distribution of xbar\_a {-} xbar\_r:}
\NormalTok{null\_distribution\_movies }\OtherTok{\textless{}{-}}\NormalTok{ movies\_sample }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ rating }\SpecialCharTok{\textasciitilde{}}\NormalTok{ genre) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{hypothesize}\NormalTok{(}\AttributeTok{null =} \StringTok{"independence"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{, }\AttributeTok{type =} \StringTok{"permute"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"diff in means"}\NormalTok{, }\AttributeTok{order =} \FunctionTok{c}\NormalTok{(}\StringTok{"Action"}\NormalTok{, }\StringTok{"Romance"}\NormalTok{))}
\FunctionTok{visualize}\NormalTok{(null\_distribution\_movies, }\AttributeTok{bins =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The \texttt{infer} package also includes some built-in theory-based test statistics as well. So instead of calculating the test statistic of interest as the \texttt{"diff\ in\ means"} \(\bar{x}_a - \bar{x}_r\), we can calculate this defined two-sample \(t\)-statistic by setting \texttt{stat\ =\ "t"}. Let's visualize this in the right-hand plot of Figure \ref{fig:comparing-diff-means-t-stat}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Construct null distribution of t:}
\NormalTok{null\_distribution\_movies\_t }\OtherTok{\textless{}{-}}\NormalTok{ movies\_sample }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ rating }\SpecialCharTok{\textasciitilde{}}\NormalTok{ genre) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{hypothesize}\NormalTok{(}\AttributeTok{null =} \StringTok{"independence"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{, }\AttributeTok{type =} \StringTok{"permute"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \CommentTok{\# Notice we switched stat from "diff in means" to "t"}
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"t"}\NormalTok{, }\AttributeTok{order =} \FunctionTok{c}\NormalTok{(}\StringTok{"Action"}\NormalTok{, }\StringTok{"Romance"}\NormalTok{))}
\FunctionTok{visualize}\NormalTok{(null\_distribution\_movies\_t, }\AttributeTok{bins =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/comparing-diff-means-t-stat-1} 

}

\caption{Comparing the null distributions of two test statistics.}\label{fig:comparing-diff-means-t-stat}
\end{figure}

Observe that while the shape of the null distributions of both the difference in means \(\bar{x}_a - \bar{x}_r\) and the two-sample \(t\)-statistics are similar, the scales on the x-axis are different. The two-sample \(t\)-statistic values are spread out over a larger range.

However, a traditional theory-based \(t\)-test doesn't look at the simulated histogram in \texttt{null\_distribution\_movies\_t}, but instead it looks at the \(t\)-distribution curve with degrees of freedom equal to roughly 65.85. This calculation is based on the complicated formula referenced previously, which we approximated with \(df = n_a + n_r - 2 = 32 + 36 - 2 = 66\). Let's overlay this \(t\)-distribution curve over the top of our simulated two-sample \(t\)-statistics using the \texttt{method\ =\ "both"} argument in \texttt{visualize()}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{visualize}\NormalTok{(null\_distribution\_movies\_t, }\AttributeTok{bins =} \DecValTok{10}\NormalTok{, }\AttributeTok{method =} \StringTok{"both"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/t-stat-3-1} 

}

\caption{Null distribution using t-statistic and t-distribution.}\label{fig:t-stat-3}
\end{figure}

Observe that the curve does a good job of approximating the histogram here. To calculate the \(p\)-value in this case, we need to figure out how much of the total area under the \(t\)-distribution curve is at or ``more extreme'' than our observed two-sample \(t\)-statistic. Since \(H_A: \mu_a - \mu_r \neq 0\) is a two-sided alternative, we need to add up the areas in both tails.

We first compute the observed two-sample \(t\)-statistic using \texttt{infer} verbs. This shortcut calculation further assumes that the null hypothesis is true: that the population of action and romance movies have an equal average rating.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{obs\_two\_sample\_t }\OtherTok{\textless{}{-}}\NormalTok{ movies\_sample }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ rating }\SpecialCharTok{\textasciitilde{}}\NormalTok{ genre) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"t"}\NormalTok{, }\AttributeTok{order =} \FunctionTok{c}\NormalTok{(}\StringTok{"Action"}\NormalTok{, }\StringTok{"Romance"}\NormalTok{))}
\NormalTok{obs\_two\_sample\_t}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 1
      stat
     <dbl>
1 -2.90589
\end{verbatim}

We want to find the percentage of values that are at or below \texttt{obs\_two\_sample\_t} \(= -2.906\) or at or above \texttt{-obs\_two\_sample\_t} \(= 2.906\). We use the \texttt{shade\_p\_value()} function with the \texttt{direction} argument set to \texttt{"both"} to do this:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{visualize}\NormalTok{(null\_distribution\_movies\_t, }\AttributeTok{method =} \StringTok{"both"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{shade\_p\_value}\NormalTok{(}\AttributeTok{obs\_stat =}\NormalTok{ obs\_two\_sample\_t, }\AttributeTok{direction =} \StringTok{"both"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning: Check to make sure the conditions have been met for the
theoretical method. {infer} currently does not check these for you.
\end{verbatim}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/t-stat-4-1} 

}

\caption{Null distribution using t-statistic and t-distribution with $p$-value shaded.}\label{fig:t-stat-4}
\end{figure}

(We'll discuss this warning message shortly.) What is the \(p\)-value? We apply \texttt{get\_p\_value()} to our null distribution saved in \texttt{null\_distribution\_movies\_t}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{null\_distribution\_movies\_t }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{get\_p\_value}\NormalTok{(}\AttributeTok{obs\_stat =}\NormalTok{ obs\_two\_sample\_t, }\AttributeTok{direction =} \StringTok{"both"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 1
  p_value
    <dbl>
1   0.002
\end{verbatim}

We have a very small \(p\)-value, and thus it is very unlikely that these results are due to \emph{sampling variation}. Thus, we are inclined to reject \(H_0\).

Let's come back to that earlier warning message: \texttt{Check\ to\ make\ sure\ the\ conditions\ have\ been\ met\ for\ the\ theoretical\ method.\ \{infer\}\ currently\ does\ not\ check\ these\ for\ you.} To be able to use the \(t\)-test and other such theoretical methods, there are always a few conditions to check. The \texttt{infer} package does not automatically check these conditions, hence the warning message we received. These conditions are necessary so that the underlying mathematical theory holds. In order for the results of our two-sample \(t\)-test to be valid, three conditions must be met:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Nearly normal populations or large sample sizes. A general rule of thumb that works in many (but not all) situations is that the sample size \(n\) should be greater than 30.
\item
  Both samples are selected independently of each other.
\item
  All observations are independent from each other.
\end{enumerate}

Let's see if these conditions hold for our \texttt{movies\_sample} data:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  This is met since \(n_a\) = 32 and \(n_r\) = 36 are both larger than 30, satisfying our rule of thumb.
\item
  This is met since we sampled the action and romance movies at random and in an unbiased fashion from the database of all IMDb movies.
\item
  Unfortunately, we don't know how IMDb computes the ratings. For example, if the same person rated multiple movies, then those observations would be related and hence not independent.
\end{enumerate}

Assuming all three conditions are roughly met, we can be reasonably certain that the theory-based \(t\)-test results are valid. If any of the conditions were clearly not met, we couldn't put as much trust into any conclusions reached. On the other hand, in most scenarios, the only assumption that needs to be met in the simulation-based method is that the sample is selected at random. Thus, in our experience, we prefer simulation-based methods as they have fewer assumptions, are conceptually easier to understand, and since computing power has recently become easily accessible, they can be run quickly. That being said since much of the world's research still relies on traditional theory-based methods, we also believe it is important to understand them.

You may be wondering why we chose \texttt{reps\ =\ 1000} for these simulation-based methods. We've noticed that after around 1000 replicates for the null distribution and the bootstrap distribution for most problems you can start to get a general sense for how the statistic behaves. You can change this value to something like 10,000 though for \texttt{reps} if you would like even finer detail but this will take more time to compute. Feel free to iterate on this as you like to get an even better idea about the shape of the null and bootstrap distributions as you wish.

\hypertarget{when-inference-is-not-needed}{%
\subsection{When inference is not needed}\label{when-inference-is-not-needed}}

We've now walked through several different examples of how to use the \texttt{infer} package to perform statistical inference: constructing confidence intervals and conducting hypothesis tests. For each of these examples, we made it a point to always perform an exploratory data analysis (EDA) first; specifically, by looking at the raw data values, by using data visualization with \texttt{ggplot2}, and by data wrangling with \texttt{dplyr} beforehand. We \emph{highly} encourage you to always do the same. As a beginner to statistics, EDA helps you develop intuition as to what statistical methods like confidence intervals and hypothesis tests can tell us. Even as a seasoned practitioner of statistics, EDA helps guide your statistical investigations. In particular, is statistical inference even needed?

Let's consider an example. Say we're interested in the following question: Of \emph{all} flights leaving a New York City airport, are Hawaiian Airlines flights in the air for longer than Alaska Airlines flights? Furthermore, let's assume that 2013 flights are a representative sample of all such flights. Then we can use the \texttt{flights} data frame in the \texttt{nycflights13} \index{R packages!nycflights13} package we introduced in Section \ref{nycflights13} to answer our question. Let's filter this data frame to only include Hawaiian and Alaska Airlines using their \texttt{carrier} codes \texttt{HA} and \texttt{AS}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights\_sample }\OtherTok{\textless{}{-}}\NormalTok{ flights }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(carrier }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\StringTok{"HA"}\NormalTok{, }\StringTok{"AS"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

There are two possible statistical inference methods we could use to answer such questions. First, we could construct a 95\% confidence interval for the difference in population means \(\mu_{HA} - \mu_{AS}\), where \(\mu_{HA}\) is the mean air time of all Hawaiian Airlines flights and \(\mu_{AS}\) is the mean air time of all Alaska Airlines flights. We could then check if the entirety of the interval is greater than 0, suggesting that \(\mu_{HA} - \mu_{AS} > 0\), or, in other words suggesting that \(\mu_{HA} > \mu_{AS}\). Second, we could perform a hypothesis test of the null hypothesis \(H_0: \mu_{HA} - \mu_{AS} = 0\) versus the alternative hypothesis \(H_A: \mu_{HA} - \mu_{AS} > 0\).

However, let's first construct an exploratory visualization as we suggested earlier. Since \texttt{air\_time} is numerical and \texttt{carrier} is categorical, a boxplot can display the relationship between these two variables, which we display in Figure \ref{fig:ha-as-flights-boxplot}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ flights\_sample, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ carrier, }\AttributeTok{y =}\NormalTok{ air\_time)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Carrier"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Air Time"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/ha-as-flights-boxplot-1} 

}

\caption{Air time for Hawaiian and Alaska Airlines flights departing NYC in 2013.}\label{fig:ha-as-flights-boxplot}
\end{figure}

This is what we like to call ``no PhD in Statistics needed'' moments. You don't have to be an expert in statistics to know that Alaska Airlines and Hawaiian Airlines have \emph{significantly} different air times. The two boxplots don't even overlap! Constructing a confidence interval or conducting a hypothesis test would frankly not provide much more insight than Figure \ref{fig:ha-as-flights-boxplot}.

Let's investigate why we observe such a clear cut difference between these two airlines using data wrangling. Let's first group by the rows of \texttt{flights\_sample} not only by \texttt{carrier} but also by destination \texttt{dest}. Subsequently, we'll compute two summary statistics: the number of observations using \texttt{n()} and the mean airtime:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{flights\_sample }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(carrier, dest) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{n =} \FunctionTok{n}\NormalTok{(), }\AttributeTok{mean\_time =} \FunctionTok{mean}\NormalTok{(air\_time, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 4
# Groups:   carrier [2]
  carrier dest      n mean_time
  <chr>   <chr> <int>     <dbl>
1 AS      SEA     714   325.618
2 HA      HNL     342   623.088
\end{verbatim}

It turns out that from New York City in 2013, Alaska only flew to \texttt{SEA} (Seattle) from New York City (NYC) while Hawaiian only flew to \texttt{HNL} (Honolulu) from NYC. Given the clear difference in distance from New York City to Seattle versus New York City to Honolulu, it is not surprising that we observe such different (\emph{statistically significantly different}, in fact) air times in flights.

This is a clear example of not needing to do anything more than a simple exploratory data analysis using data visualization and descriptive statistics to get an appropriate conclusion. This is why we highly recommend you perform an EDA of any sample data before running statistical inference methods like confidence intervals and hypothesis tests.

\hypertarget{problems-with-p-values}{%
\subsection{Problems with p-values}\label{problems-with-p-values}}

On top of the many common misunderstandings about hypothesis testing and \(p\)-values we listed in Section \ref{ht-interpretation}, another unfortunate consequence of the expanded use of \(p\)-values and hypothesis testing is a phenomenon known as ``p-hacking.'' \index{p-hacking} p-hacking is the act of ``cherry-picking'' only results that are ``statistically significant'' while dismissing those that aren't, even if at the expense of the scientific ideas. There are lots of articles written recently about misunderstandings and the problems with \(p\)-values. We encourage you to check some of them out:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \href{https://en.wikipedia.org/wiki/Misunderstandings_of_p-values}{Misunderstandings of \(p\)-values}
\item
  \href{https://www.vox.com/science-and-health/2017/7/31/16021654/p-values-statistical-significance-redefine-0005}{What a nerdy debate about \(p\)-values shows about science - and how to fix it}
\item
  \href{https://www.nature.com/news/statisticians-issue-warning-over-misuse-of-p-values-1.19503}{Statisticians issue warning over misuse of \(P\) values}
\item
  \href{https://fivethirtyeight.com/features/you-cant-trust-what-you-read-about-nutrition/}{You Can't Trust What You Read About Nutrition}
\item
  \href{http://www.fharrell.com/post/pval-litany/}{A Litany of Problems with p-values}
\end{enumerate}

Such issues were getting so problematic that the American Statistical Association (ASA) put out a statement in 2016 titled, \href{https://www.amstat.org/asa/files/pdfs/P-ValueStatement.pdf}{``The ASA Statement on Statistical Significance and \(P\)-Values,''} with six principles underlying the proper use and interpretation of \(p\)-values. The ASA released this guidance on \(p\)-values to improve the conduct and interpretation of quantitative science and to inform the growing emphasis on reproducibility of science research.

We as authors much prefer the use of confidence intervals for statistical inference, since in our opinion they are much less prone to large misinterpretation. However, many fields still exclusively use \(p\)-values for statistical inference and this is one reason for including them in this text. We encourage you to learn more about ``p-hacking'' as well and its implication for science.

\hypertarget{additional-resources-7}{%
\subsection{Additional resources}\label{additional-resources-7}}

Solutions to all \emph{Learning checks} can be found online in \href{https://moderndive.com/D-appendixD.html}{Appendix D}.

An R script file of all R code used in this chapter is available at \url{https://www.moderndive.com/scripts/09-hypothesis-testing.R}.

If you want more examples of the \texttt{infer} workflow for conducting hypothesis tests, we suggest you check out the \texttt{infer} package homepage, in particular, a series of example analyses available at \url{https://infer.netlify.app/articles/}.

\hypertarget{whats-to-come-8}{%
\subsection{What's to come}\label{whats-to-come-8}}

We conclude with the \texttt{infer} pipeline for hypothesis testing in Figure \ref{fig:infer-workflow-ht}.

\begin{figure}

{\centering \includegraphics[width=1\linewidth,height=1\textheight]{images/flowcharts/infer/ht_diagram_trimmed} 

}

\caption{infer package workflow for hypothesis testing.}\label{fig:infer-workflow-ht}
\end{figure}

Now that we've armed ourselves with an understanding of confidence intervals from Chapter \ref{confidence-intervals} and hypothesis tests from this chapter, we'll now study inference for regression in the upcoming Chapter \ref{inference-for-regression}.

We'll revisit the regression models we studied in Chapter \ref{regression} on basic regression and Chapter \ref{multiple-regression} on multiple regression. For example, recall Table \ref{tab:regtable} (shown again here in Table \ref{tab:regression-table-inference}), corresponding to our regression model for an instructor's teaching score as a function of their ``beauty'' score.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit regression model:}
\NormalTok{score\_model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(score }\SpecialCharTok{\textasciitilde{}}\NormalTok{ bty\_avg, }\AttributeTok{data =}\NormalTok{ evals)}

\CommentTok{\# Get regression table:}
\FunctionTok{get\_regression\_table}\NormalTok{(score\_model)}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]

\caption{\label{tab:regression-table-inference}Linear regression table}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{lrrrrrr}
\toprule
term & estimate & std\_error & statistic & p\_value & lower\_ci & upper\_ci\\
\midrule
intercept & 3.880 & 0.076 & 50.96 & 0 & 3.731 & 4.030\\
bty\_avg & 0.067 & 0.016 & 4.09 & 0 & 0.035 & 0.099\\
\bottomrule
\end{tabular}
\end{table}

We previously saw in Subsection \ref{model1table} that the values in the \texttt{estimate} column are the fitted intercept \(b_0\) and fitted slope for beauty score \(b_1\). In Chapter \ref{inference-for-regression}, we'll unpack the remaining columns: \texttt{std\_error} which is the standard error, \texttt{statistic} which is the observed \emph{standardized} test statistic to compute the \texttt{p\_value}, and the 95\% confidence intervals as given by \texttt{lower\_ci} and \texttt{upper\_ci}.

\hypertarget{inference-for-regression}{%
\chapter{Inference for Regression}\label{inference-for-regression}}

In our penultimate chapter, we'll revisit the regression models we first studied in Chapters \ref{regression} and \ref{multiple-regression}. Armed with our knowledge of confidence intervals and hypothesis tests from Chapters \ref{confidence-intervals} and \ref{hypothesis-testing}, we'll be able to apply statistical inference to further our understanding of relationships between outcome and explanatory variables.

\hypertarget{inf-packages}{%
\subsection*{Needed packages}\label{inf-packages}}


Let's load all the packages needed for this chapter (this assumes you've already installed them). Recall from our discussion in Section \ref{tidyverse-package} that loading the \texttt{tidyverse} package by running \texttt{library(tidyverse)} loads the following commonly used data science packages all at once:

\begin{itemize}
\tightlist
\item
  \texttt{ggplot2} for data visualization
\item
  \texttt{dplyr} for data wrangling
\item
  \texttt{tidyr} for converting data to ``tidy'' format
\item
  \texttt{readr} for importing spreadsheet data into R
\item
  As well as the more advanced \texttt{purrr}, \texttt{tibble}, \texttt{stringr}, and \texttt{forcats} packages
\end{itemize}

If needed, read Section \ref{packages} for information on how to install and load R packages.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(moderndive)}
\FunctionTok{library}\NormalTok{(infer)}
\end{Highlighting}
\end{Shaded}

\hypertarget{regression-refresher}{%
\section{Regression refresher}\label{regression-refresher}}

Before jumping into inference for regression, let's remind ourselves of the University of Texas Austin teaching evaluations analysis in Section \ref{model1}.

\hypertarget{teaching-evaluations-analysis}{%
\subsection{Teaching evaluations analysis}\label{teaching-evaluations-analysis}}

Recall using simple linear regression \index{regression!simple linear} we modeled the relationship between

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A numerical outcome variable \(y\) (the instructor's teaching score) and
\item
  A single numerical explanatory variable \(x\) (the instructor's ``beauty'' score).
\end{enumerate}

We first created an \texttt{evals\_ch5} data frame that selected a subset of variables from the \texttt{evals} data frame included in the \texttt{moderndive} package. This \texttt{evals\_ch5} data frame contains only the variables of interest for our analysis, in particular the instructor's teaching \texttt{score} and the ``beauty'' rating \texttt{bty\_avg}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{evals\_ch5 }\OtherTok{\textless{}{-}}\NormalTok{ evals }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(ID, score, bty\_avg, age)}
\FunctionTok{glimpse}\NormalTok{(evals\_ch5)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 463
Columns: 4
$ ID      <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,...
$ score   <dbl> 4.7, 4.1, 3.9, 4.8, 4.6, 4.3, 2.8, 4.1, 3.4, 4.5, 3.8,...
$ bty_avg <dbl> 5.00, 5.00, 5.00, 5.00, 3.00, 3.00, 3.00, 3.33, 3.33, ...
$ age     <int> 36, 36, 36, 36, 59, 59, 59, 51, 51, 40, 40, 40, 40, 40...
\end{verbatim}

In Subsection \ref{model1EDA}, we performed an exploratory data analysis of the relationship between these two variables of \texttt{score} and \texttt{bty\_avg}. We saw there that a weakly positive correlation of 0.187 existed between the two variables.

This was evidenced in Figure \ref{fig:regline} of the scatterplot along with the ``best-fitting'' regression line that summarizes the linear relationship between the two variables of \texttt{score} and \texttt{bty\_avg}. Recall in Subsection \ref{leastsquares} that we defined a ``best-fitting'' line as the line that minimizes the \emph{sum of squared residuals}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(evals\_ch5, }
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ bty\_avg, }\AttributeTok{y =}\NormalTok{ score)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Beauty Score"}\NormalTok{, }
       \AttributeTok{y =} \StringTok{"Teaching Score"}\NormalTok{,}
       \AttributeTok{title =} \StringTok{"Relationship between teaching and beauty scores"}\NormalTok{) }\SpecialCharTok{+}  
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/regline-1} 

}

\caption{Relationship with regression line.}\label{fig:regline}
\end{figure}

Looking at this plot again, you might be asking, ``Does that line really have all that positive of a slope?''. It does increase from left to right as the \texttt{bty\_avg} variable increases, but by how much? To get to this information, recall that we followed a two-step procedure:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  We first ``fit'' the linear regression model using the \texttt{lm()} function with the formula \texttt{score\ \textasciitilde{}\ bty\_avg}. We saved this model in \texttt{score\_model}.
\item
  We get the regression table by applying the \texttt{get\_regression\_table()}\index{moderndive!get\_regression\_table()} function from the \texttt{moderndive} package to \texttt{score\_model}.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit regression model:}
\NormalTok{score\_model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(score }\SpecialCharTok{\textasciitilde{}}\NormalTok{ bty\_avg, }\AttributeTok{data =}\NormalTok{ evals\_ch5)}
\CommentTok{\# Get regression table:}
\FunctionTok{get\_regression\_table}\NormalTok{(score\_model)}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]

\caption{\label{tab:regtable-11}Previously seen linear regression table}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{lrrrrrr}
\toprule
term & estimate & std\_error & statistic & p\_value & lower\_ci & upper\_ci\\
\midrule
intercept & 3.880 & 0.076 & 50.96 & 0 & 3.731 & 4.030\\
bty\_avg & 0.067 & 0.016 & 4.09 & 0 & 0.035 & 0.099\\
\bottomrule
\end{tabular}
\end{table}

Using the values in the \texttt{estimate} column of the resulting regression table in Table \ref{tab:regtable-11}, we could then obtain the equation of the ``best-fitting'' regression line in Figure \ref{fig:regline}:

\[
\begin{aligned}
\widehat{y} &= b_0 + b_1 \cdot x\\
\widehat{\text{score}} &= b_0 + b_{\text{bty}\_\text{avg}} \cdot\text{bty}\_\text{avg}\\
&= 3.880 + 0.067\cdot\text{bty}\_\text{avg}
\end{aligned}
\]

where \(b_0\) is the fitted intercept and \(b_1\) is the fitted slope for \texttt{bty\_avg}. Recall the interpretation of the \(b_1\) = 0.067 value of the fitted slope:

\begin{quote}
For every increase of one unit in ``beauty'' rating, there is an associated increase, on average, of 0.067 units of evaluation score.
\end{quote}

Thus, the slope value quantifies the relationship between the \(y\) variable \texttt{score} and the \(x\) variable \texttt{bty\_avg}. We also discussed the intercept value of \(b_0\) = 3.88 and its lack of practical interpretation, since the range of possible ``beauty'' scores does not include 0.

\hypertarget{sampling-scenario-2}{%
\subsection{Sampling scenario}\label{sampling-scenario-2}}

Let's now revisit this study in terms of the terminology and notation related to sampling we studied in Subsection \ref{terminology-and-notation}.

First, let's view the instructors for these 463 courses as a \emph{representative sample} from a greater \emph{study population}. In our case, let's assume that the study population is \emph{all} instructors at UT Austin and that the sample of instructors who taught these 463 courses is a representative sample. Unfortunately, we can only \emph{assume} these two facts without more knowledge of the \emph{sampling methodology}\index{sampling methodology} used by the researchers.

Since we are viewing these \(n\) = 463 courses as a sample, we can view our fitted slope \(b_1\) = 0.067 as a \emph{point estimate} of the \emph{population slope} \(\beta_1\). In other words, \(\beta_1\) quantifies the relationship between teaching \texttt{score} and ``beauty'' average \texttt{bty\_avg} for \emph{all} instructors at UT Austin. Similarly, we can view our fitted intercept \(b_0\) = 3.88 as a \emph{point estimate} of the \emph{population intercept} \(\beta_0\) for \emph{all} instructors at UT Austin.

Putting these two ideas together, we can view the equation of the fitted line \(\widehat{y}\) = \(b_0 + b_1 \cdot x\) = \(3.880 + 0.067 \cdot \text{bty}\_\text{avg}\) as an estimate of some true and unknown \emph{population line} \(y = \beta_0 + \beta_1 \cdot x\). Thus we can draw parallels between our teaching evaluations analysis and all the sampling scenarios we've seen previously. In this chapter, we'll focus on the final scenario of regression slopes as shown in Table \ref{tab:summarytable-ch11}.

\begin{table}[!h]

\caption{\label{tab:summarytable-ch11}Scenarios of sampling for inference}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{>{\raggedleft\arraybackslash}p{0.5in}>{\raggedright\arraybackslash}p{0.7in}>{\raggedright\arraybackslash}p{1in}>{\raggedright\arraybackslash}p{1.1in}>{\raggedright\arraybackslash}p{1in}}
\toprule
Scenario & Population parameter & Notation & Point estimate & Symbol(s)\\
\midrule
1 & Population proportion & $p$ & Sample proportion & $\widehat{p}$\\
2 & Population mean & $\mu$ & Sample mean & $\overline{x}$ or $\widehat{\mu}$\\
3 & Difference in population proportions & $p_1 - p_2$ & Difference in sample proportions & $\widehat{p}_1 - \widehat{p}_2$\\
4 & Difference in population means & $\mu_1 - \mu_2$ & Difference in sample means & $\overline{x}_1 - \overline{x}_2$\\
5 & Population regression slope & $\beta_1$ & Fitted regression slope & $b_1$ or $\widehat{\beta}_1$\\
\bottomrule
\end{tabular}
\end{table}

Since we are now viewing our fitted slope \(b_1\) and fitted intercept \(b_0\) as \emph{point estimates} based on a \emph{sample}, these estimates will again be subject to \emph{sampling variability}. In other words, if we collected a new sample of data on a different set of \(n\) = 463 courses and their instructors, the new fitted slope \(b_1\) will likely differ from 0.067. The same goes for the new fitted intercept \(b_0\). But by how much will these estimates \emph{vary}? This information is in the remaining columns of the regression table in Table \ref{tab:regtable-11}. Our knowledge of sampling from Chapter \ref{sampling}, confidence intervals from Chapter \ref{confidence-intervals}, and hypothesis tests from Chapter \ref{hypothesis-testing} will help us interpret these remaining columns.

\hypertarget{regression-interp}{%
\section{Interpreting regression tables}\label{regression-interp}}

We've so far focused only on the two leftmost columns of the regression table in Table \ref{tab:regtable-11}: \texttt{term} and \texttt{estimate}. Let's now shift our attention to the remaining columns: \texttt{std\_error}, \texttt{statistic}, \texttt{p\_value}, \texttt{lower\_ci} and \texttt{upper\_ci} in Table \ref{tab:score-model-part-deux}.

\begin{table}[!h]

\caption{\label{tab:score-model-part-deux}Previously seen regression table}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{lrrrrrr}
\toprule
term & estimate & std\_error & statistic & p\_value & lower\_ci & upper\_ci\\
\midrule
intercept & 3.880 & 0.076 & 50.96 & 0 & 3.731 & 4.030\\
bty\_avg & 0.067 & 0.016 & 4.09 & 0 & 0.035 & 0.099\\
\bottomrule
\end{tabular}
\end{table}

Given the lack of practical interpretation for the fitted intercept \(b_0\), in this section we'll focus only on the second row of the table corresponding to the fitted slope \(b_1\). We'll first interpret the \texttt{std\_error}, \texttt{statistic}, \texttt{p\_value}, \texttt{lower\_ci} and \texttt{upper\_ci} columns. Afterwards in the upcoming Subsection \ref{regression-table-computation}, we'll discuss how R computes these values.

\hypertarget{regression-se}{%
\subsection{Standard error}\label{regression-se}}

The third column of the regression table in Table \ref{tab:regtable-11} \texttt{std\_error} corresponds to the \emph{standard error} of our estimates. Recall the definition of \textbf{standard error} \index{standard error} we saw in Subsection \ref{sampling-definitions}:

\begin{quote}
The \emph{standard error} is the standard deviation of any point estimate computed from a sample.
\end{quote}

So what does this mean in terms of the fitted slope \(b_1\) = 0.067? This value is just one possible value of the fitted slope resulting from \emph{this particular sample} of \(n\) = 463 pairs of teaching and beauty scores. However, if we collected a different sample of \(n\) = 463 pairs of teaching and beauty scores, we will almost certainly obtain a different fitted slope \(b_1\). This is due to \emph{sampling variability}.

Say we hypothetically collected 1000 such samples of pairs of teaching and beauty scores, computed the 1000 resulting values of the fitted slope \(b_1\), and visualized them in a histogram. This would be a visualization of the \emph{sampling distribution} of \(b_1\), which we defined in Subsection \ref{sampling-definitions}. Further recall that the standard deviation of the \emph{sampling distribution} of \(b_1\) has a special name: the \emph{standard error}.

Recall that we constructed three sampling distributions for the sample proportion \(\widehat{p}\) using shovels of size 25, 50, and 100 in Figure \ref{fig:comparing-sampling-distributions}. We observed that as the sample size increased, the standard error decreased as evidenced by the narrowing sampling distribution.

The \emph{standard error} of \(b_1\) similarly quantifies how much variation in the fitted slope \(b_1\) one would expect between different samples. So in our case, we can expect about 0.016 units of variation in the \texttt{bty\_avg} slope variable. Recall that the \texttt{estimate} and \texttt{std\_error} values play a key role in \emph{inferring} the value of the unknown population slope \(\beta_1\) relating to \emph{all} instructors.

In Section \ref{infer-regression}, we'll perform a simulation using the \texttt{infer} package to construct the bootstrap distribution for \(b_1\) in this case. Recall from Subsection \ref{bootstrap-vs-sampling} that the bootstrap distribution is an \emph{approximation} to the sampling distribution in that they have a similar shape. Since they have a similar shape, they have similar \emph{standard errors}. However, unlike the sampling distribution, the bootstrap distribution is constructed from a \emph{single} sample, which is a practice more aligned with what's done in real life.

\hypertarget{regression-test-statistic}{%
\subsection{Test statistic}\label{regression-test-statistic}}

The fourth column of the regression table in Table \ref{tab:regtable-11} \texttt{statistic} corresponds to a \emph{test statistic} relating to the following \emph{hypothesis test}:

\[
\begin{aligned}
H_0 &: \beta_1 = 0\\
\text{vs } H_A&: \beta_1 \neq 0.
\end{aligned}
\]

Recall our terminology, notation, and definitions related to hypothesis tests we introduced in Section \ref{understanding-ht}.

\begin{quote}
A \emph{hypothesis test} consists of a test between two competing hypotheses: (1) a \emph{null hypothesis} \(H_0\) versus (2) an \emph{alternative hypothesis} \(H_A\).

A \emph{test statistic} is a point estimate/sample statistic formula used for hypothesis testing.
\end{quote}

Here, our \emph{null hypothesis} \(H_0\) assumes that the population slope \(\beta_1\) is 0. If the population slope \(\beta_1\) is truly 0, then this is saying that there is \emph{no true relationship} between teaching and ``beauty'' scores for \emph{all} the instructors in our population. In other words, \(x\) = ``beauty'' score would have no associated effect on \(y\) = teaching score.
The \emph{alternative hypothesis} \(H_A\), on the other hand, assumes that the population slope \(\beta_1\) is not 0, meaning it could be either positive or negative. This suggests either a positive or negative relationship between teaching and ``beauty'' scores. Recall we called such alternative hypotheses \emph{two-sided}. By convention, all hypothesis testing for regression assumes two-sided alternatives.

Recall our ``hypothesized universe'' of no gender discrimination we \emph{assumed} in our \texttt{promotions} activity in Section \ref{ht-activity}. Similarly here when conducting this hypothesis test, we'll assume a ``hypothesized universe'' where there is no relationship between teaching and ``beauty'' scores. In other words, we'll assume the null hypothesis \(H_0: \beta_1 = 0\) is true.

The \texttt{statistic} column in the regression table is a tricky one, however. It corresponds to a standardized \emph{t-test statistic}, much like the \emph{two-sample \(t\) statistic} we saw in Subsection \ref{theory-hypo} where we used a theory-based method for conducting hypothesis tests. In both these cases, the \emph{null distribution} can be mathematically proven to be a \emph{\(t\)-distribution}. Since such test statistics are tricky for individuals new to statistical inference to study, we'll skip this and jump into interpreting the \(p\)-value. If you're curious, we have included a discussion of this standardized \emph{t-test statistic} in Subsection \ref{theory-regression}.

\hypertarget{p-value}{%
\subsection{p-value}\label{p-value}}

The fifth column of the regression table in Table \ref{tab:regtable-11} \texttt{p\_value} corresponds to the \emph{p-value} of the hypothesis test \(H_0: \beta_1 = 0\) versus \(H_A: \beta_1 \neq 0\).

Again recalling our terminology, notation, and definitions related to hypothesis tests we introduced in Section \ref{understanding-ht}, let's focus on the definition of the \(p\)-value:

\begin{quote}
A \emph{p-value} is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic \emph{assuming the null hypothesis \(H_0\) is true}.
\end{quote}

Recall that you can intuitively think of the \(p\)-value as quantifying how ``extreme'' the observed fitted slope of \(b_1\) = 0.067 is in a ``hypothesized universe'' where there is no relationship between teaching and ``beauty'' scores.

Following the hypothesis testing procedure we outlined in Section \ref{ht-interpretation}, since the \(p\)-value in this case is 0, for any choice of significance level \(\alpha\) we would reject \(H_0\) in favor of \(H_A\). Using non-statistical language, this is saying: we reject the hypothesis that there is no relationship between teaching and ``beauty'' scores in favor of the hypothesis that there is. That is to say, the evidence suggests there is a significant relationship, one that is positive.

More precisely, however, the \(p\)-value corresponds to how extreme the observed test statistic of 4.09 is when compared to the appropriate \emph{null distribution}. In Section \ref{infer-regression}, we'll perform a simulation using the \texttt{infer} package to construct the null distribution in this case.

An extra caveat here is that the results of this hypothesis test are only valid if certain ``conditions for inference for regression'' are met, which we'll introduce shortly in Section \ref{regression-conditions}.

\hypertarget{confidence-interval}{%
\subsection{Confidence interval}\label{confidence-interval}}

The two rightmost columns of the regression table in Table \ref{tab:regtable-11} (\texttt{lower\_ci} and \texttt{upper\_ci}) correspond to the endpoints of the 95\% \emph{confidence interval} for the population slope \(\beta_1\). Recall our analogy of ``nets are to fish'' what ``confidence intervals are to population parameters'' from Section \ref{ci-build-up}. The resulting 95\% confidence interval for \(\beta_1\) of (0.035, 0.099) can be thought of as a range of plausible values for the population slope \(\beta_1\) of the linear relationship between teaching and ``beauty'' scores.

As we introduced in Subsection \ref{shorthand} on the precise and shorthand interpretation of confidence intervals, the statistically precise interpretation of this confidence interval is: ``if we repeated this sampling procedure a large number of times, we expect about 95\% of the resulting confidence intervals to capture the value of the population slope \(\beta_1\).'' However, we'll summarize this using our shorthand interpretation that ``we're 95\% `confident' that the true population slope \(\beta_1\) lies between 0.035 and 0.099.''

Notice in this case that the resulting 95\% confidence interval for \(\beta_1\) of \((0.035, \, 0.099)\) does not contain a very particular value: \(\beta_1\) equals 0. Recall we mentioned that if the population regression slope \(\beta_1\) is 0, this is equivalent to saying there is \emph{no} relationship between teaching and ``beauty'' scores. Since \(\beta_1\) = 0 is not in our plausible range of values for \(\beta_1\), we are inclined to believe that there, in fact, \emph{is} a relationship between teaching and ``beauty'' scores and a positive one at that. So in this case, the conclusion about the population slope \(\beta_1\) from the 95\% confidence interval matches the conclusion from the hypothesis test: evidence suggests that there is a meaningful relationship between teaching and ``beauty'' scores.

Recall from Subsection \ref{ci-width}, however, that the \emph{confidence level} is one of many factors that determine confidence interval widths. So for example, say we used a higher confidence level of 99\% instead of 95\%. The resulting confidence interval for \(\beta_1\) would be wider and thus might now include 0. The lesson to remember here is that any confidence-interval-based conclusion depends highly on the confidence level used.

What are the calculations that went into computing the two endpoints of the 95\% confidence interval for \(\beta_1\)?

Recall our sampling bowl example from Subsection \ref{theory-ci} discussing \texttt{lower\_ci} and \texttt{upper\_ci}. Since the sampling and bootstrap distributions of the sample proportion \(\widehat{p}\) were roughly normal, we could use the rule of thumb for bell-shaped distributions from Appendix \ref{appendix-normal-curve} to create a 95\% confidence interval for \(p\) with the following equation:

\[\widehat{p} \pm \text{MoE}_{\widehat{p}} = \widehat{p} \pm 1.96 \cdot \text{SE}_{\widehat{p}} = \widehat{p} \pm 1.96 \cdot \sqrt{\frac{\widehat{p}(1-\widehat{p})}{n}}\]

We can generalize this to other point estimates that have roughly normally shaped sampling and/or bootstrap distributions:

\[\text{point estimate} \pm \text{MoE} = \text{point estimate} \pm 1.96 \cdot \text{SE}.\]

We'll show in Section \ref{infer-regression} that the sampling/bootstrap distribution for the fitted slope \(b_1\) is in fact bell-shaped as well. Thus we can construct a 95\% confidence interval for \(\beta_1\) with the following equation:

\[b_1 \pm \text{MoE}_{b_1} = b_1 \pm 1.96 \cdot \text{SE}_{b_1}.\]

What is the value of the standard error \(\text{SE}_{b_1}\)? It is in fact in the third column of the regression table in Table \ref{tab:regtable-11}: 0.016. Thus

\[
\begin{aligned}
b_1 \pm 1.96 \cdot \text{SE}_{b_1} &= 0.067 \pm 1.96 \cdot 0.016 = 0.067 \pm 0.031\\
&= (0.036, 0.098)
\end{aligned}
\]

This closely matches the \((0.035, 0.099)\) confidence interval in the last two columns of Table \ref{tab:regtable-11}.

Much like hypothesis tests, however, the results of this confidence interval also are only valid if the ``conditions for inference for regression'' to be discussed in Section \ref{regression-conditions} are met.

\hypertarget{regression-table-computation}{%
\subsection{How does R compute the table?}\label{regression-table-computation}}

Since we didn't perform the simulation to get the values of the standard error, test statistic, \(p\)-value, and endpoints of the 95\% confidence interval in Table \ref{tab:regtable-11}, you might be wondering how were these values computed. What did R do behind the scenes? Does R run simulations like we did using the \texttt{infer} package in Chapters \ref{confidence-intervals} and \ref{hypothesis-testing} on confidence intervals and hypothesis testing?

The answer is no! Much like the theory-based method for constructing confidence intervals you saw in Subsection \ref{theory-ci} and the theory-based hypothesis test you saw in Subsection \ref{theory-hypo}, there exist mathematical formulas that allow you to construct confidence intervals and conduct hypothesis tests for inference for regression. These formulas were derived in a time when computers didn't exist, so it would've been impossible to run the extensive computer simulations we have in this book. We present these formulas in Subsection \ref{theory-regression} on ``theory-based inference for regression.''

In Section \ref{infer-regression}, we'll go over a simulation-based approach to constructing confidence intervals and conducting hypothesis tests using the \texttt{infer} package. In particular, we'll convince you that the bootstrap distribution of the fitted slope \(b_1\) is indeed bell-shaped.

\hypertarget{regression-conditions}{%
\section{Conditions for inference for regression}\label{regression-conditions}}

Recall in Subsection \ref{se-method} we stated that we could only use the standard-error-based method for constructing confidence intervals if the bootstrap distribution was bell shaped. Similarly, there are certain conditions that need to be met in order for the results of our hypothesis tests and confidence intervals we described in Section \ref{regression-interp} to have valid meaning. These conditions must be met for the assumed underlying mathematical and probability theory to hold true.

For inference for regression, there are four conditions that need to be met. Note the first four letters of these conditions are highlighted in bold in what follows: \textbf{LINE}. This can serve as a nice reminder of what to check for whenever you perform linear regression. \index{regression!conditions for inference (LINE)}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{L}inearity of relationship between variables
\item
  \textbf{I}ndependence of the residuals
\item
  \textbf{N}ormality of the residuals
\item
  \textbf{E}quality of variance of the residuals
\end{enumerate}

Conditions \textbf{L}, \textbf{N}, and \textbf{E} can be verified through what is known as a \emph{residual analysis}.\index{residual analysis} Condition \textbf{I} can only be verified through an understanding of how the data was collected.

In this section, we'll go over a refresher on residuals, verify whether each of the four \textbf{LINE} conditions hold true, and then discuss the implications.

\hypertarget{residuals-refresher}{%
\subsection{Residuals refresher}\label{residuals-refresher}}

Recall our definition of a residual from Subsection \ref{model1points}: it is the \emph{observed value} minus the \emph{fitted value} denoted by \(y - \widehat{y}\). Recall that residuals can be thought of as the error or the ``lack-of-fit'' between the observed value \(y\) and the fitted value \(\widehat{y}\) on the regression line in Figure \ref{fig:regline}. In Figure \ref{fig:residual-example}, we illustrate one particular residual out of 463 using an arrow, as well as its corresponding observed and fitted values using a circle and a square, respectively.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/residual-example-1} 

}

\caption{Example of observed value, fitted value, and residual.}\label{fig:residual-example}
\end{figure}

Furthermore, we can automate the calculation of all \(n\) = 463 residuals by applying the \texttt{get\_regression\_points()} function to our saved regression model in \texttt{score\_model}. Observe how the resulting values of \texttt{residual} are roughly equal to \texttt{score\ -\ score\_hat} (there is potentially a slight difference due to rounding error).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit regression model:}
\NormalTok{score\_model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(score }\SpecialCharTok{\textasciitilde{}}\NormalTok{ bty\_avg, }\AttributeTok{data =}\NormalTok{ evals\_ch5)}
\CommentTok{\# Get regression points:}
\NormalTok{regression\_points }\OtherTok{\textless{}{-}} \FunctionTok{get\_regression\_points}\NormalTok{(score\_model)}
\NormalTok{regression\_points}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 463 x 5
      ID score bty_avg score_hat residual
   <int> <dbl>   <dbl>     <dbl>    <dbl>
 1     1 4.7   5           4.214  0.486  
 2     2 4.100 5           4.214 -0.114  
 3     3 3.9   5           4.214 -0.314  
 4     4 4.8   5           4.214  0.586  
 5     5 4.600 3           4.08   0.52   
 6     6 4.3   3           4.08   0.22   
 7     7 2.8   3           4.08  -1.28   
 8     8 4.100 3.333       4.102 -0.002  
 9     9 3.4   3.333       4.102 -0.702  
10    10 4.5   3.16700     4.091  0.40900
# ... with 453 more rows
\end{verbatim}

A \emph{residual analysis} is used to verify conditions \textbf{L}, \textbf{N}, and \textbf{E} and can be performed using appropriate data visualizations. While there are more sophisticated statistical approaches that can also be done, we'll focus on the much simpler approach of looking at plots.

\hypertarget{linearity-of-relationship}{%
\subsection{Linearity of relationship}\label{linearity-of-relationship}}

The first condition is that the relationship between the outcome variable \(y\) and the explanatory variable \(x\) must be \textbf{L}inear. Recall the scatterplot in Figure \ref{fig:regline} where we had the explanatory variable \(x\) as ``beauty'' score and the outcome variable \(y\) as teaching score. Would you say that the relationship between \(x\) and \(y\) is linear? It's hard to say because of the scatter of the points about the line. In the authors' opinions, we feel this relationship is ``linear enough.''

Let's present an example where the relationship between \(x\) and \(y\) is clearly not linear in Figure \ref{fig:non-linear}. In this case, the points clearly do not form a line, but rather a U-shaped polynomial curve. In this case, any results from an inference for regression would not be valid.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/non-linear-1} 

}

\caption{Example of a clearly non-linear relationship.}\label{fig:non-linear}
\end{figure}

\hypertarget{independence-of-residuals}{%
\subsection{Independence of residuals}\label{independence-of-residuals}}

The second condition is that the residuals must be \textbf{I}ndependent. In other words, the different observations in our data must be independent of one another.

For our UT Austin data, while there is data on 463 courses, these 463 courses were actually taught by 94 unique instructors. In other words, the same professor is often included more than once in our data. The original \texttt{evals} data frame that we used to construct the \texttt{evals\_ch5} data frame has a variable \texttt{prof\_ID}, which is an anonymized identification variable for the professor:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{evals }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(ID, prof\_ID, score, bty\_avg)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 463 x 4
      ID prof_ID score bty_avg
   <int>   <int> <dbl>   <dbl>
 1     1       1 4.7   5      
 2     2       1 4.100 5      
 3     3       1 3.9   5      
 4     4       1 4.8   5      
 5     5       2 4.600 3      
 6     6       2 4.3   3      
 7     7       2 2.8   3      
 8     8       3 4.100 3.333  
 9     9       3 3.4   3.333  
10    10       4 4.5   3.16700
# ... with 453 more rows
\end{verbatim}

For example, the professor with \texttt{prof\_ID} equal to 1 taught the first 4 courses in the data, the professor with \texttt{prof\_ID} equal to 2 taught the next 3, and so on. Given that the same professor taught these first four courses, it is reasonable to expect that these four teaching scores are related to each other. If a professor gets a high \texttt{score} in one class, chances are fairly good they'll get a high \texttt{score} in another. This dataset thus provides different information than if we had 463 unique instructors teaching the 463 courses.

In this case, we say there exists \emph{dependence} between observations. The first four courses taught by professor 1 are dependent, the next 3 courses taught by professor 2 are related, and so on. Any proper analysis of this data needs to take into account that we have \emph{repeated measures} for the same profs.

So in this case, the independence condition is not met. What does this mean for our analysis? We'll address this in Subsection \ref{what-is-the-conclusion} coming up, after we check the remaining two conditions.

\hypertarget{normality-of-residuals}{%
\subsection{Normality of residuals}\label{normality-of-residuals}}

The third condition is that the residuals should follow a \textbf{N}ormal distribution. Furthermore, the center of this distribution should be 0. In other words, sometimes the regression model will make positive errors: \(y - \widehat{y} > 0\). Other times, the regression model will make equally negative errors: \(y - \widehat{y} < 0\). However, \emph{on average} the errors should equal 0 and their shape should be similar to that of a bell.

The simplest way to check the normality of the residuals is to look at a histogram, which we visualize in Figure \ref{fig:model1residualshist}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(regression\_points, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ residual)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \FloatTok{0.25}\NormalTok{, }\AttributeTok{color =} \StringTok{"white"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Residual"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/model1residualshist-1} 

}

\caption{Histogram of residuals.}\label{fig:model1residualshist}
\end{figure}

This histogram shows that we have more positive residuals than negative. Since the residual \(y-\widehat{y}\) is positive when \(y > \widehat{y}\), it seems our regression model's fitted teaching scores \(\widehat{y}\) tend to \emph{underestimate} the true teaching scores \(y\). Furthermore, this histogram has a slight \emph{left-skew}\index{skew} in that there is a tail on the left. This is another way to say the residuals exhibit a \emph{negative skew}.

Is this a problem? Again, there is a certain amount of subjectivity in the response. In the authors' opinion, while there is a slight skew to the residuals, we feel it isn't drastic. On the other hand, others might disagree with our assessment.

Let's present examples where the residuals clearly do and don't follow a normal distribution in Figure \ref{fig:normal-residuals}. In this case of the model yielding the clearly non-normal residuals on the right, any results from an inference for regression would not be valid.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/normal-residuals-1} 

}

\caption{Example of clearly normal and clearly not normal residuals.}\label{fig:normal-residuals}
\end{figure}

\hypertarget{equality-of-variance}{%
\subsection{Equality of variance}\label{equality-of-variance}}

The fourth and final condition is that the residuals should exhibit \textbf{E}qual variance across all values of the explanatory variable \(x\). In other words, the value and spread of the residuals should not depend on the value of the explanatory variable \(x\).

Recall the scatterplot in Figure \ref{fig:regline}: we had the explanatory variable \(x\) of ``beauty'' score on the x-axis and the outcome variable \(y\) of teaching score on the y-axis. Instead, let's create a scatterplot that has the same values on the x-axis, but now with the residual \(y-\widehat{y}\) on the y-axis as seen in Figure \ref{fig:numxplot6}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(regression\_points, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ bty\_avg, }\AttributeTok{y =}\NormalTok{ residual)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Beauty Score"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Residual"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{size =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/numxplot6-1} 

}

\caption{Plot of residuals over beauty score.}\label{fig:numxplot6}
\end{figure}

You can think of Figure \ref{fig:numxplot6} as a modified version of the plot with the regression line in Figure \ref{fig:regline}, but with the regression line flattened out to \(y=0\). Looking at this plot, would you say that the spread of the residuals around the line at \(y=0\) is constant across all values of the explanatory variable \(x\) of ``beauty'' score? This question is rather qualitative and subjective in nature, thus different people may respond with different answers. For example, some people might say that there is slightly more variation in the residuals for smaller values of \(x\) than for higher ones. However, it can be argued that there isn't a \emph{drastic} non-constancy.

In Figure \ref{fig:equal-variance-residuals} let's present an example where the residuals clearly do not have equal variance across all values of the explanatory variable \(x\).

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/equal-variance-residuals-1} 

}

\caption{Example of clearly non-equal variance.}\label{fig:equal-variance-residuals}
\end{figure}

Observe how the spread of the residuals increases as the value of \(x\) increases. This is a situation known as \index{heteroskedasticity} \emph{heteroskedasticity}. Any inference for regression based on a model yielding such a pattern in the residuals would not be valid.

\hypertarget{what-is-the-conclusion}{%
\subsection{What's the conclusion?}\label{what-is-the-conclusion}}

Let's list our four conditions for inference for regression again and indicate whether or not they were satisfied in our analysis:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{L}inearity of relationship between variables: Yes
\item
  \textbf{I}ndependence of residuals: No
\item
  \textbf{N}ormality of residuals: Somewhat
\item
  \textbf{E}quality of variance: Yes
\end{enumerate}

So what does this mean for the results of our confidence intervals and hypothesis tests in Section \ref{regression-interp}?

First, the \textbf{I}ndependence condition. The fact that there exist dependencies between different rows in \texttt{evals\_ch5} must be addressed. In more advanced statistics courses, you'll learn how to incorporate such dependencies into your regression models. One such technique is called \emph{hierarchical/multilevel modeling}.

Second, when conditions \textbf{L}, \textbf{N}, \textbf{E} are not met, it often means there is a shortcoming in our model. For example, it may be the case that using only a single explanatory variable is insufficient, as we did with ``beauty'' score. We may need to incorporate more explanatory variables in a multiple regression model as we did in Chapter \ref{multiple-regression}.

In our case, the best we can do is view the results suggested by our confidence intervals and hypothesis tests as preliminary. While a preliminary analysis suggests that there is a significant relationship between teaching and ``beauty'' scores, further investigation is warranted; in particular, by improving the preliminary \texttt{score\ \textasciitilde{}\ bty\_avg} model so that the four conditions are met. When the four conditions are roughly met, then we can put more faith into our confidence intervals and \(p\)-values.

The conditions for inference in regression problems are a key part of regression analysis that are of vital importance to the processes of constructing confidence intervals and conducting hypothesis tests. However, it is often the case with regression analysis in the real world that not all the conditions are completely met. Furthermore, as you saw, there is a level of subjectivity in the residual analyses to verify the \textbf{L}, \textbf{N}, and \textbf{E} conditions. So what can you do? We as authors advocate for transparency in communicating all results. This lets the stakeholders of any analysis know about a model's shortcomings or whether the model is ``good enough.'' So while this checking of assumptions has lead to some fuzzy ``it depends'' results, we decided as authors to show you these scenarios to help prepare you for difficult statistical decisions you may need to make down the road.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC10.1)} Continuing with our regression using \texttt{age} as the explanatory variable and teaching \texttt{score} as the outcome variable.

\begin{itemize}
\tightlist
\item
  Use the \texttt{get\_regression\_points()} function to get the observed values, fitted values, and residuals for all 463 instructors.
\item
  Perform a residual analysis and look for any systematic patterns in the residuals. Ideally, there should be little to no pattern but comment on what you find here.
\end{itemize}

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{infer-regression}{%
\section{Simulation-based inference for regression}\label{infer-regression}}

Recall in Subsection \ref{regression-table-computation} when we interpreted the third through seventh columns of a regression table, we stated that R doesn't do simulations to compute these values. Rather R uses theory-based methods that involve mathematical formulas.

In this section, we'll use the simulation-based methods you previously learned in Chapters \ref{confidence-intervals} and \ref{hypothesis-testing} to recreate the values in the regression table in Table \ref{tab:regtable-11}. In particular, we'll use the \texttt{infer} package workflow to

\begin{itemize}
\tightlist
\item
  Construct a 95\% confidence interval for the population slope \(\beta_1\) using bootstrap resampling with replacement. We did this previously in Sections \ref{bootstrap-process} with the \texttt{pennies} data and \ref{case-study-two-prop-ci} with the \texttt{mythbusters\_yawn} data.
\item
  Conduct a hypothesis test of \(H_0: \beta_1 = 0\) versus \(H_A: \beta_1 \neq 0\) using a permutation test. We did this previously in Sections \ref{ht-infer} with the \texttt{promotions} data and \ref{ht-case-study} with the \texttt{movies\_sample} IMDb data.
\end{itemize}

\hypertarget{confidence-interval-for-slope}{%
\subsection{Confidence interval for slope}\label{confidence-interval-for-slope}}

We'll construct a 95\% confidence interval for \(\beta_1\) using the \texttt{infer} workflow outlined in Subsection \ref{infer-workflow}. Specifically, we'll first construct the bootstrap distribution for the fitted slope \(b_1\) using our single sample of 463 courses:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{specify()} the variables of interest in \texttt{evals\_ch5} with the formula: \texttt{score\ \textasciitilde{}\ bty\_avg}.
\item
  \texttt{generate()} replicates by using \texttt{bootstrap} resampling with replacement from the original sample of 463 courses. We generate \texttt{reps\ =\ 1000} replicates using \texttt{type\ =\ "bootstrap"}.
\item
  \texttt{calculate()} the summary statistic of interest: the fitted \texttt{slope} \(b_1\).
\end{enumerate}

Using this bootstrap distribution, we'll construct the 95\% confidence interval using the percentile method and (if appropriate) the standard error method as well. It is important to note in this case that the bootstrapping with replacement is done \emph{row-by-row}. Thus, the original pairs of \texttt{score} and \texttt{bty\_avg} values are always kept together, but different pairs of \texttt{score} and \texttt{bty\_avg} values may be resampled multiple times. The resulting confidence interval will denote a range of plausible values for the unknown population slope \(\beta_1\) quantifying the relationship between teaching and ``beauty'' scores for \emph{all} professors at UT Austin.

Let's first construct the bootstrap distribution for the fitted slope \(b_1\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bootstrap\_distn\_slope }\OtherTok{\textless{}{-}}\NormalTok{ evals\_ch5 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ score }\SpecialCharTok{\textasciitilde{}}\NormalTok{ bty\_avg) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{, }\AttributeTok{type =} \StringTok{"bootstrap"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"slope"}\NormalTok{)}
\NormalTok{bootstrap\_distn\_slope}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,000 x 2
   replicate      stat
       <int>     <dbl>
 1         1 0.0651055
 2         2 0.0382313
 3         3 0.108056 
 4         4 0.0666601
 5         5 0.0715932
 6         6 0.0854565
 7         7 0.0624868
 8         8 0.0412859
 9         9 0.0796269
10        10 0.0761299
# ... with 990 more rows
\end{verbatim}

Observe how we have 1000 values of the bootstrapped slope \(b_1\) in the \texttt{stat} column. Let's visualize the 1000 bootstrapped values in Figure \ref{fig:bootstrap-distribution-slope}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{visualize}\NormalTok{(bootstrap\_distn\_slope)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/bootstrap-distribution-slope-1} 

}

\caption{Bootstrap distribution of slope.}\label{fig:bootstrap-distribution-slope}
\end{figure}

Observe how the bootstrap distribution is roughly bell-shaped. Recall from Subsection \ref{bootstrap-vs-sampling} that the shape of the bootstrap distribution of \(b_1\) closely approximates the shape of the sampling distribution of \(b_1\).

\hypertarget{percentile-method-1}{%
\subsubsection*{Percentile-method}\label{percentile-method-1}}


First, let's compute the 95\% confidence interval for \(\beta_1\) using the percentile method. We'll do so by identifying the 2.5th and 97.5th percentiles which include the middle 95\% of values. Recall that this method does not require the bootstrap distribution to be normally shaped.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{percentile\_ci }\OtherTok{\textless{}{-}}\NormalTok{ bootstrap\_distn\_slope }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{get\_confidence\_interval}\NormalTok{(}\AttributeTok{type =} \StringTok{"percentile"}\NormalTok{, }\AttributeTok{level =} \FloatTok{0.95}\NormalTok{)}
\NormalTok{percentile\_ci}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 2
   lower_ci  upper_ci
      <dbl>     <dbl>
1 0.0323411 0.0990027
\end{verbatim}

The resulting percentile-based 95\% confidence interval for \(\beta_1\) of (0.032, 0.099) is similar to the confidence interval in the regression Table \ref{tab:regtable-11} of (0.035, 0.099).

\hypertarget{standard-error-method}{%
\subsubsection*{Standard error method}\label{standard-error-method}}


Since the bootstrap distribution in Figure \ref{fig:bootstrap-distribution-slope} appears to be roughly bell-shaped, we can also construct a 95\% confidence interval for \(\beta_1\) using the standard error method.

In order to do this, we need to first compute the fitted slope \(b_1\), which will act as the center of our standard error-based confidence interval. While we saw in the regression table in Table \ref{tab:regtable-11} that this was \(b_1\) = 0.067, we can also use the \texttt{infer} pipeline with the \texttt{generate()} step removed to calculate it:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{observed\_slope }\OtherTok{\textless{}{-}}\NormalTok{ evals }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(score }\SpecialCharTok{\textasciitilde{}}\NormalTok{ bty\_avg) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"slope"}\NormalTok{)}
\NormalTok{observed\_slope}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 1
       stat
      <dbl>
1 0.0666370
\end{verbatim}

We then use the \texttt{get\_ci()} function with \texttt{level\ =\ 0.95} to compute the 95\% confidence interval for \(\beta_1\). Note that setting the \texttt{point\_estimate} argument to the \texttt{observed\_slope} of 0.067 sets the center of the confidence interval.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{se\_ci }\OtherTok{\textless{}{-}}\NormalTok{ bootstrap\_distn\_slope }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{get\_ci}\NormalTok{(}\AttributeTok{level =} \FloatTok{0.95}\NormalTok{, }\AttributeTok{type =} \StringTok{"se"}\NormalTok{, }\AttributeTok{point\_estimate =}\NormalTok{ observed\_slope)}
\NormalTok{se\_ci}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 2
   lower_ci  upper_ci
      <dbl>     <dbl>
1 0.0333767 0.0998974
\end{verbatim}

The resulting standard error-based 95\% confidence interval for \(\beta_1\) of \((, )\) is slightly different than the confidence interval in the regression Table \ref{tab:regtable-11} of \((0.035, 0.099)\).

\hypertarget{comparing-all-three}{%
\subsubsection*{Comparing all three}\label{comparing-all-three}}


Let's compare all three confidence intervals in Figure \ref{fig:bootstrap-distribution-slope-CI}, where the percentile-based confidence interval is marked with solid lines, the standard error based confidence interval is marked with dashed lines, and the theory-based confidence interval (0.035, 0.099) from the regression table in Table \ref{tab:regtable-11} is marked with dotted lines.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{visualize}\NormalTok{(bootstrap\_distn\_slope) }\SpecialCharTok{+} 
  \FunctionTok{shade\_confidence\_interval}\NormalTok{(}\AttributeTok{endpoints =}\NormalTok{ percentile\_ci, }\AttributeTok{fill =} \ConstantTok{NULL}\NormalTok{, }
                            \AttributeTok{linetype =} \StringTok{"solid"}\NormalTok{, }\AttributeTok{color =} \StringTok{"grey90"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{shade\_confidence\_interval}\NormalTok{(}\AttributeTok{endpoints =}\NormalTok{ se\_ci, }\AttributeTok{fill =} \ConstantTok{NULL}\NormalTok{, }
                            \AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{, }\AttributeTok{color =} \StringTok{"grey60"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{shade\_confidence\_interval}\NormalTok{(}\AttributeTok{endpoints =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.035}\NormalTok{, }\FloatTok{0.099}\NormalTok{), }\AttributeTok{fill =} \ConstantTok{NULL}\NormalTok{, }
                            \AttributeTok{linetype =} \StringTok{"dotted"}\NormalTok{, }\AttributeTok{color =} \StringTok{"black"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/bootstrap-distribution-slope-CI-1} 

}

\caption{Comparing three confidence intervals for the slope.}\label{fig:bootstrap-distribution-slope-CI}
\end{figure}

Observe that all three are quite similar! Furthermore, none of the three confidence intervals for \(\beta_1\) contain 0 and are entirely located above 0. This is suggesting that there is in fact a meaningful positive relationship between teaching and ``beauty'' scores.

\hypertarget{hypothesis-test-for-slope}{%
\subsection{Hypothesis test for slope}\label{hypothesis-test-for-slope}}

Let's now conduct a hypothesis test of \(H_0: \beta_1 = 0\) vs.~\(H_A: \beta_1 \neq 0\). We will use the \texttt{infer} package, which follows the hypothesis testing paradigm in the ``There is only one test'' diagram in Figure \ref{fig:htdowney}.

Let's first think about what it means for \(\beta_1\) to be zero as assumed in the null hypothesis \(H_0\). Recall we said if \(\beta_1 = 0\), then this is saying there is no relationship between the teaching and ``beauty'' scores. Thus assuming this particular null hypothesis \(H_0\) means that in our ``hypothesized universe'' there is no relationship between \texttt{score} and \texttt{bty\_avg}. We can therefore shuffle/permute the \texttt{bty\_avg} variable to no consequence.

We construct the null distribution of the fitted slope \(b_1\) by performing the steps that follow. Recall from Section \ref{understanding-ht} on terminology, notation, and definitions related to hypothesis testing where we defined the \emph{null distribution}: the sampling distribution of our test statistic \(b_1\) assuming the null hypothesis \(H_0\) is true.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{specify()} the variables of interest in \texttt{evals\_ch5} with the formula: \texttt{score\ \textasciitilde{}\ bty\_avg}.
\item
  \texttt{hypothesize()} the null hypothesis of \texttt{independence}. Recall from Section \ref{ht-infer} that this is an additional step that needs to be added for hypothesis testing.
\item
  \texttt{generate()} replicates by permuting/shuffling values from the original sample of 463 courses. We generate \texttt{reps\ =\ 1000} replicates using \texttt{type\ =\ "permute"} here.
\item
  \texttt{calculate()} the test statistic of interest: the fitted \texttt{slope} \(b_1\).
\end{enumerate}

In this case, we \texttt{permute} the values of \texttt{bty\_avg} across the values of \texttt{score} 1000 times. We can do this shuffling/permuting since we assumed a ``hypothesized universe'' of no relationship between these two variables. Then we \texttt{calculate} the \texttt{"slope"} coefficient for each of these 1000 \texttt{generate}d samples.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{null\_distn\_slope }\OtherTok{\textless{}{-}}\NormalTok{ evals }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{specify}\NormalTok{(score }\SpecialCharTok{\textasciitilde{}}\NormalTok{ bty\_avg) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{hypothesize}\NormalTok{(}\AttributeTok{null =} \StringTok{"independence"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{generate}\NormalTok{(}\AttributeTok{reps =} \DecValTok{1000}\NormalTok{, }\AttributeTok{type =} \StringTok{"permute"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{calculate}\NormalTok{(}\AttributeTok{stat =} \StringTok{"slope"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Observe the resulting null distribution for the fitted slope \(b_1\) in Figure \ref{fig:null-distribution-slope}.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/null-distribution-slope-1} 

}

\caption{Null distribution of slopes.}\label{fig:null-distribution-slope}
\end{figure}

Notice how it is centered at \(b_1\) = 0. This is because in our hypothesized universe, there is no relationship between \texttt{score} and \texttt{bty\_avg} and so \(\beta_1 = 0\). Thus, the most typical fitted slope \(b_1\) we observe across our simulations is 0. Observe, furthermore, how there is variation around this central value of 0.

Let's visualize the \(p\)-value in the null distribution by comparing it to the observed test statistic of \(b_1\) = 0.067 in Figure \ref{fig:p-value-slope}. We'll do this by adding a \texttt{shade\_p\_value()} layer to the previous \texttt{visualize()} code.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/p-value-slope-1} 

}

\caption{Null distribution and $p$-value.}\label{fig:p-value-slope}
\end{figure}

Since the observed fitted slope 0.067 falls far to the right of this null distribution and thus the shaded region doesn't overlap it, we'll have a \(p\)-value of 0. For completeness, however, let's compute the numerical value of the \(p\)-value anyways using the \texttt{get\_p\_value()} function. Recall that it takes the same inputs as the \texttt{shade\_p\_value()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{null\_distn\_slope }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{get\_p\_value}\NormalTok{(}\AttributeTok{obs\_stat =}\NormalTok{ observed\_slope, }\AttributeTok{direction =} \StringTok{"both"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 1
  p_value
    <dbl>
1       0
\end{verbatim}

This matches the \(p\)-value of 0 in the regression table in Table \ref{tab:regtable-11}. We therefore reject the null hypothesis \(H_0: \beta_1 = 0\) in favor of the alternative hypothesis \(H_A: \beta_1 \neq 0\). We thus have evidence that suggests there is a significant relationship between teaching and ``beauty'' scores for \emph{all} instructors at UT Austin.

When the conditions for inference for regression are met and the null distribution has a bell shape, we are likely to see similar results between the simulation-based results we just demonstrated and the theory-based results shown in the regression table in Table \ref{tab:regtable-11}.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC10.2)} Repeat the inference but this time for the correlation coefficient instead of the slope. Note the implementation of \texttt{stat\ =\ "correlation"} in the \texttt{calculate()} function of the \texttt{infer} package.

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{inference-conclusion}{%
\section{Conclusion}\label{inference-conclusion}}

\hypertarget{theory-regression}{%
\subsection{Theory-based inference for regression}\label{theory-regression}}

Recall in Subsection \ref{regression-table-computation} when we interpreted the regression table in Table \ref{tab:regtable-11}, we mentioned that R does not compute its values using simulation-based methods for constructing confidence intervals and conducting hypothesis tests as we did in Chapters \ref{confidence-intervals} and \ref{hypothesis-testing} using the \texttt{infer} package. Rather, R uses a theory-based approach using mathematical formulas, much like the theory-based confidence intervals you saw in Subsection \ref{theory-ci} and the theory-based hypothesis tests you saw in Subsection \ref{theory-hypo}. These formulas were derived in a time when computers didn't exist, so it would've been incredibly labor intensive to run extensive simulations.

In particular, there is a formula for the \emph{standard error} of the fitted slope \(b_1\):

\[\text{SE}_{b_1} = \dfrac{\dfrac{s_y}{s_x} \cdot \sqrt{1-r^2}}{\sqrt{n-2}}\]

As with many formulas in statistics, there's a lot going on here, so let's first break down what each symbol represents. First \(s_x\) and \(s_y\) are the \emph{sample standard deviations} of the explanatory variable \texttt{bty\_avg} and the response variable \texttt{score}, respectively. Second, \(r\) is the sample \emph{correlation coefficient} between \texttt{score} and \texttt{bty\_avg}. This was computed as 0.187 in Chapter \ref{regression}. Lastly, \(n\) is the number of pairs of points in the \texttt{evals\_ch5} data frame, here 463.

To put this formula into words, the standard error of \(b_1\) depends on the relationship between the variability of the response variable and the variability of the explanatory variable as measured in the \(s_y / s_x\) term. Next, it looks into how the two variables relate to each other in the \(\sqrt{1-r^2}\) term.

However, the most important observation to make in the previous formula is that there is an \(n - 2\) in the denominator. In other words, as the sample size \(n\) increases, the standard error \(\text{SE}_{b_1}\) decreases. Just as we demonstrated in Subsection \ref{moral-of-the-story} when we used shovels with \(n\) = 25, 50, and 100 slots, the amount of sampling variation of the fitted slope \(b_1\) will depend on the sample size \(n\). In particular, as the sample size increases, both the sampling and bootstrap distributions narrow and the standard error \(\text{SE}_{b_1}\) decreases. Hence, our estimates of \(b_1\) for the true population slope \(\beta_1\) get more and more \emph{precise}.

R then uses this formula for the standard error of \(b_1\) in the third column of the regression table and subsequently to construct 95\% confidence intervals. But what about the hypothesis test? Much like with our theory-based hypothesis test in Subsection \ref{theory-hypo}, R uses the following \emph{\(t\)-statistic} as the test statistic for hypothesis testing:

\[
t = \dfrac{ b_1 - \beta_1}{ \text{SE}_{b_1}}
\]

And since the null hypothesis \(H_0: \beta_1 = 0\) is assumed during the hypothesis test, the \(t\)-statistic becomes

\[
t = \dfrac{ b_1 - 0}{ \text{SE}_{b_1}} = \dfrac{ b_1 }{ \text{SE}_{b_1}}
\]

What are the values of \(b_1\) and \(\text{SE}_{b_1}\)? They are in the \texttt{estimate} and \texttt{std\_error} column of the regression table in Table \ref{tab:regtable-11}. Thus the value of 4.09 in the table is computed as 0.067/0.016 = 4.188. Note there is a difference due to some rounding error here.

Lastly, to compute the \(p\)-value, we need to compare the observed test statistic of 4.09 to the appropriate null distribution. Recall from Section \ref{understanding-ht}, that a null distribution is the sampling distribution of the test statistic \emph{assuming the null hypothesis \(H_0\) is true}. Much like in our theory-based hypothesis test in Subsection \ref{theory-hypo}, it can be mathematically proven that this distribution is a \(t\)-distribution with degrees of freedom equal to \(df = n - 2 = 463 - 2 = 461\).

Don't worry if you're feeling a little overwhelmed at this point. There is a lot of background theory to understand before you can fully make sense of the equations for theory-based methods. That being said, theory-based methods and simulation-based methods for constructing confidence intervals and conducting hypothesis tests often yield consistent results. As mentioned before, in our opinion, two large benefits of simulation-based methods over theory-based are that (1) they are easier for people new to statistical inference to understand, and (2) they also work in situations where theory-based methods and mathematical formulas don't exist.

\hypertarget{summary-of-statistical-inference}{%
\subsection{Summary of statistical inference}\label{summary-of-statistical-inference}}

We've finished the last two scenarios from the ``Scenarios of sampling for inference'' table in Subsection \ref{sampling-conclusion-table}, which we re-display in Table \ref{tab:table-ch11}.

\begin{table}[!h]

\caption{\label{tab:table-ch11}\label{tab:summarytable-ch9}Scenarios of sampling for inference}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{>{\raggedleft\arraybackslash}p{0.5in}>{\raggedright\arraybackslash}p{1.5in}>{\raggedright\arraybackslash}p{0.65in}>{\raggedright\arraybackslash}p{1.6in}>{\raggedright\arraybackslash}p{0.65in}}
\toprule
Scenario & Population parameter & Notation & Point estimate & Symbol(s)\\
\midrule
1 & Population proportion & $p$ & Sample proportion & $\widehat{p}$\\
2 & Population mean & $\mu$ & Sample mean & $\overline{x}$ or $\widehat{\mu}$\\
3 & Difference in population proportions & $p_1 - p_2$ & Difference in sample proportions & $\widehat{p}_1 - \widehat{p}_2$\\
4 & Difference in population means & $\mu_1 - \mu_2$ & Difference in sample means & $\overline{x}_1 - \overline{x}_2$\\
5 & Population regression slope & $\beta_1$ & Fitted regression slope & $b_1$ or $\widehat{\beta}_1$\\
\bottomrule
\end{tabular}
\end{table}

Armed with the regression modeling techniques you learned in Chapters \ref{regression} and \ref{multiple-regression}, your understanding of sampling for inference in Chapter \ref{sampling}, and the tools for statistical inference like confidence intervals and hypothesis tests in Chapters \ref{confidence-intervals} and \ref{hypothesis-testing}, you're now equipped to study the significance of relationships between variables in a wide array of data! Many of the ideas presented here can be extended into multiple regression and other more advanced modeling techniques.

\hypertarget{additional-resources-8}{%
\subsection{Additional resources}\label{additional-resources-8}}

Solutions to all \emph{Learning checks} can be found online in \href{https://moderndive.com/D-appendixD.html}{Appendix D}.

An R script file of all R code used in this chapter is available at \url{https://www.moderndive.com/scripts/10-inference-for-regression.R}.

\hypertarget{whats-to-come-9}{%
\subsection{What's to come}\label{whats-to-come-9}}

You've now concluded the last major part of the book on ``Statistical Inference with \texttt{infer}.'' The closing Chapter \ref{thinking-with-data} concludes this book with various short case studies involving real data, such as house prices in the city of Seattle, Washington in the US. You'll see how the principles in this book can help you become a great storyteller with data!

\hypertarget{part-conclusion}{%
\part{Conclusion}\label{part-conclusion}}

\hypertarget{thinking-with-data}{%
\chapter{Tell Your Story with Data}\label{thinking-with-data}}

Recall in the Preface and at the end of chapters throughout this book, we displayed the ``\emph{ModernDive} flowchart'' mapping your journey through this book.



\begin{figure}

{\centering \includegraphics[width=1\linewidth,height=1\textheight]{images/flowcharts/flowchart/flowchart.002} 

}

\caption{\emph{ModernDive} flowchart.}\label{fig:moderndive-figure-conclusion}
\end{figure}

\hypertarget{review}{%
\section{Review}\label{review}}

Let's go over a refresher of what you've covered so far. You first got started with data in Chapter \ref{getting-started} where you learned about the difference between R and RStudio, started coding in R, installed and loaded your first R packages, and explored your first dataset: all domestic departure \texttt{flights} from a major New York City airport in 2013. Then you covered the following three parts of this book (Parts 2 and 4 are combined into a single portion):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Data science with \texttt{tidyverse}. You assembled your data science toolbox using \texttt{tidyverse} packages. In particular, you

  \begin{itemize}
  \tightlist
  \item
    Ch.\ref{viz}: Visualized data using the \texttt{ggplot2} package.
  \item
    Ch.\ref{wrangling}: Wrangled data using the \texttt{dplyr} package.
  \item
    Ch.\ref{tidy}: Learned about the concept of ``tidy'' data as a standardized data frame input and output format for all packages in the \texttt{tidyverse}. Furthermore, you learned how to import spreadsheet files into R using the \texttt{readr} package.
  \end{itemize}
\item
  Data modeling with \texttt{moderndive}. Using these data science tools and helper functions from the \texttt{moderndive} package, you fit your first data models. In particular, you

  \begin{itemize}
  \tightlist
  \item
    Ch.\ref{regression}: Discovered basic regression models with only one explanatory variable.
  \item
    Ch.\ref{multiple-regression}: Examined multiple regression models with more than one explanatory variable.
  \end{itemize}
\item
  Statistical inference with \texttt{infer}. Once again using your newly acquired data science tools, you unpacked statistical inference using the \texttt{infer} package. In particular, you

  \begin{itemize}
  \tightlist
  \item
    Ch.\ref{sampling}: Learned about the role that sampling variability plays in statistical inference and the role that sample size plays in this sampling variability.
  \item
    Ch.\ref{confidence-intervals}: Constructed confidence intervals using bootstrapping.
  \item
    Ch.\ref{hypothesis-testing}: Conducted hypothesis tests using permutation.
  \end{itemize}
\item
  Data modeling with \texttt{moderndive} (revisited): Armed with your understanding of statistical inference, you revisited and reviewed the models you constructed in Ch.\ref{regression} and Ch.\ref{multiple-regression}. In particular, you

  \begin{itemize}
  \tightlist
  \item
    Ch.\ref{inference-for-regression}: Interpreted confidence intervals and hypothesis tests in a regression setting.
  \end{itemize}
\end{enumerate}

We've guided you through your first experiences of \href{https://arxiv.org/pdf/1410.3127.pdf}{``thinking with data,''} an expression originally coined by \index{Lambert, Diane} Dr.~Diane Lambert. The philosophy underlying this expression guided your path in the flowchart in Figure \ref{fig:moderndive-figure-conclusion}.

This philosophy is also well-summarized in \href{https://peerj.com/collections/50-practicaldatascistats/}{``Practical Data Science for Stats''}: a collection of pre-prints focusing on the practical side of data science workflows and statistical analysis curated by \href{https://twitter.com/jennybryan}{Dr.~Jennifer Bryan} \index{Bryan, Jenny} and \href{https://twitter.com/hadleywickham}{Dr.~Hadley Wickham}. They quote:

\begin{quote}
There are many aspects of day-to-day analytical work that are almost absent from the conventional statistics literature and curriculum. And yet these activities account for a considerable share of the time and effort of data analysts and applied statisticians. The goal of this collection is to increase the visibility and adoption of modern data analytical workflows. We aim to facilitate the transfer of tools and frameworks between industry and academia, between software engineering and statistics and computer science, and across different domains.
\end{quote}

In other words, to be equipped to ``think with data'' in the 21st century, analysts need practice going through the \href{http://r4ds.had.co.nz/explore-intro.html}{``data/science pipeline''} we saw in the Preface (re-displayed in Figure \ref{fig:pipeline-figure-conclusion}). It is our opinion that, for too long, statistics education has only focused on parts of this pipeline, instead of going through it in its \emph{entirety}.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth,height=0.7\textheight]{images/r4ds/data_science_pipeline} 

}

\caption{Data/science pipeline.}\label{fig:pipeline-figure-conclusion}
\end{figure}

To conclude this book, we'll present you with some additional case studies of working with data. In Section \ref{seattle-house-prices} we'll take you through a full-pass of the ``Data/Science Pipeline'' in order to analyze the sale price of houses in Seattle, WA, USA. In Section \ref{data-journalism}, we'll present you with some examples of effective data storytelling drawn from the data journalism website, \href{https://fivethirtyeight.com/}{FiveThirtyEight.com}\index{FiveThirtyEight}. We present these case studies to you because we believe that you should not only be able to ``think with data,'' but also be able to ``tell your story with data.'' Let's explore how to do this!

\hypertarget{story-packages}{%
\subsection*{Needed packages}\label{story-packages}}


Let's load all the packages needed for this chapter (this assumes you've already installed them). Read Section \ref{packages} for information on how to install and load R packages.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(moderndive)}
\FunctionTok{library}\NormalTok{(skimr)}
\FunctionTok{library}\NormalTok{(fivethirtyeight)}
\end{Highlighting}
\end{Shaded}

\hypertarget{seattle-house-prices}{%
\section{Case study: Seattle house prices}\label{seattle-house-prices}}

\href{https://www.kaggle.com/}{Kaggle.com} is a machine learning and predictive modeling competition website that hosts datasets uploaded by companies, governmental organizations, and other individuals. One of their datasets is the \href{https://www.kaggle.com/harlfoxem/housesalesprediction}{``House Sales in King County, USA''}. It consists of sale prices of homes sold between May 2014 and May 2015 in King County, Washington, USA, which includes the greater Seattle metropolitan area. This dataset is in the \texttt{house\_prices} data frame included in the \texttt{moderndive} package.

The dataset consists of 21,613 houses and 23 variables describing these houses (for a full list and description of these variables, see the help file by running \texttt{?house\_prices} in the console). In this case study, we'll create a multiple regression model where:

\begin{itemize}
\tightlist
\item
  The outcome variable \(y\) is the sale \texttt{price} of houses.
\item
  Two explanatory variables:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    A numerical explanatory variable \(x_1\): house size \texttt{sqft\_living} as measured in square feet of living space. Note that 1 square foot is about 0.09 square meters.
  \item
    A categorical explanatory variable \(x_2\): house \texttt{condition}, a categorical variable with five levels where \texttt{1} indicates ``poor'' and \texttt{5} indicates ``excellent.''
  \end{enumerate}
\end{itemize}

\hypertarget{house-prices-EDA-I}{%
\subsection{Exploratory data analysis: Part I}\label{house-prices-EDA-I}}

As we've said numerous times throughout this book, a crucial first step when presented with data is to perform an exploratory data analysis (EDA). Exploratory data analysis can give you a sense of your data, help identify issues with your data, bring to light any outliers, and help inform model construction.

Recall the three common steps in an exploratory data analysis we introduced in Subsection \ref{model1EDA}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Looking at the raw data values.
\item
  Computing summary statistics.
\item
  Creating data visualizations.
\end{enumerate}

First, let's look at the raw data using \texttt{View()} to bring up RStudio's spreadsheet viewer and the \texttt{glimpse()} function from the \texttt{dplyr} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{View}\NormalTok{(house\_prices)}
\FunctionTok{glimpse}\NormalTok{(house\_prices)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 21,613
Columns: 23
$ id            <chr> "7129300520", "6414100192", "5631500400", "24872...
$ date          <date> 2014-10-13, 2014-12-09, 2015-02-25, 2014-12-09,...
$ price         <dbl> 221900, 538000, 180000, 604000, 510000, 1225000,...
$ bedrooms      <int> 3, 3, 2, 4, 3, 4, 3, 3, 3, 3, 3, 2, 3, 3, 5, 4, ...
$ bathrooms     <dbl> 1.00, 2.25, 1.00, 3.00, 2.00, 4.50, 2.25, 1.50, ...
$ sqft_living   <int> 1180, 2570, 770, 1960, 1680, 5420, 1715, 1060, 1...
$ sqft_lot      <int> 5650, 7242, 10000, 5000, 8080, 101930, 6819, 971...
$ floors        <dbl> 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0...
$ waterfront    <lgl> FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE,...
$ view          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, ...
$ condition     <fct> 3, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3, ...
$ grade         <fct> 7, 7, 6, 7, 8, 11, 7, 7, 7, 7, 8, 7, 7, 7, 7, 9,...
$ sqft_above    <int> 1180, 2170, 770, 1050, 1680, 3890, 1715, 1060, 1...
$ sqft_basement <int> 0, 400, 0, 910, 0, 1530, 0, 0, 730, 0, 1700, 300...
$ yr_built      <int> 1955, 1951, 1933, 1965, 1987, 2001, 1995, 1963, ...
$ yr_renovated  <int> 0, 1991, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
$ zipcode       <fct> 98178, 98125, 98028, 98136, 98074, 98053, 98003,...
$ lat           <dbl> 47.5, 47.7, 47.7, 47.5, 47.6, 47.7, 47.3, 47.4, ...
$ long          <dbl> -122, -122, -122, -122, -122, -122, -122, -122, ...
$ sqft_living15 <int> 1340, 1690, 2720, 1360, 1800, 4760, 2238, 1650, ...
$ sqft_lot15    <int> 5650, 7639, 8062, 5000, 7503, 101930, 6819, 9711...
$ log10_price   <dbl> 5.35, 5.73, 5.26, 5.78, 5.71, 6.09, 5.41, 5.47, ...
$ log10_size    <dbl> 3.07, 3.41, 2.89, 3.29, 3.23, 3.73, 3.23, 3.03, ...
\end{verbatim}

Here are some questions you can ask yourself at this stage of an EDA: Which variables are numerical? Which are categorical? For the categorical variables, what are their levels? Besides the variables we'll be using in our regression model, what other variables do you think would be useful to use in a model for house price?

Observe, for example, that while the \texttt{condition} variable has values \texttt{1} through \texttt{5}, these are saved in R as \texttt{fct} standing for ``factors.'' This is one of R's ways of saving categorical variables. So you should think of these as the ``labels'' \texttt{1} through \texttt{5} and not the numerical values \texttt{1} through \texttt{5}.

Let's now perform the second step in an EDA: computing summary statistics. Recall from Section \ref{summarize} that \emph{summary statistics} are single numerical values that summarize a large number of values. Examples of summary statistics include the mean, the median, the standard deviation, and various percentiles.

We could do this using the \texttt{summarize()} function in the \texttt{dplyr} package along with R's built-in \emph{summary functions}, like \texttt{mean()} and \texttt{median()}. However, recall in Section \ref{mutate}, we saw the following code that computes a variety of summary statistics of the variable \texttt{gain}, which is the amount of time that a flight makes up mid-air:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gain\_summary }\OtherTok{\textless{}{-}}\NormalTok{ flights }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}
    \AttributeTok{min =} \FunctionTok{min}\NormalTok{(gain, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q1 =} \FunctionTok{quantile}\NormalTok{(gain, }\FloatTok{0.25}\NormalTok{, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{median =} \FunctionTok{quantile}\NormalTok{(gain, }\FloatTok{0.5}\NormalTok{, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{q3 =} \FunctionTok{quantile}\NormalTok{(gain, }\FloatTok{0.75}\NormalTok{, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{max =} \FunctionTok{max}\NormalTok{(gain, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(gain, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{sd =} \FunctionTok{sd}\NormalTok{(gain, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{),}
    \AttributeTok{missing =} \FunctionTok{sum}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(gain))}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

To repeat this for all three \texttt{price}, \texttt{sqft\_living}, and \texttt{condition} variables would be tedious to code up. So instead, let's use the convenient \texttt{skim()} function from the \texttt{skimr} package we first used in Subsection \ref{model4EDA}\index{R packages!skimr!skim()}, being sure to only \texttt{select()} the variables of interest for our model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{house\_prices }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(price, sqft\_living, condition) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{skim}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Skim summary statistics
 n obs: 21613 
 n variables: 3 

── Variable type:factor 
  variable missing complete     n n_unique                         top_counts ordered
 condition       0    21613 21613        5 3: 14031, 4: 5679, 5: 1701, 2: 172   FALSE

── Variable type:integer 
    variable missing complete     n   mean     sd  p0  p25  p50  p75  p100
 sqft_living       0    21613 21613 2079.9 918.44 290 1427 1910 2550 13540 

── Variable type:numeric 
 variable missing complete     n      mean       sd    p0    p25    p50    p75    p100
    price       0    21613 21613 540088.14 367127.2 75000 321950 450000 645000 7700000
\end{verbatim}

Observe that the mean \texttt{price} of \$540,088 is larger than the median of \$450,000. This is because a small number of very expensive houses are inflating the average. In other words, there are ``outlier'' house prices in our dataset. (This fact will become even more apparent when we create our visualizations next.)

However, the median is not as sensitive to such outlier house prices. This is why news about the real estate market generally report median house prices and not mean/average house prices. We say here that the median is more \emph{robust to outliers} than the mean. Similarly, while both the standard deviation and interquartile-range (IQR) are both measures of spread and variability, the IQR is more \emph{robust to outliers}.\index{outliers}

Let's now perform the last of the three common steps in an exploratory data analysis: creating data visualizations. Let's first create \emph{univariate} visualizations. These are plots focusing on a single variable at a time. Since \texttt{price} and \texttt{sqft\_living} are numerical variables, we can visualize their distributions using a \texttt{geom\_histogram()} as seen in Section \ref{histograms} on histograms. On the other hand, since \texttt{condition} is categorical, we can visualize its distribution using a \texttt{geom\_bar()}. Recall from Section \ref{geombar} on barplots that since \texttt{condition} is not ``pre-counted'', we use a \texttt{geom\_bar()} and not a \texttt{geom\_col()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Histogram of house price:}
\FunctionTok{ggplot}\NormalTok{(house\_prices, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ price)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{color =} \StringTok{"white"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"price (USD)"}\NormalTok{, }\AttributeTok{title =} \StringTok{"House price"}\NormalTok{)}

\CommentTok{\# Histogram of sqft\_living:}
\FunctionTok{ggplot}\NormalTok{(house\_prices, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ sqft\_living)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{color =} \StringTok{"white"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"living space (square feet)"}\NormalTok{, }\AttributeTok{title =} \StringTok{"House size"}\NormalTok{)}

\CommentTok{\# Barplot of condition:}
\FunctionTok{ggplot}\NormalTok{(house\_prices, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ condition)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"condition"}\NormalTok{, }\AttributeTok{title =} \StringTok{"House condition"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In Figure \ref{fig:house-prices-viz}, we display all three of these visualizations at once.

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/house-prices-viz-1} 

}

\caption{Exploratory visualizations of Seattle house prices data.}\label{fig:house-prices-viz}
\end{figure}

First, observe in the bottom plot that most houses are of condition ``3'', with a few more of conditions ``4'' and ``5'', and almost none that are ``1'' or ``2''.

Next, observe in the histogram for \texttt{price} in the top-left plot that a majority of houses are less than two million dollars. Observe also that the x-axis stretches out to 8 million dollars, even though there does not appear to be any houses close to that price. This is because there are a \emph{very small number} of houses with prices closer to 8 million. These are the outlier house prices we mentioned earlier. We say that the variable \texttt{price} is \emph{right-skewed} as exhibited by the long right tail.\index{skew}

Further, observe in the histogram of \texttt{sqft\_living} in the middle plot as well that most houses appear to have less than 5000 square feet of living space. For comparison, a football field in the US is about 57,600 square feet, whereas a standard soccer/association football field is about 64,000 square feet. Observe also that this variable is also right-skewed, although not as drastically as the \texttt{price} variable.

For both the \texttt{price} and \texttt{sqft\_living} variables, the right-skew makes distinguishing houses at the lower end of the x-axis hard. This is because the scale of the x-axis is compressed by the small number of quite expensive and immensely-sized houses.

So what can we do about this skew? Let's apply a \emph{log10 transformation} to these variables. If you are unfamiliar with such transformations, we highly recommend you read Appendix \ref{appendix-log10-transformations} on logarithmic (log) transformations.\index{log transformations} In summary, log transformations allow us to alter the scale of a variable to focus on \emph{multiplicative} changes instead of \emph{additive} changes. In other words, they shift the view to be on \emph{relative} changes instead of \emph{absolute} changes. Such multiplicative/relative changes are also called changes in \emph{orders of magnitude}.

Let's create new log10 transformed versions of the right-skewed variable \texttt{price} and \texttt{sqft\_living} using the \texttt{mutate()} function from Section \ref{mutate}, but we'll give the latter the name \texttt{log10\_size}, which is shorter and easier to understand than the name \texttt{log10\_sqft\_living}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{house\_prices }\OtherTok{\textless{}{-}}\NormalTok{ house\_prices }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{log10\_price =} \FunctionTok{log10}\NormalTok{(price),}
    \AttributeTok{log10\_size =} \FunctionTok{log10}\NormalTok{(sqft\_living)}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

Let's display the before and after effects of this transformation on these variables for only the first 10 rows of \texttt{house\_prices}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{house\_prices }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(price, log10\_price, sqft\_living, log10\_size)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 21,613 x 4
     price log10_price sqft_living log10_size
     <dbl>       <dbl>       <int>      <dbl>
 1  221900     5.34616        1180    3.07188
 2  538000     5.73078        2570    3.40993
 3  180000     5.25527         770    2.88649
 4  604000     5.78104        1960    3.29226
 5  510000     5.70757        1680    3.22531
 6 1225000     6.08814        5420    3.73400
 7  257500     5.41078        1715    3.23426
 8  291850     5.46516        1060    3.02531
 9  229500     5.36078        1780    3.25042
10  323000     5.50920        1890    3.27646
# ... with 21,603 more rows
\end{verbatim}

Observe in particular the houses in the sixth and third rows. The house in the sixth row has \texttt{price} \$1,225,000, which is just above one million dollars. Since \(10^6\) is one million, its \texttt{log10\_price} is around 6.09.

Contrast this with all other houses with \texttt{log10\_price} less than six, since they all have \texttt{price} less than \$1,000,000. The house in the third row is the only house with \texttt{sqft\_living} less than 1000. Since \(1000 = 10^3\), it's the lone house with \texttt{log10\_size} less than 3.

Let's now visualize the before and after effects of this transformation for \texttt{price} in Figure \ref{fig:log10-price-viz}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Before log10 transformation:}
\FunctionTok{ggplot}\NormalTok{(house\_prices, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ price)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{color =} \StringTok{"white"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"price (USD)"}\NormalTok{, }\AttributeTok{title =} \StringTok{"House price: Before"}\NormalTok{)}

\CommentTok{\# After log10 transformation:}
\FunctionTok{ggplot}\NormalTok{(house\_prices, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ log10\_price)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{color =} \StringTok{"white"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"log10 price (USD)"}\NormalTok{, }\AttributeTok{title =} \StringTok{"House price: After"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/log10-price-viz-1} 

}

\caption{House price before and after log10 transformation.}\label{fig:log10-price-viz}
\end{figure}

Observe that after the transformation, the distribution is much less skewed, and in this case, more symmetric and more bell-shaped. Now you can more easily distinguish the lower priced houses.

Let's do the same for house size, where the variable \texttt{sqft\_living} was log10 transformed to \texttt{log10\_size}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Before log10 transformation:}
\FunctionTok{ggplot}\NormalTok{(house\_prices, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ sqft\_living)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{color =} \StringTok{"white"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"living space (square feet)"}\NormalTok{, }\AttributeTok{title =} \StringTok{"House size: Before"}\NormalTok{)}

\CommentTok{\# After log10 transformation:}
\FunctionTok{ggplot}\NormalTok{(house\_prices, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ log10\_size)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{color =} \StringTok{"white"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"log10 living space (square feet)"}\NormalTok{, }\AttributeTok{title =} \StringTok{"House size: After"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/log10-size-viz-1} 

}

\caption{House size before and after log10 transformation.}\label{fig:log10-size-viz}
\end{figure}

Observe in Figure \ref{fig:log10-size-viz} that the log10 transformation has a similar effect of unskewing the variable. We emphasize that while in these two cases the resulting distributions are more symmetric and bell-shaped, this is not always necessarily the case.

Given the now symmetric nature of \texttt{log10\_price} and \texttt{log10\_size}, we are going to revise our multiple regression model to use our new variables:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The outcome variable \(y\) is the sale \texttt{log10\_price} of houses.
\item
  Two explanatory variables:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    A numerical explanatory variable \(x_1\): house size \texttt{log10\_size} as measured in log base 10 square feet of living space.
  \item
    A categorical explanatory variable \(x_2\): house \texttt{condition}, a categorical variable with five levels where \texttt{1} indicates ``poor'' and \texttt{5} indicates ``excellent.''
  \end{enumerate}
\end{enumerate}

\hypertarget{house-prices-EDA-II}{%
\subsection{Exploratory data analysis: Part II}\label{house-prices-EDA-II}}

Let's now continue our EDA by creating \emph{multivariate} visualizations. Unlike the \emph{univariate} histograms and barplot in the earlier Figures \ref{fig:house-prices-viz}, \ref{fig:log10-price-viz}, and \ref{fig:log10-size-viz}, \emph{multivariate} visualizations show relationships between more than one variable. This is an important step of an EDA to perform since the goal of modeling is to explore relationships between variables.

Since our model involves a numerical outcome variable, a numerical explanatory variable, and a categorical explanatory variable, we are in a similar regression modeling situation as in Section \ref{model4} where we studied the UT Austin teaching scores dataset. Recall in that case the numerical outcome variable was teaching \texttt{score}, the numerical explanatory variable was instructor \texttt{age}, and the categorical explanatory variable was (binary) \texttt{gender}.

We thus have two choices of models we can fit: either (1) an \emph{interaction model} where the regression line for each \texttt{condition} level will have both a different slope and a different intercept or (2) a \emph{parallel slopes model} where the regression line for each \texttt{condition} level will have the same slope but different intercepts.

Recall from Subsection \ref{model4table} that the \texttt{geom\_parallel\_slopes()} function is a special purpose function that Evgeni Chasnovski created and included in the \texttt{moderndive} package, since the \texttt{geom\_smooth()} method in the \texttt{ggplot2} package does not have a convenient way to plot parallel slopes models. We plot both resulting models in Figure \ref{fig:house-price-parallel-slopes}, with the interaction model on the left.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Plot interaction model}
\FunctionTok{ggplot}\NormalTok{(house\_prices, }
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ log10\_size, }\AttributeTok{y =}\NormalTok{ log10\_price, }\AttributeTok{col =}\NormalTok{ condition)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.05}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{"log10 price"}\NormalTok{, }
       \AttributeTok{x =} \StringTok{"log10 size"}\NormalTok{, }
       \AttributeTok{title =} \StringTok{"House prices in Seattle"}\NormalTok{)}
\CommentTok{\# Plot parallel slopes model}
\FunctionTok{ggplot}\NormalTok{(house\_prices, }
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ log10\_size, }\AttributeTok{y =}\NormalTok{ log10\_price, }\AttributeTok{col =}\NormalTok{ condition)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.05}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_parallel\_slopes}\NormalTok{(}\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{"log10 price"}\NormalTok{, }
       \AttributeTok{x =} \StringTok{"log10 size"}\NormalTok{, }
       \AttributeTok{title =} \StringTok{"House prices in Seattle"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/house-price-parallel-slopes-1} 

}

\caption{Interaction and parallel slopes models.}\label{fig:house-price-parallel-slopes}
\end{figure}

In both cases, we see there is a positive relationship between house price and size, meaning as houses are larger, they tend to be more expensive. Furthermore, in both plots it seems that houses of condition 5 tend to be the most expensive for most house sizes as evidenced by the fact that the line for condition 5 is highest, followed by conditions 4 and 3. As for conditions 1 and 2, this pattern isn't as clear. Recall from the univariate barplot of \texttt{condition} in Figure \ref{fig:house-prices-viz}, there are only a few houses of condition 1 or 2.

Let's also show a faceted version of just the interaction model in Figure \ref{fig:house-price-interaction-2}. It is now much more apparent just how few houses are of condition 1 or 2.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(house\_prices, }
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ log10\_size, }\AttributeTok{y =}\NormalTok{ log10\_price, }\AttributeTok{col =}\NormalTok{ condition)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.4}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{y =} \StringTok{"log10 price"}\NormalTok{, }
       \AttributeTok{x =} \StringTok{"log10 size"}\NormalTok{, }
       \AttributeTok{title =} \StringTok{"House prices in Seattle"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ condition)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/house-price-interaction-2-1} 

}

\caption{Faceted plot of interaction model.}\label{fig:house-price-interaction-2}
\end{figure}

Which exploratory visualization of the interaction model is better, the one in the left-hand plot of Figure \ref{fig:house-price-parallel-slopes} or the faceted version in Figure \ref{fig:house-price-interaction-2}? There is no universal right answer. You need to make a choice depending on what you want to convey, and own that choice, with including and discussing both also as an option as needed.

\hypertarget{house-prices-regression}{%
\subsection{Regression modeling}\label{house-prices-regression}}

Which of the two models in Figure \ref{fig:house-price-parallel-slopes} is ``better''? The interaction model in the left-hand plot or the parallel slopes model in the right-hand plot?

We had a similar discussion in Subsection \ref{model-selection} on \emph{model selection}. Recall that we stated that we should only favor more complex models if the additional complexity is \emph{warranted}. In this case, the more complex model is the interaction model since it considers five intercepts and five slopes total. This is in contrast to the parallel slopes model which considers five intercepts but only one common slope.

Is the additional complexity of the interaction model warranted? Looking at the left-hand plot in Figure \ref{fig:house-price-parallel-slopes}, we're of the opinion that it is, as evidenced by the slight x-like pattern to some of the lines. Therefore, we'll focus the rest of this analysis only on the interaction model. This visual approach is somewhat subjective, however, so feel free to disagree! What are the five different slopes and five different intercepts for the interaction model? We can obtain these values from the regression table. Recall our two-step process for getting the regression table:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit regression model:}
\NormalTok{price\_interaction }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(log10\_price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ log10\_size }\SpecialCharTok{*}\NormalTok{ condition, }
                        \AttributeTok{data =}\NormalTok{ house\_prices)}

\CommentTok{\# Get regression table:}
\FunctionTok{get\_regression\_table}\NormalTok{(price\_interaction)}
\end{Highlighting}
\end{Shaded}

\begin{table}[!h]

\caption{\label{tab:seattle-interaction}Regression table for interaction model}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{lrrrrrr}
\toprule
term & estimate & std\_error & statistic & p\_value & lower\_ci & upper\_ci\\
\midrule
intercept & 3.330 & 0.451 & 7.380 & 0.000 & 2.446 & 4.215\\
log10\_size & 0.690 & 0.148 & 4.652 & 0.000 & 0.399 & 0.980\\
condition2 & 0.047 & 0.498 & 0.094 & 0.925 & -0.930 & 1.024\\
condition3 & -0.367 & 0.452 & -0.812 & 0.417 & -1.253 & 0.519\\
condition4 & -0.398 & 0.453 & -0.879 & 0.380 & -1.286 & 0.490\\
condition5 & -0.883 & 0.457 & -1.931 & 0.053 & -1.779 & 0.013\\
log10\_size:condition2 & -0.024 & 0.163 & -0.148 & 0.882 & -0.344 & 0.295\\
log10\_size:condition3 & 0.133 & 0.148 & 0.893 & 0.372 & -0.158 & 0.424\\
log10\_size:condition4 & 0.146 & 0.149 & 0.979 & 0.328 & -0.146 & 0.437\\
log10\_size:condition5 & 0.310 & 0.150 & 2.067 & 0.039 & 0.016 & 0.604\\
\bottomrule
\end{tabular}
\end{table}

Recall we saw in Subsection \ref{model4interactiontable} how to interpret a regression table when there are both numerical and categorical explanatory variables. Let's now do the same for all 10 values in the \texttt{estimate} column of Table \ref{tab:seattle-interaction}.

In this case, the ``baseline for comparison'' group for the categorical variable \texttt{condition} are the condition 1 houses, since ``1'' comes first alphanumerically. Thus, the \texttt{intercept} and \texttt{log10\_size} values are the intercept and slope for \texttt{log10\_size} for this baseline group. Next, the \texttt{condition2} through \texttt{condition5} terms are the \emph{offsets} in intercepts relative to the condition 1 intercept. Finally, the \texttt{log10\_size:condition2} through \texttt{log10\_size:condition5} are the \emph{offsets} in slopes for \texttt{log10\_size} relative to the condition 1 slope for \texttt{log10\_size}.

Let's simplify this by writing out the equation of each of the five regression lines using these 10 \texttt{estimate} values. We'll write out each line in the following format:

\[
\widehat{\log10(\text{price})} = \hat{\beta}_0 + \hat{\beta}_{\text{size}} \cdot \log10(\text{size})
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Condition 1:
\end{enumerate}

\[\widehat{\log10(\text{price})} = 3.33 + 0.69 \cdot \log10(\text{size})\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Condition 2:
\end{enumerate}

\[
\begin{aligned} 
\widehat{\log10(\text{price})} &= (3.33 + 0.047) + (0.69 - 0.024) \cdot \log10(\text{size}) \\ 
                               &= 3.377 + 0.666 \cdot \log10(\text{size})
\end{aligned}
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Condition 3:
\end{enumerate}

\[
\begin{aligned} 
\widehat{\log10(\text{price})} &= (3.33 - 0.367) + (0.69 + 0.133) \cdot \log10(\text{size}) \\
                               &= 2.963 + 0.823 \cdot \log10(\text{size})
\end{aligned}
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Condition 4:
\end{enumerate}

\[
\begin{aligned}
\widehat{\log10(\text{price})} &= (3.33 - 0.398) + (0.69 + 0.146) \cdot \log10(\text{size}) \\
                               &= 2.932 + 0.836 \cdot \log10(\text{size})
\end{aligned}
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Condition 5:
\end{enumerate}

\[
\begin{aligned}
\widehat{\log10(\text{price})} &= (3.33 - 0.883) + (0.69 + 0.31) \cdot \log10(\text{size}) \\
                               &= 2.447 + 1 \cdot \log10(\text{size})
\end{aligned}
\]

These correspond to the regression lines in the left-hand plot of Figure \ref{fig:house-price-parallel-slopes} and the faceted plot in Figure \ref{fig:house-price-interaction-2}. For homes of all five condition types, as the size of the house increases, the price increases. This is what most would expect. However, the rate of increase of price with size is fastest for the homes with conditions 3, 4, and 5 of 0.823, 0.836, and 1, respectively. These are the three largest slopes out of the five.

\hypertarget{house-prices-making-predictions}{%
\subsection{Making predictions}\label{house-prices-making-predictions}}

Say you're a realtor and someone calls you asking you how much their home will sell for. They tell you that it's in condition = 5 and is sized 1900 square feet. What do you tell them? Let's use the interaction model we fit to make predictions!

We first make this prediction visually in Figure \ref{fig:house-price-interaction-3}. The predicted \texttt{log10\_price} of this house is marked with a black dot. This is where the following two lines intersect:

\begin{itemize}
\tightlist
\item
  The regression line for the condition = 5 homes and
\item
  The vertical dashed black line at \texttt{log10\_size} equals 3.28, since our predictor variable is the log10 transformed square feet of living space of \(\log10(1900) = 3.28\).
\end{itemize}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/house-price-interaction-3-1} 

}

\caption{Interaction model with prediction.}\label{fig:house-price-interaction-3}
\end{figure}

Eyeballing it, it seems the predicted \texttt{log10\_price} seems to be around 5.75. Let's now obtain the exact numerical value for the prediction using the equation of the regression line for the condition = 5 houses, being sure to \texttt{log10()} the square footage first.

\begin{Shaded}
\begin{Highlighting}[]
\FloatTok{2.45} \SpecialCharTok{+} \DecValTok{1} \SpecialCharTok{*} \FunctionTok{log10}\NormalTok{(}\DecValTok{1900}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 5.73
\end{verbatim}

This value is very close to our earlier visually made prediction of 5.75. But wait! Is our prediction for the price of this house \$5.75? No! Remember that we are using \texttt{log10\_price} as our outcome variable! So, if we want a prediction in dollar units of \texttt{price}, we need to unlog this by taking a power of 10 as described in Appendix \ref{appendix-log10-transformations}.

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{10}\SpecialCharTok{\^{}}\NormalTok{(}\FloatTok{2.45} \SpecialCharTok{+} \DecValTok{1} \SpecialCharTok{*} \FunctionTok{log10}\NormalTok{(}\DecValTok{1900}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 535493
\end{verbatim}

So our predicted price for this home of condition 5 and of size 1900 square feet is \$535,493.

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC11.1)} Repeat the regression modeling in Subsection \ref{house-prices-regression} and the prediction making you just did on the house of condition 5 and size 1900 square feet in Subsection \ref{house-prices-making-predictions}, but using the parallel slopes model you visualized in Figure \ref{fig:house-price-parallel-slopes}. Show that it's \$524,807!

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\hypertarget{data-journalism}{%
\section{Case study: Effective data storytelling}\label{data-journalism}}

As we've progressed throughout this book, you've seen how to work with data in a variety of ways. You've learned effective strategies for plotting data by understanding which types of plots work best for which combinations of variable types. You've summarized data in spreadsheet form and calculated summary statistics for a variety of different variables. Furthermore, you've seen the value of statistical inference as a process to come to conclusions about a population by using sampling. Lastly, you've explored how to fit linear regression models and the importance of checking the conditions required so that all confidence intervals and hypothesis tests have valid interpretation. All throughout, you've learned many computational techniques and focused on writing R code that's reproducible.

We now present another set of case studies, but this time on the ``effective data storytelling'' done by data journalists around the world. Great data stories don't mislead the reader, but rather engulf them in understanding the importance that data plays in our lives through storytelling.

\hypertarget{bechdel-test-for-hollywood-gender-representation}{%
\subsection{Bechdel test for Hollywood gender representation}\label{bechdel-test-for-hollywood-gender-representation}}

We recommend you read and analyze Walt Hickey's\index{Hickey, Walt} FiveThirtyEight.com article, \href{http://fivethirtyeight.com/features/the-dollar-and-cents-case-against-hollywoods-exclusion-of-women/}{``The Dollar-And-Cents Case Against Hollywood's Exclusion of Women.''} In it, Walt completed a multidecade study of how many movies pass the \href{https://bechdeltest.com/}{Bechdel test}, an informal test of gender representation in a movie that was created by \index{Bechdel, Alison} Alison Bechdel.

As you read over the article, think carefully about how Walt Hickey is using data, graphics, and analyses to tell the reader a story. In the spirit of reproducibility, FiveThirtyEight have also shared the \href{https://github.com/fivethirtyeight/data/tree/master/bechdel}{data and R code} that they used for this article. You can also find the data used in many more of their articles on their \href{https://github.com/fivethirtyeight/data}{GitHub} page.

\emph{ModernDive} co-authors Chester Ismay and Albert Y. Kim along with Jennifer Chunn went one step further by creating the \texttt{fivethirtyeight} package which provides access to these datasets more easily in R. For a complete list of all 129 datasets included in the \texttt{fivethirtyeight} package, check out the package webpage at \url{https://fivethirtyeight-r.netlify.app/articles/fivethirtyeight.html}.\index{R packages!fivethirtyeight}

Furthermore, example ``vignettes'' of fully reproducible start-to-finish analyses of some of these data using \texttt{dplyr}, \texttt{ggplot2}, and other packages in the \texttt{tidyverse} are available \href{https://fivethirtyeight-r.netlify.app/articles/}{here}. For example, a vignette showing how to reproduce one of the plots at the end of the article on the Bechdel test is available \href{https://fivethirtyeight-r.netlify.app/articles/bechdel.html}{here}.

\hypertarget{us-births-in-1999}{%
\subsection{US Births in 1999}\label{us-births-in-1999}}

The \texttt{US\_births\_1994\_2003} data frame included in the \texttt{fivethirtyeight} package provides information about the number of daily births in the United States between 1994 and 2003. For more information on this data frame including a link to the original article on FiveThirtyEight.com, check out the help file by running \texttt{?US\_births\_1994\_2003} in the console.

It's always a good idea to preview your data, either by using RStudio's spreadsheet \texttt{View()} function or using \texttt{glimpse()} from the \texttt{dplyr} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(US\_births\_1994\_2003)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 3,652
Columns: 6
$ year          <int> 1994, 1994, 1994, 1994, 1994, 1994, 1994, 1994, ...
$ month         <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
$ date_of_month <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1...
$ date          <date> 1994-01-01, 1994-01-02, 1994-01-03, 1994-01-04,...
$ day_of_week   <ord> Sat, Sun, Mon, Tues, Wed, Thurs, Fri, Sat, Sun, ...
$ births        <int> 8096, 7772, 10142, 11248, 11053, 11406, 11251, 8...
\end{verbatim}

We'll focus on the number of \texttt{births} for each \texttt{date}, but only for births that occurred in 1999. Recall from Section \ref{filter} we can do this using the \texttt{filter()} function from the \texttt{dplyr} package:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{US\_births\_1999 }\OtherTok{\textless{}{-}}\NormalTok{ US\_births\_1994\_2003 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(year }\SpecialCharTok{==} \DecValTok{1999}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

As discussed in Section \ref{linegraphs}, since \texttt{date} is a notion of time and thus has sequential ordering to it, a linegraph would be a more appropriate visualization to use than a scatterplot. In other words, we should use a \texttt{geom\_line()} instead of \texttt{geom\_point()}. Recall that such plots are called \index{time series plots} \emph{time series} plots.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(US\_births\_1999, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ date, }\AttributeTok{y =}\NormalTok{ births)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Date"}\NormalTok{, }
       \AttributeTok{y =} \StringTok{"Number of births"}\NormalTok{, }
       \AttributeTok{title =} \StringTok{"US Births in 1999"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/us-births-1} 

}

\caption{Number of births in the US in 1999.}\label{fig:us-births}
\end{figure}

We see a big dip occurring just before January 1st, 2000, most likely due to the holiday season. However, what about the large spike of over 14,000 births occurring just before October 1st, 1999? What could be the reason for this anomalously high spike?

Let's sort the rows of \texttt{US\_births\_1999} in descending order of the number of births. Recall from Section \ref{arrange} that we can use the \texttt{arrange()} function from the \texttt{dplyr} function to do this, making sure to sort \texttt{births} in \texttt{desc}ending order:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{US\_births\_1999 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(births))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 365 x 6
    year month date_of_month date       day_of_week births
   <int> <int>         <int> <date>     <ord>        <int>
 1  1999     9             9 1999-09-09 Thurs        14540
 2  1999    12            21 1999-12-21 Tues         13508
 3  1999     9             8 1999-09-08 Wed          13437
 4  1999     9            21 1999-09-21 Tues         13384
 5  1999     9            28 1999-09-28 Tues         13358
 6  1999     7             7 1999-07-07 Wed          13343
 7  1999     7             8 1999-07-08 Thurs        13245
 8  1999     8            17 1999-08-17 Tues         13201
 9  1999     9            10 1999-09-10 Fri          13181
10  1999    12            28 1999-12-28 Tues         13158
# ... with 355 more rows
\end{verbatim}

The date with the highest number of births (14,540) is in fact 1999-09-09. If we write down this date in month/day/year format (a standard format in the US), the date with the highest number of births is 9/9/99! All nines! Could it be that parents deliberately induced labor at a higher rate on this date? Maybe? Whatever the cause may be, this fact makes a fun story!

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

\textbf{(LC11.2)} What date between 1994 and 2003 has the fewest number of births in the US? What story could you tell about why this is the case?

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

Time to think with data and further tell your story with data! How could statistical modeling help you here? What types of statistical inference would be helpful? What else can you find and where can you take this analysis? What assumptions did you make in this analysis? We leave these questions to you as the reader to explore and examine.

Remember to get in touch with us via our contact info in the Preface. We'd love to see what you come up with!

Please check out additional problem sets and labs at \url{https://moderndive.com/labs} as well.

\hypertarget{scripts-of-r-code}{%
\subsection{Scripts of R code}\label{scripts-of-r-code}}

An R script file of all R code used in this chapter is available at \url{https://www.moderndive.com/scripts/11-tell-your-story-with-data.R}.

R code files saved as *.R files for all relevant chapters throughout the entire book are in the following table.

\begin{tabular}{r|l}
\hline
chapter & link\\
\hline
1 & https://moderndive.com/scripts/01-getting-started.R\\
\hline
2 & https://moderndive.com/scripts/02-visualization.R\\
\hline
3 & https://moderndive.com/scripts/03-wrangling.R\\
\hline
4 & https://moderndive.com/scripts/04-tidy.R\\
\hline
5 & https://moderndive.com/scripts/05-regression.R\\
\hline
6 & https://moderndive.com/scripts/06-multiple-regression.R\\
\hline
7 & https://moderndive.com/scripts/07-sampling.R\\
\hline
8 & https://moderndive.com/scripts/08-confidence-intervals.R\\
\hline
9 & https://moderndive.com/scripts/09-hypothesis-testing.R\\
\hline
10 & https://moderndive.com/scripts/10-inference-for-regression.R\\
\hline
11 & https://moderndive.com/scripts/11-tell-your-story-with-data.R\\
\hline
\end{tabular}

\hypertarget{concluding-remarks}{%
\section*{Concluding remarks}\label{concluding-remarks}}


Now that you've made it to this point in the book, we suspect that you know a thing or two about how to work with data in R! You've also gained a lot of knowledge about how to use simulation-based techniques for statistical inference and how these techniques help build intuition about traditional theory-based inferential methods like the \(t\)-test.

The hope is that you've come to appreciate the power of data in all respects, such as data wrangling, tidying datasets, data visualization, data modeling, and statistical inference. In our opinion, while each of these is important, data visualization may be the most important tool for a citizen or professional data scientist to have in their toolbox. If you can create truly beautiful graphics that display information in ways that the reader can clearly understand, you have great power to tell your tale with data. Let's hope that these skills help you tell great stories with data into the future. Thanks for coming along this journey as we dove into modern data analysis using R and the \texttt{tidyverse}!

\hypertarget{part-hr-analytics-practical-examples}{%
\part{HR Analytics practical examples}\label{part-hr-analytics-practical-examples}}

\hypertarget{pay-gap}{%
\chapter{Gender Pay Gap}\label{pay-gap}}

According to a recent Glassdoor survey, more than two-thirds (67 percent)
of U.S. employees say they would not apply for jobs at employers where they
believe a gender pay gap exists.1 Today, the gender pay gap is more than
a social or legal issue. It's an issue that can affect the ability of employers to
attract and retain talent.

How should HR practitioners react to concerns about the gender pay gap?
One increasingly popular way is to perform an internal gender pay audit to
understand whether a gap exists at your company. This involves examining
your own payroll data for evidence of a gender pay gap, and making
recommendations to senior management about ways to lower gender
barriers in recruitment, hiring, pay and promotion before they arise as
broader organizational concerns.

The following example appeared on the Glassdor website in March 2017, with the title ``How to Audit Your Gender Pay Gap: An Employers Guide Using R'' and was written by Andrew Chamberlain, Ph.D.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load R libraries.}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(tidymodels)}
\FunctionTok{library}\NormalTok{(devtools)}
\CommentTok{\#install\_github("Hendrik147/HR\_Analytics\_in\_R\_book\_v2/HRAnalytics")}
\FunctionTok{library}\NormalTok{(HRAnalytics) }\CommentTok{\# Helper package}

\CommentTok{\#Turn off scientific notation. }
\FunctionTok{options}\NormalTok{(}\AttributeTok{scipen =} \DecValTok{999}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load data. }
\NormalTok{gd\_data }\OtherTok{=} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"https://glassdoor.box.com/shared/static/beukjzgrsu35fqe59f7502hruribd5tt.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
-- Column specification ---------------------------------------------------
cols(
  jobTitle = col_character(),
  gender = col_character(),
  age = col_double(),
  perfEval = col_double(),
  edu = col_character(),
  dept = col_character(),
  seniority = col_double(),
  basePay = col_double(),
  bonus = col_double()
)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# N = 1000 total observations.}
\end{Highlighting}
\end{Shaded}

\hypertarget{data-cleaning-and-prep.}{%
\section{Data Cleaning and Prep.}\label{data-cleaning-and-prep.}}

Before we can analyse whether we have gender pay gap, we need to prepare our data so it is cleaned up and has useful columns we can analyse.

We will perform the following steps in our process:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Make a number of sensible age brackets e.g.~\textless25, 25-34, 35-44, 45-54, \textgreater54
\item
  Create total compensation variable (base pay + bonus)
\item
  Take natural logarithm of compensation variables (for percentage pay gap interpretation in regressions).
\item
  Convert values like department into values that can act as flags in our model and making the most commonly occuring value the default which everything else gets compared to in our model.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gd\_data }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# Age brackets}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{age\_bin =} \FunctionTok{cut}\NormalTok{(age,}
                  \AttributeTok{breaks =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{35}\NormalTok{, }\DecValTok{45}\NormalTok{, }\DecValTok{55}\NormalTok{, }\ConstantTok{Inf}\NormalTok{),}
                  \AttributeTok{right =} \ConstantTok{FALSE}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# Total compensation}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{total\_pay =}\NormalTok{ basePay }\SpecialCharTok{+}\NormalTok{ bonus) }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# Log of compensation}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{log\_base =} \FunctionTok{log}\NormalTok{(basePay, }\AttributeTok{base =} \FunctionTok{exp}\NormalTok{(}\DecValTok{1}\NormalTok{)),}
         \AttributeTok{log\_total =} \FunctionTok{log}\NormalTok{(total\_pay, }\AttributeTok{base =} \FunctionTok{exp}\NormalTok{(}\DecValTok{1}\NormalTok{)),}
         \CommentTok{\# Adds 1 to allow for log of 0 bonus values.}
         \AttributeTok{log\_bonus =} \FunctionTok{log}\NormalTok{(bonus }\SpecialCharTok{+} \DecValTok{1}\NormalTok{, }\AttributeTok{base =} \FunctionTok{exp}\NormalTok{(}\DecValTok{1}\NormalTok{))) }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# Make flags}
  \FunctionTok{mutate\_if}\NormalTok{(is\_character, fct\_infreq) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{age\_bin =} \FunctionTok{fct\_infreq}\NormalTok{(age\_bin)) }\OtherTok{{-}\textgreater{}}
\NormalTok{  gd\_data\_clean}
\end{Highlighting}
\end{Shaded}

\hypertarget{summary-statistics-by-gender}{%
\section{Summary Statistics by gender}\label{summary-statistics-by-gender}}

We can look at summary statistics by gender.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Base pay summary stats.}
\NormalTok{gd\_data\_clean }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# Exclude stuff with missing info}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(basePay)) }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# Analyse by gender}
  \FunctionTok{group\_by}\NormalTok{(gender) }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# Retrieve summary statistics}
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{mean\_base =} \FunctionTok{mean}\NormalTok{(basePay),}
    \AttributeTok{median\_base =} \FunctionTok{median}\NormalTok{(basePay),}
    \AttributeTok{count =} \FunctionTok{n}\NormalTok{()}
\NormalTok{  ) }\OtherTok{{-}\textgreater{}}
\NormalTok{  gd\_summary\_gender\_base}

\NormalTok{gd\_summary\_gender\_base}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 4
  gender mean_base median_base count
  <fct>      <dbl>       <dbl> <int>
1 Male     98457.5     98223     532
2 Female   89942.8     89913.5   468
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Total pay summary stats.}
\NormalTok{gd\_data\_clean }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# Use a helper function to save typing}
\NormalTok{  HRAnalytics}\SpecialCharTok{::}\FunctionTok{gap\_summary}\NormalTok{(gender, total\_pay) }\OtherTok{{-}\textgreater{}}
\NormalTok{  gd\_summary\_gender\_total}

\NormalTok{gd\_summary\_gender\_total}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 4
  gender     mean  median count
  <fct>     <dbl>   <dbl> <int>
1 Male   104919.  105100.   532
2 Female  96416.8  96571    468
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Bonus summary stats. }
\NormalTok{gd\_data\_clean }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# Use a helper function to save typing}
\NormalTok{  HRAnalytics}\SpecialCharTok{::}\FunctionTok{gap\_summary}\NormalTok{(gender, bonus) }\OtherTok{{-}\textgreater{}}
\NormalTok{  gd\_summary\_gender\_bonus}

\NormalTok{gd\_summary\_gender\_bonus}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 4
  gender    mean median count
  <fct>    <dbl>  <dbl> <int>
1 Male   6461.13 6480.5   532
2 Female 6474.01 6553     468
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Performance evaluations summary stats. }
\NormalTok{gd\_data\_clean }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# Use a helper function to save typing}
\NormalTok{  HRAnalytics}\SpecialCharTok{::}\FunctionTok{gap\_summary}\NormalTok{(gender, perfEval)  }\OtherTok{{-}\textgreater{}}
\NormalTok{  gd\_summary\_gender\_perf}

\NormalTok{gd\_summary\_gender\_perf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 4
  gender    mean median count
  <fct>    <dbl>  <dbl> <int>
1 Male   3.12594      3   532
2 Female 2.93590      3   468
\end{verbatim}

\hypertarget{avoiding-simpsons-paradox}{%
\section{Avoiding Simpson's Paradox}\label{avoiding-simpsons-paradox}}

Simspson's Paradox is where some high level aggregations lead to a different conclusion than would have been arrived at if a lower level of detail had been looked at. We should check out values split by department and job title to see if they showcase wage differences.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Performance evaluations summary stats. }
\NormalTok{gd\_data\_clean }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(total\_pay)) }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# Create a summary by department and gender}
  \FunctionTok{group\_by}\NormalTok{(dept, gender) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{mean\_perf =} \FunctionTok{mean}\NormalTok{(total\_pay),}
    \AttributeTok{median\_perf =} \FunctionTok{median}\NormalTok{(total\_pay),}
    \AttributeTok{count =} \FunctionTok{n}\NormalTok{()}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# "Unpivot" the data}
  \FunctionTok{gather}\NormalTok{(measure, value, mean\_perf}\SpecialCharTok{:}\NormalTok{count) }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# Combine the gender with measure}
  \FunctionTok{unite}\NormalTok{(combo, measure, gender) }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# "Pivot" the data to see all the measures split by gender}
  \FunctionTok{spread}\NormalTok{(combo, value) }\OtherTok{{-}\textgreater{}}
\NormalTok{  gd\_summary\_dept\_gender\_total}

\NormalTok{gd\_summary\_dept\_gender\_total}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 5 x 7
# Groups:   dept [5]
  dept  count_Female count_Male mean_perf_Female mean_perf_Male
  <fct>        <dbl>      <dbl>            <dbl>          <dbl>
1 Oper~           96        114          93098.5        99214.7
2 Sales          101        106          99222.3       108388. 
3 Mana~           87        111          99172.1       106201. 
4 Admi~           95         98          93429.2       102680. 
5 Engi~           89        103          97308.1       108408. 
# ... with 2 more variables: median_perf_Female <dbl>,
#   median_perf_Male <dbl>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Performance evaluations summary stats. }
\NormalTok{gd\_data\_clean }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(total\_pay)) }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# Create a summary by job and gender}
  \FunctionTok{group\_by}\NormalTok{(jobTitle, gender) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{mean\_perf =} \FunctionTok{mean}\NormalTok{(total\_pay),}
    \AttributeTok{median\_perf =} \FunctionTok{median}\NormalTok{(total\_pay),}
    \AttributeTok{count =} \FunctionTok{n}\NormalTok{()}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# "Unpivot" the data}
  \FunctionTok{gather}\NormalTok{(measure, value, mean\_perf}\SpecialCharTok{:}\NormalTok{count) }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# Combine the gender with measure}
  \FunctionTok{unite}\NormalTok{(combo, measure, gender) }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# "Pivot" the data to see all the measures split by gender}
  \FunctionTok{spread}\NormalTok{(combo, value) }\OtherTok{{-}\textgreater{}}
\NormalTok{  gd\_summary\_job\_gender\_total}

\NormalTok{gd\_summary\_job\_gender\_total}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 10 x 7
# Groups:   jobTitle [10]
   jobTitle count_Female count_Male mean_perf_Female mean_perf_Male
   <fct>           <dbl>      <dbl>            <dbl>          <dbl>
 1 Marketi~          107         11          82251.1        88161.6
 2 Softwar~            8        101         101147.        113034. 
 3 Data Sc~           53         54         102452.         95450  
 4 Financi~           49         58         101744.        101116. 
 5 Graphic~           48         50          98810.4        96584.1
 6 IT                 50         46          96837.9        97239.6
 7 Sales A~           43         51          98220.5       100894. 
 8 Driver             46         45          93197.3        98417.8
 9 Manager            18         72         133244.        130929. 
10 Warehou~           46         44         100084.         93363.6
# ... with 2 more variables: median_perf_Female <dbl>,
#   median_perf_Male <dbl>
\end{verbatim}

\hypertarget{model-estimation-ols-with-controls.}{%
\section{Model Estimation: OLS with controls.}\label{model-estimation-ols-with-controls.}}

Coefficient on ``male'' has the interpretation of approximate male pay advantage (``gender pay gap'').

\hypertarget{logarithm-of-base-pay}{%
\subsection{Logarithm of Base Pay}\label{logarithm-of-base-pay}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# No controls. ("unadjusted" pay gap.)}
\NormalTok{lm\_gender }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(log\_base }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gender, }\AttributeTok{data =}\NormalTok{ gd\_data\_clean)}

\CommentTok{\# Adding "human capital" controls (performance evals, age and education).}
\NormalTok{lm\_humancapital }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(log\_base }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ perfEval }\SpecialCharTok{+}\NormalTok{ age\_bin }\SpecialCharTok{+}\NormalTok{ edu, }\AttributeTok{data =}\NormalTok{ gd\_data\_clean)}

\CommentTok{\# Adding all controls. ("adjusted" pay gap.)}
\NormalTok{lm\_allcontrols }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(log\_base }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gender }\SpecialCharTok{+}\NormalTok{ perfEval }\SpecialCharTok{+}\NormalTok{ age\_bin }\SpecialCharTok{+}\NormalTok{ edu }\SpecialCharTok{+}\NormalTok{ dept }\SpecialCharTok{+}\NormalTok{ seniority }\SpecialCharTok{+}\NormalTok{ jobTitle, }\AttributeTok{data =}\NormalTok{ gd\_data\_clean)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_gender }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Call:
lm(formula = log_base ~ gender, data = gd_data_clean)

Residuals:
    Min      1Q  Median      3Q     Max 
-0.9529 -0.1610  0.0356  0.2080  0.6374 

Coefficients:
             Estimate Std. Error t value             Pr(>|t|)    
(Intercept)   11.4618     0.0123  933.21 < 0.0000000000000002 ***
genderFemale  -0.0953     0.0180   -5.31           0.00000014 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.283 on 998 degrees of freedom
Multiple R-squared:  0.0275,    Adjusted R-squared:  0.0265 
F-statistic: 28.2 on 1 and 998 DF,  p-value: 0.000000136
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_gender }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# Get the predicted values}
  \FunctionTok{augment}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# Rename columns to more friendly names}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{actual =}\NormalTok{ log\_base, }\AttributeTok{predicted =}\NormalTok{ .fitted) }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# Build a chart}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \CommentTok{\# Add columns to it}
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{actual, }\AttributeTok{y=}\NormalTok{predicted) }\SpecialCharTok{+}
  \CommentTok{\# Choose chart type}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \CommentTok{\# Add a diagonal line representing perfect predictions}
  \FunctionTok{geom\_abline}\NormalTok{(}\AttributeTok{colour=}\StringTok{"blue"}\NormalTok{, }\AttributeTok{slope=}\DecValTok{1}\NormalTok{, }\AttributeTok{intercept=}\DecValTok{0}\NormalTok{)}\SpecialCharTok{+}
  \CommentTok{\# Split by gender}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{gender) }\SpecialCharTok{+}
  \CommentTok{\# Add some labels}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title=}\StringTok{"Actual vs predicted"}\NormalTok{, }
       \AttributeTok{subtitle=}\StringTok{"Values predicted using a linear model containing gender"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/lm_gender viz-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_humancapital }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Call:
lm(formula = log_base ~ gender + perfEval + age_bin + edu, data = gd_data_clean)

Residuals:
    Min      1Q  Median      3Q     Max 
-0.7985 -0.1384  0.0216  0.1550  0.5663 

Coefficients:
               Estimate Std. Error t value             Pr(>|t|)    
(Intercept)    11.62854    0.02500  465.06 < 0.0000000000000002 ***
genderFemale   -0.10019    0.01472   -6.81       0.000000000017 ***
perfEval       -0.00696    0.00516   -1.35                0.178    
age_bin[25,35) -0.35168    0.02191  -16.05 < 0.0000000000000002 ***
age_bin[45,55) -0.11506    0.02206   -5.21       0.000000223886 ***
age_bin[35,45) -0.19201    0.02238   -8.58 < 0.0000000000000002 ***
age_bin[0,25)  -0.43181    0.02307  -18.72 < 0.0000000000000002 ***
eduMasters      0.08767    0.02029    4.32       0.000017159218 ***
eduCollege      0.03757    0.02056    1.83                0.068 .  
eduPhD          0.11096    0.02072    5.35       0.000000106653 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.231 on 990 degrees of freedom
Multiple R-squared:  0.361, Adjusted R-squared:  0.355 
F-statistic: 62.2 on 9 and 990 DF,  p-value: <0.0000000000000002
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_humancapital }\SpecialCharTok{\%\textgreater{}\%} 
  \CommentTok{\# Get the predicted values}
  \FunctionTok{augment}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \CommentTok{\# Rename columns to more friendly names}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{actual =}\NormalTok{ log\_base, }\AttributeTok{predicted =}\NormalTok{ .fitted) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  HRAnalytics}\SpecialCharTok{::}\FunctionTok{pred\_vs\_actuals}\NormalTok{() }\SpecialCharTok{+}
  \CommentTok{\# Add some labels}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title=}\StringTok{"Actual vs predicted"}\NormalTok{, }
       \AttributeTok{subtitle=}\StringTok{"Values predicted using a linear model containing human capital measures"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/lm_humancapital viz-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_allcontrols }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Call:
lm(formula = log_base ~ gender + perfEval + age_bin + edu + dept + 
    seniority + jobTitle, data = gd_data_clean)

Residuals:
    Min      1Q  Median      3Q     Max 
-0.5679 -0.0824  0.0099  0.0865  0.3378 

Coefficients:
                             Estimate Std. Error t value
(Intercept)                 11.006946   0.022887  480.93
genderFemale                -0.010945   0.009246   -1.18
perfEval                     0.000107   0.002948    0.04
age_bin[25,35)              -0.336882   0.012492  -26.97
age_bin[45,55)              -0.100828   0.012613   -7.99
age_bin[35,45)              -0.197028   0.012808  -15.38
age_bin[0,25)               -0.432314   0.013127  -32.93
eduMasters                   0.068456   0.011553    5.93
eduCollege                   0.012278   0.011735    1.05
eduPhD                       0.080431   0.011793    6.82
deptSales                    0.077778   0.012933    6.01
deptManagement               0.034401   0.013069    2.63
deptAdministration           0.002596   0.013114    0.20
deptEngineering              0.035649   0.013148    2.71
seniority                    0.108084   0.002990   36.15
jobTitleSoftware Engineer    0.345077   0.019067   18.10
jobTitleData Scientist       0.206233   0.017899   11.52
jobTitleFinancial Analyst    0.252215   0.018063   13.96
jobTitleGraphic Designer     0.178280   0.018388    9.70
jobTitleIT                   0.179373   0.018364    9.77
jobTitleSales Associate      0.212518   0.018659   11.39
jobTitleDriver               0.168299   0.018679    9.01
jobTitleManager              0.511504   0.019565   26.14
jobTitleWarehouse Associate  0.198159   0.018752   10.57
                                        Pr(>|t|)    
(Intercept)                 < 0.0000000000000002 ***
genderFemale                              0.2368    
perfEval                                  0.9709    
age_bin[25,35)              < 0.0000000000000002 ***
age_bin[45,55)                0.0000000000000037 ***
age_bin[35,45)              < 0.0000000000000002 ***
age_bin[0,25)               < 0.0000000000000002 ***
eduMasters                    0.0000000043195308 ***
eduCollege                                0.2957    
eduPhD                        0.0000000000158866 ***
deptSales                     0.0000000025571978 ***
deptManagement                            0.0086 ** 
deptAdministration                        0.8431    
deptEngineering                           0.0068 ** 
seniority                   < 0.0000000000000002 ***
jobTitleSoftware Engineer   < 0.0000000000000002 ***
jobTitleData Scientist      < 0.0000000000000002 ***
jobTitleFinancial Analyst   < 0.0000000000000002 ***
jobTitleGraphic Designer    < 0.0000000000000002 ***
jobTitleIT                  < 0.0000000000000002 ***
jobTitleSales Associate     < 0.0000000000000002 ***
jobTitleDriver              < 0.0000000000000002 ***
jobTitleManager             < 0.0000000000000002 ***
jobTitleWarehouse Associate < 0.0000000000000002 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.131 on 976 degrees of freedom
Multiple R-squared:  0.798, Adjusted R-squared:  0.793 
F-statistic:  167 on 23 and 976 DF,  p-value: <0.0000000000000002
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_allcontrols }\SpecialCharTok{\%\textgreater{}\%} 
  \CommentTok{\# Get the predicted values}
  \FunctionTok{augment}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \CommentTok{\# Rename columns to more friendly names}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{actual =}\NormalTok{ log\_base, }\AttributeTok{predicted =}\NormalTok{ .fitted) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  HRAnalytics}\SpecialCharTok{::}\FunctionTok{pred\_vs\_actuals}\NormalTok{() }\SpecialCharTok{+}
  \CommentTok{\# Add some labels}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title=}\StringTok{"Actual vs predicted"}\NormalTok{, }
       \AttributeTok{subtitle=}\StringTok{"Values predicted using a linear model all controls"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/lm_allcontrols viz-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#  Gather up all the models}
\FunctionTok{list}\NormalTok{(lm\_gender, lm\_humancapital, lm\_allcontrols) }\SpecialCharTok{\%\textgreater{}\%} 
  \CommentTok{\# Extract coefficients for all models at once and combine into a single table}
  \FunctionTok{map\_df}\NormalTok{(tidy, }\AttributeTok{.id =} \StringTok{"model"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \CommentTok{\# Let\textquotesingle{}s look at the impact of gender}
  \FunctionTok{filter}\NormalTok{(term}\SpecialCharTok{==}\StringTok{"genderFemale"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \CommentTok{\# P values less than 0.05 are usually taken to mean an estimate is reliable}
  \FunctionTok{select}\NormalTok{(model, }\AttributeTok{log\_gap=}\NormalTok{estimate, p.value)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 3
  model    log_gap     p.value
  <chr>      <dbl>       <dbl>
1 1     -0.0953156 1.35864e- 7
2 2     -0.100189  1.71745e-11
3 3     -0.0109446 2.36793e- 1
\end{verbatim}

Note that too little data can make results unreliable for a complex model because there are so few records to use for each combination of values.

\hypertarget{results-by-department}{%
\subsection{Results by Department}\label{results-by-department}}

(Interaction of gender x dept)
To test for differences by department, examine significance of each ``gender x dept'' coefficient.
For the gender pay gap by department, add the ``gender'' + ``gender x dept'' coefficients from this model.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# All controls with department interaction terms. }
\NormalTok{lm\_allcontrols\_dept }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(log\_base }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gender}\SpecialCharTok{*}\NormalTok{dept }\SpecialCharTok{+}\NormalTok{ perfEval }\SpecialCharTok{+}\NormalTok{ age\_bin }\SpecialCharTok{+}\NormalTok{ edu }\SpecialCharTok{+}\NormalTok{ seniority }\SpecialCharTok{+}\NormalTok{ jobTitle, }\AttributeTok{data =}\NormalTok{ gd\_data\_clean)}

\FunctionTok{tidy}\NormalTok{(lm\_allcontrols\_dept)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 28 x 5
   term                    estimate  std.error    statistic      p.value
   <chr>                      <dbl>      <dbl>        <dbl>        <dbl>
 1 (Intercept)        11.0016       0.0237984  462.284      0.          
 2 genderFemale        0.00152592   0.0184980    0.0824912  9.34273e-  1
 3 deptSales           0.0902054    0.0177766    5.07438    4.65639e-  7
 4 deptManagement      0.0426739    0.0176537    2.41728    1.58202e-  2
 5 deptAdministration  0.00182408   0.0181763    0.100355   9.20083e-  1
 6 deptEngineering     0.0446260    0.0180041    2.47865    1.33567e-  2
 7 perfEval           -0.0000291152 0.00295858  -0.00984092 9.92150e-  1
 8 age_bin[25,35)     -0.336629     0.0125206  -26.8860     1.73518e-119
 9 age_bin[45,55)     -0.100342     0.0126338   -7.94233    5.46121e- 15
10 age_bin[35,45)     -0.196490     0.0128459  -15.2959     1.73511e- 47
# ... with 18 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_allcontrols\_dept }\SpecialCharTok{\%\textgreater{}\%} 
  \CommentTok{\# Get the predicted values}
  \FunctionTok{augment}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \CommentTok{\# Rename columns to more friendly names}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{actual =}\NormalTok{ log\_base, }\AttributeTok{predicted =}\NormalTok{ .fitted) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  HRAnalytics}\SpecialCharTok{::}\FunctionTok{pred\_vs\_actuals}\NormalTok{() }\SpecialCharTok{+}
  \CommentTok{\# Add some labels}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title=}\StringTok{"Actual vs predicted"}\NormalTok{, }
       \AttributeTok{subtitle=}\StringTok{"Values predicted using a linear model all controls \& department interaction"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/lm_allcontrols_dept-1} \end{center}

\hypertarget{results-by-job-title}{%
\subsection{Results by Job Title}\label{results-by-job-title}}

(Interaction of gender x job )
To test for differences by department, examine significance of each ``gender x job title'' coefficient.
For the gender pay gap by job, add the ``gender'' + ``gender x job title'' coefficients from this model.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# All controls with department interaction terms. }
\NormalTok{lm\_allcontrols\_job }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(log\_base }\SpecialCharTok{\textasciitilde{}}\NormalTok{ gender}\SpecialCharTok{*}\NormalTok{jobTitle }\SpecialCharTok{+}\NormalTok{ perfEval }\SpecialCharTok{+}\NormalTok{ age\_bin }\SpecialCharTok{+}\NormalTok{ edu }\SpecialCharTok{+}\NormalTok{ seniority }\SpecialCharTok{+}\NormalTok{ dept, }\AttributeTok{data =}\NormalTok{ gd\_data\_clean)}

\FunctionTok{tidy}\NormalTok{(lm\_allcontrols\_job)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 33 x 5
   term                        estimate std.error statistic     p.value
   <chr>                          <dbl>     <dbl>     <dbl>       <dbl>
 1 (Intercept)               11.0638    0.0442857 249.828   0.         
 2 genderFemale              -0.0724912 0.0416612  -1.74002 8.21742e- 2
 3 jobTitleSoftware Engineer  0.289254  0.0418482   6.91198 8.66714e-12
 4 jobTitleData Scientist     0.142351  0.0435503   3.26866 1.11867e- 3
 5 jobTitleFinancial Analyst  0.202106  0.0433312   4.66421 3.53339e- 6
 6 jobTitleGraphic Designer   0.110316  0.0439206   2.51171 1.21768e- 2
 7 jobTitleIT                 0.130398  0.0441617   2.95273 3.22592e- 3
 8 jobTitleSales Associate    0.166820  0.0438793   3.80180 1.52646e- 4
 9 jobTitleDriver             0.106498  0.0442196   2.40840 1.62091e- 2
10 jobTitleManager            0.455642  0.0426281  10.6888  2.77904e-25
# ... with 23 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_allcontrols\_job }\SpecialCharTok{\%\textgreater{}\%} 
  \CommentTok{\# Get the predicted values}
  \FunctionTok{augment}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \CommentTok{\# Rename columns to more friendly names}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{actual =}\NormalTok{ log\_base, }\AttributeTok{predicted =}\NormalTok{ .fitted) }\SpecialCharTok{\%\textgreater{}\%} 
  \CommentTok{\# Build a chart}
\NormalTok{  HRAnalytics}\SpecialCharTok{::}\FunctionTok{pred\_vs\_actuals}\NormalTok{() }\SpecialCharTok{+}
  \CommentTok{\# Add some labels}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title=}\StringTok{"Actual vs predicted"}\NormalTok{, }
       \AttributeTok{subtitle=}\StringTok{"Values predicted using a linear model all controls \& job interaction"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/lm_allcontrols_job-1} \end{center}

For additional analysis via Oaxaca-Blinder decomposition, please see documentation for the \href{https://cran.r-project.org/package=oaxaca}{\texttt{oaxaca} package} in R.

\hypertarget{stop-appraisals}{%
\chapter{Stop appraisals}\label{stop-appraisals}}

How can HR give time back to the organisation? In a \href{https://hrtrendinstitute.com/2018/04/13/how-can-hr-give-time-back-to-the-organisation/}{recent post} Tom Haak from the HR Trend Institute suggested that a huge time saver was to stop the formal performance management process. Such ideas are constantly mentioned in management meetings. In fact, several companies are taking that route.

Tom Haak is mentioning an organisation in which they calculated that all the work around the performance management process for one employee costs manager and employee around 10 hours (preparation, two formal meetings per year, completing the online forms, meeting with HR to review the results etc.). By simplifying the process (no mandatory meetings, no forms, no review meetings, just one annual rating to be submitted per employee by the manager), HR was able to give back many hours to the organisation (to the relief of managers and employees).

So how would one go about, to introduce such change in an organisation. Here the Overhead value analysis (OVA) can come to the rescue. It is a technique used to find opportunities to reduce overhead costs \citep{keymanagementmodels2015}.

The technique previews the following steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The first step is to create a foundation including a definition of the required output, the required activities and an assessment of the end product.
\item
  The second step is to make an orderly listing of activities and costs. This step includes estimating the costs of input/resources, the costs of activities and the allocation of the cost to products, generally with the help of activity-based costing (ABC).
\item
  In step three, a customer evaluation of the service and output is required. Relevant aspects are necessity (i.e.~critical, desired or nice-to-have), quality, quantity and cost. Customers are asked for both an assessment of the current output and an indication of the improvements that need to be made. Both interviews and questionnaires are used in the customer evaluation.
\item
  In step 4, the OVA team must identify cost-saving opportunities based on the possible improvements identified. This forces the organisation to make a statement with regard to priorities for output and required activities.
\item
  Step 5 is to prioritise opportunities with the aid of the four elements used earlier in the customer evaluation:
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Is the activity adding value?
\item
  Is quality of output sufficient?
\item
  Is quantity of output sufficient?
\item
  Can it be done at reasonable cost?
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Finally, as a project in itself, the last step is to implement the set of changes discussed and decided upon in the previous five steps.
\end{enumerate}

In the following we will analyse the cost in appraising employees (step 2 of the OVA technique). Let us have a look at an organisation which utilises an activity based costing framework. An organisation in which staff working time is allocated to the different projects and activities, among others time spent on staff apparaisals.

We will use only the tidyverse library.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

First, let's import some data on self declared time spent on appraisals,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Time.spent.on.appraisals }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"https://hranalytics.netlify.com/data/Time.spent.on.appraisals.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(Hours }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Count.of.Appraisee,        }\CommentTok{\# plot the variables y \textasciitilde{} x}
     \AttributeTok{xlab=}\StringTok{"Number of appraisee"}\NormalTok{,        }\CommentTok{\# x−axis label }
     \AttributeTok{ylab=}\StringTok{"Hours"}\NormalTok{,                      }\CommentTok{\# y−axis label}
     \AttributeTok{data=}\NormalTok{Time.spent.on.appraisals)     }\CommentTok{\# data set}

\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Hours }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Count.of.Appraisee, }\AttributeTok{data=}\NormalTok{Time.spent.on.appraisals)}
\FunctionTok{abline}\NormalTok{(model, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-516-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#The following produces the formula of the linear regression}
\FunctionTok{paste}\NormalTok{(}\StringTok{\textquotesingle{}y =\textquotesingle{}}\NormalTok{, }\FunctionTok{round}\NormalTok{(}\FunctionTok{coef}\NormalTok{(model)[[}\DecValTok{2}\NormalTok{]], }\AttributeTok{digits =} \DecValTok{3}\NormalTok{), }\StringTok{\textquotesingle{}* x\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}+\textquotesingle{}}\NormalTok{, }\FunctionTok{round}\NormalTok{(}\FunctionTok{coef}\NormalTok{(model)[[}\DecValTok{1}\NormalTok{]], }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "y = 3.362 * x + 49.134"
\end{verbatim}

Is is now an interpolation or a linear regression?

Interpolation is a method of constructing new data points within the range of a discrete set of known data points. Regression analysis is a statistical process for estimating the relationships among variables.

y = 3.362 * x + 49.134" What does it mean?

From these numbers it would appear, that every manager spends on average 49 hours independently from the number of staff to appraise (for their own self-assessment and for the appraisal process in general, i.e.~reading material, management meetings, refreshing how to appraise staff and re-training with the IT tools). In addition every manager spends on average 3 hours and 20 minutes to appraise each indivual subordinate, initial meeting with staff member and then writing up the appraisal itself.

From looking at the chart, it would appear that there are a couple of outliers. In statistics, an outlier is defined as an observation which stands far away from the most of other observations. Often an outlier is present due to the measurements error. Therefore, one of the most important tasks in data analysis is to identify and only if it is necessary to remove the outlier. Recording time to the closest 15 minutes requires a lot of attention from the manager.

Let us identify the outliers and remove them. A script from Klodian Dhana, Creator of DataScience+, comes in handy: \url{https://datascienceplus.com/identify-describe-plot-and-removing-the-outliers-from-the-dataset/}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# First let us identify the outliers and plot the initial plot with the names of the appraiser}

\CommentTok{\# Calculate Mahalanobis Distance with height and weight distributions}
\NormalTok{m\_dist }\OtherTok{\textless{}{-}} \FunctionTok{mahalanobis}\NormalTok{(Time.spent.on.appraisals[,}\DecValTok{2}\SpecialCharTok{:}\DecValTok{3}\NormalTok{], }\FunctionTok{colMeans}\NormalTok{(Time.spent.on.appraisals[,}\DecValTok{2}\SpecialCharTok{:}\DecValTok{3}\NormalTok{], }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{), }\FunctionTok{cov}\NormalTok{(Time.spent.on.appraisals[,}\DecValTok{2}\SpecialCharTok{:}\DecValTok{3}\NormalTok{], }\AttributeTok{use=}\StringTok{"complete.obs"}\NormalTok{))}
\NormalTok{Time.spent.on.appraisals}\SpecialCharTok{$}\NormalTok{m\_dist }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(m\_dist, }\DecValTok{2}\NormalTok{)}

\CommentTok{\# A quick check by sorting the value will indicate the threshold you may want to use:}
\FunctionTok{sort}\NormalTok{(Time.spent.on.appraisals}\SpecialCharTok{$}\NormalTok{m\_dist)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1]  0.10  0.15  0.15  0.16  0.16  0.18  0.18  0.18  0.22  0.23  0.23
[12]  0.27  0.29  0.31  0.34  0.36  0.41  0.43  0.43  0.43  0.44  0.50
[23]  0.50  0.54  0.54  0.55  0.57  0.62  0.64  0.65  0.68  0.70  0.74
[34]  0.76  0.82  0.84  0.85  0.90  0.93  1.01  1.03  1.09  1.11  1.15
[45]  1.16  1.17  1.35  1.53  1.54  1.73  1.87  1.92  2.35  2.36  2.70
[56]  3.52  3.77  3.87  5.74  7.40  7.49 23.79 26.16
\end{verbatim}

The \href{https://www.statisticshowto.datasciencecentral.com/mahalanobis-distance/}{Mahalanobis distance (MD)} is the distance between two points in multivariate space. In a regular Euclidean space, variables (e.g.~x, y, z) are represented by axes drawn at right angles to each other; The distance between any two points can be measured with a ruler. For uncorrelated variables, the Euclidean distance equals the MD. However, if two or more variables are correlated, the axes are no longer at right angles, and the measurements become impossible with a ruler. In addition, if you have more than three variables, you can't plot them in regular 3D space at all. The MD solves this measurement problem, as it measures distances between points, even correlated points for multiple variables.

The Mahalanobis function returns the squared Mahalanobis distance of all rows in x and the vector mu = center with respect to Sigma = cov. This is (for vector x) defined as

D\^{}2 = (x - μ)' Σ\^{}-1 (x - μ)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# We chose to set the threshold at 4, as this would result in five outliers. There is no "rule of thumb" on where to set the threshold and the decision is up to the analyst.}

\CommentTok{\# Mahalanobis Outliers {-} Threshold set to 4}
\NormalTok{Time.spent.on.appraisals}\SpecialCharTok{$}\NormalTok{outlier\_maha }\OtherTok{\textless{}{-}} \StringTok{"No"}
\NormalTok{Time.spent.on.appraisals}\SpecialCharTok{$}\NormalTok{outlier\_maha[Time.spent.on.appraisals}\SpecialCharTok{$}\NormalTok{m\_dist }\SpecialCharTok{\textgreater{}} \DecValTok{4}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"Yes"}

\CommentTok{\# Scatterplot with Mahalanobis\textquotesingle{} Outliers}
\FunctionTok{ggplot}\NormalTok{(Time.spent.on.appraisals, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Count.of.Appraisee, }\AttributeTok{y =}\NormalTok{ Hours, }\AttributeTok{color =}\NormalTok{ outlier\_maha)) }\SpecialCharTok{+}
      \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{size =} \DecValTok{5}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.6}\NormalTok{, }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+}    \CommentTok{\# na.rm=TRUE removes missing values error message silently}
      \FunctionTok{xlim}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{40}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{ylim}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{600}\NormalTok{) }\SpecialCharTok{+}
      \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Number of appraisees vs Hours"}\NormalTok{,}
           \AttributeTok{subtitle =} \StringTok{"Outlier Detection in time spent on appraisals data {-} Using Mahalanobis Distances"}\NormalTok{) }\SpecialCharTok{+}
      \FunctionTok{ylab}\NormalTok{(}\StringTok{"Hours"}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{xlab}\NormalTok{(}\StringTok{"Number of appraisees"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-518-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_without\_mahaoutliers }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Hours }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Count.of.Appraisee, }\AttributeTok{data=}\NormalTok{Time.spent.on.appraisals[}\FunctionTok{which}\NormalTok{(Time.spent.on.appraisals}\SpecialCharTok{$}\NormalTok{outlier\_maha}\SpecialCharTok{==} \StringTok{"No"}\NormalTok{),])}


\CommentTok{\#The following produces the formula of the new linear regression without the Mahalanobis outliers}

\FunctionTok{paste}\NormalTok{(}\StringTok{\textquotesingle{}y =\textquotesingle{}}\NormalTok{, }\FunctionTok{round}\NormalTok{(}\FunctionTok{coef}\NormalTok{(model\_without\_mahaoutliers)[[}\DecValTok{2}\NormalTok{]], }\AttributeTok{digits =} \DecValTok{3}\NormalTok{), }\StringTok{\textquotesingle{}* x\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}+\textquotesingle{}}\NormalTok{, }\FunctionTok{round}\NormalTok{(}\FunctionTok{coef}\NormalTok{(model\_without\_mahaoutliers)[[}\DecValTok{1}\NormalTok{]], }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "y = 3.741 * x + 30.099"
\end{verbatim}

y = 3.741 * x + 30.099

Now it would appear, that every manager spends on average 26 hours independently from the number of staff to appraise and an additional 3 hours and 47 minutes to appraise each indivual subordinate.

Let us start all over again with another method for excluding outliers, using the Cook distance.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Time.spent.on.appraisals }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"https://https://hranalytics.netlify.com/data/Time.spent.on.appraisals.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(Hours }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Count.of.Appraisee,        }\CommentTok{\# plot the variables y \textasciitilde{} x}
     \AttributeTok{xlab=}\StringTok{"Number of appraisee"}\NormalTok{,        }\CommentTok{\# x−axis label }
     \AttributeTok{ylab=}\StringTok{"Hours"}\NormalTok{,                      }\CommentTok{\# y−axis label}
     \AttributeTok{data=}\NormalTok{Time.spent.on.appraisals)     }\CommentTok{\# data set}

\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Hours }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Count.of.Appraisee, }\AttributeTok{data=}\NormalTok{Time.spent.on.appraisals)}
\FunctionTok{abline}\NormalTok{(model, }\AttributeTok{col =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-520-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cooksd }\OtherTok{\textless{}{-}} \FunctionTok{cooks.distance}\NormalTok{(model)}

\CommentTok{\# Plot the Cook\textquotesingle{}s Distance using the traditional 4/n criterion}
\NormalTok{sample\_size }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(Time.spent.on.appraisals)}
\FunctionTok{plot}\NormalTok{(cooksd, }\AttributeTok{pch=}\StringTok{"*"}\NormalTok{, }\AttributeTok{cex=}\DecValTok{2}\NormalTok{, }\AttributeTok{main=}\StringTok{"Influential Obs by Cooks distance"}\NormalTok{)         }\CommentTok{\# plot cook\textquotesingle{}s distance}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{h =} \DecValTok{4}\SpecialCharTok{/}\NormalTok{sample\_size, }\AttributeTok{col=}\StringTok{"red"}\NormalTok{)         }\CommentTok{\# add cutoff line}
\FunctionTok{text}\NormalTok{(}\AttributeTok{x=}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(cooksd)}\SpecialCharTok{+}\DecValTok{1}\NormalTok{, }\AttributeTok{y=}\NormalTok{cooksd, }\AttributeTok{labels=}\FunctionTok{ifelse}\NormalTok{(cooksd}\SpecialCharTok{\textgreater{}}\DecValTok{4}\SpecialCharTok{/}\NormalTok{sample\_size, }\FunctionTok{names}\NormalTok{(cooksd),}\StringTok{""}\NormalTok{), }\AttributeTok{col=}\StringTok{"red"}\NormalTok{)  }\CommentTok{\# add labels}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-520-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Removing Outliers}

\CommentTok{\# influential row numbers}
\NormalTok{influential }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{names}\NormalTok{(cooksd)[(cooksd }\SpecialCharTok{\textgreater{}}\NormalTok{ (}\DecValTok{4}\SpecialCharTok{/}\NormalTok{sample\_size))])}

\NormalTok{Time.spent.on.appraisals\_screen }\OtherTok{\textless{}{-}}\NormalTok{ Time.spent.on.appraisals[}\SpecialCharTok{{-}}\NormalTok{influential, ]}

\NormalTok{plot3 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ Time.spent.on.appraisals, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Count.of.Appraisee, }\AttributeTok{y =}\NormalTok{ Hours)) }\SpecialCharTok{+}
        \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{size =} \DecValTok{2}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.6}\NormalTok{, }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+} 
        \FunctionTok{xlim}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{40}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{ylim}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{600}\NormalTok{) }\SpecialCharTok{+}
        \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =}\NormalTok{ lm, }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+}             \CommentTok{\# geom\_smooth }
        \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Before"}\NormalTok{)}

\NormalTok{plot4 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ Time.spent.on.appraisals\_screen, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Count.of.Appraisee, }\AttributeTok{y =}\NormalTok{ Hours)) }\SpecialCharTok{+}
        \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{size =} \DecValTok{2}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.6}\NormalTok{, }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+} 
        \FunctionTok{xlim}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{40}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{ylim}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{600}\NormalTok{) }\SpecialCharTok{+}
        \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =}\NormalTok{ lm, }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+}
        \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"After"}\NormalTok{)}

\NormalTok{gridExtra}\SpecialCharTok{::}\FunctionTok{grid.arrange}\NormalTok{(plot3, plot4, }\AttributeTok{ncol=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-520-3} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_screen }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Hours }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Count.of.Appraisee, }\AttributeTok{data=}\NormalTok{Time.spent.on.appraisals\_screen)}

\CommentTok{\#The following produces the formula of the new linear regression without the outliers}

\FunctionTok{paste}\NormalTok{(}\StringTok{\textquotesingle{}y =\textquotesingle{}}\NormalTok{, }\FunctionTok{round}\NormalTok{(}\FunctionTok{coef}\NormalTok{(model\_screen)[[}\DecValTok{2}\NormalTok{]], }\AttributeTok{digits =} \DecValTok{3}\NormalTok{), }\StringTok{\textquotesingle{}* x\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}+\textquotesingle{}}\NormalTok{, }\FunctionTok{round}\NormalTok{(}\FunctionTok{coef}\NormalTok{(model\_screen)[[}\DecValTok{1}\NormalTok{]], }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "y = 3.741 * x + 30.099"
\end{verbatim}

y = 3.741 * x + 30.099

Now it would appear, that every manager spends on average 30 hours independently from the number of staff to appraise and an additional 3 hours and 45 minutes to appraise each indivual subordinate.

You can imagine that we could push the analysis even further, but we will stop here for the time being. Ready to resume when management is again interested in the matter.

\hypertarget{service-desk}{%
\chapter{HR Service Desk}\label{service-desk}}

How to use metrics:

\begin{itemize}
\tightlist
\item
  Inform your stakeholders
\item
  Report measurements so that stakeholders can understand activities and results
\item
  Promote the value of the organization
\item
  Determine the best way to communicate the information to the stakeholders
\item
  Perform better stakeholder analysis to facilitate stakeholder buy-in
\item
  Improve performance - people do what is measured
\end{itemize}

Four types of process metrics:

\begin{itemize}
\tightlist
\item
  Monitor progress by checking in process maturity
\item
  Monitor efficiency by checking use of resources
\item
  Monitor effectiveness by checking how many correct and complete first time
\item
  Monitor compliance in relation to process and regulatory requirements
\end{itemize}

Factors to consider when reporting:

\begin{itemize}
\tightlist
\item
  Who are the stakeholders?
\item
  How does what you are reporting impact the stakeholders?
\item
  Reports must be easy to read and understood, thus they need to be developed with
  the stakeholder in mind.
\item
  Reports need to show how the support center is contributing to the goals of each
  stakeholder and the business.
\item
  Reports must identify the appropriate channels to communicate with each of the
  stakeholders.
\end{itemize}

Source: \url{https://www.kaggle.com/lyndonsundmark/service-request-analysis/data}

Ensure all needed libraries are installed

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(lubridate)}
\end{Highlighting}
\end{Shaded}

First, let's get some data from our service desk by exporting a CSV. We can then read this CSV (or excel spreadsheet) into R for us to perform analysis.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{service\_requests }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"https:///hranalytics.netlify.com/data/ServiceRequestExtract2.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Note that we can solve some things as we load the data using \texttt{read\_csv()} like the column data types and handling different ways people can represent missing or unknown data.

We then need to get this data analysis-ready. First of all, we need to make sure dates are filled in and/or reasonable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{service\_requests }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{DateStarted =} \FunctionTok{coalesce}\NormalTok{(DateStarted, DateSubmitted),}
         \AttributeTok{DateCompleted=}\FunctionTok{coalesce}\NormalTok{(DateCompleted, DateStarted }\SpecialCharTok{+} \FunctionTok{hours}\NormalTok{(}\DecValTok{2}\NormalTok{))) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{DateCompleted =} 
           \FunctionTok{pmin}\NormalTok{(DateCompleted,}
\NormalTok{              DateStarted }\SpecialCharTok{+} \FunctionTok{hours}\NormalTok{(}\FunctionTok{floor}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(}\FunctionTok{n}\NormalTok{(), }\AttributeTok{mean =} \DecValTok{71}\NormalTok{, }\AttributeTok{sd=}\DecValTok{20}\NormalTok{))))) }\OtherTok{{-}\textgreater{}}
\NormalTok{  service\_requests}
\end{Highlighting}
\end{Shaded}

Then we can work out how long it took to complete different stages of a request.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{service\_requests }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{RequestID =} \FunctionTok{as.character}\NormalTok{(RequestID)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{WaitTime =} \FunctionTok{difftime}\NormalTok{(DateStarted, }
\NormalTok{                        DateSubmitted, }
                        \AttributeTok{units =} \StringTok{"hours"}\NormalTok{)}
\NormalTok{    ,}\AttributeTok{TaskTime =} \FunctionTok{difftime}\NormalTok{(DateCompleted, }
\NormalTok{                        DateStarted, }
                        \AttributeTok{units =} \StringTok{"hours"}\NormalTok{)}
\NormalTok{    ,}\AttributeTok{TotalTime =} \FunctionTok{difftime}\NormalTok{(DateCompleted, }
\NormalTok{                        DateSubmitted, }
                        \AttributeTok{units =} \StringTok{"hours"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate\_at}\NormalTok{(}\FunctionTok{vars}\NormalTok{(}\FunctionTok{ends\_with}\NormalTok{(}\StringTok{"Time"}\NormalTok{)), as.numeric)}\OtherTok{{-}\textgreater{}}
\NormalTok{  service\_requests}

\NormalTok{service\_requests}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1,152 x 8
   RequestID DateSubmitted       DateStarted         DateCompleted      
   <chr>     <dttm>              <dttm>              <dttm>             
 1 1         2014-11-26 13:43:00 2014-12-13 06:02:00 2014-12-13 06:02:00
 2 2         2014-11-29 14:41:00 2014-12-20 06:47:00 2014-12-22 03:47:00
 3 3         2014-11-29 14:43:00 2014-12-24 08:06:00 2014-12-27 18:06:00
 4 4         2014-11-29 14:45:00 2015-02-09 03:31:00 2015-02-11 09:31:00
 5 5         2014-11-29 14:49:00 2014-12-06 06:43:00 2014-12-06 06:43:00
 6 6         2014-11-29 14:50:00 2014-12-21 06:00:00 2014-12-21 09:00:00
 7 7         2014-11-29 14:50:00 2015-01-07 00:55:00 2015-01-09 23:55:00
 8 8         2014-12-01 08:38:00 2015-01-14 03:29:00 2015-01-15 08:29:00
 9 9         2014-12-03 16:26:00 2014-12-07 01:12:00 2014-12-08 17:12:00
10 10        2014-12-07 11:41:00 2014-12-16 00:25:00 2014-12-16 00:25:00
# ... with 1,142 more rows, and 4 more variables: Category <chr>,
#   WaitTime <dbl>, TaskTime <dbl>, TotalTime <dbl>
\end{verbatim}

We should now be able to get a view as to the distribution of the times taken to start, complete, and the overall turnaround time for requests.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(DataExplorer)}
\FunctionTok{plot\_density}\NormalTok{(service\_requests, }
             \AttributeTok{title =} \StringTok{"Distribution of task times"}\NormalTok{,}
             \AttributeTok{ggtheme =} \FunctionTok{theme\_minimal}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-526-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot\_bar}\NormalTok{(service\_requests,}
         \AttributeTok{title=}\StringTok{"Distributions"}\NormalTok{,}
         \AttributeTok{ggtheme =} \FunctionTok{theme\_minimal}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
4 columns ignored with more than 50 categories.
RequestID: 1152 categories
DateSubmitted: 1135 categories
DateStarted: 1132 categories
DateCompleted: 1146 categories
\end{verbatim}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-527-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{service\_requests }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(Category) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise\_at}\NormalTok{(}\FunctionTok{vars}\NormalTok{(}\FunctionTok{ends\_with}\NormalTok{(}\StringTok{"Time"}\NormalTok{)),}
              \AttributeTok{.funs =} \FunctionTok{c}\NormalTok{(}\StringTok{"mean"}\NormalTok{,}\StringTok{"min"}\NormalTok{,}\StringTok{"max"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(WaitTime\_mean)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 5 x 10
  Category WaitTime_mean TaskTime_mean TotalTime_mean WaitTime_min
  <chr>            <dbl>         <dbl>          <dbl>        <dbl>
1 Grievan~       11.8252      71.1333         82.9585            0
2 HR Repo~       23.8362       4.76644        28.6026            0
3 Trainin~       26.1803      58.8442         85.0245            0
4 Recruit~       28.89        67.2125         96.1025            0
5 Job Cla~       33.3248      35.1070         68.4318            0
# ... with 5 more variables: TaskTime_min <dbl>, TotalTime_min <dbl>,
#   WaitTime_max <dbl>, TaskTime_max <dbl>, TotalTime_max <dbl>
\end{verbatim}

Now that we've checked our data for issues and tidied it up, we can start understanding what's happening in-depth.

For instance, are the differences in category mean times significant or could it be due to the different volumes of requests? We can use the ANOVA test to check to see if each category does indeed seem to have differing response times. If the resulting P-value is small then we have more certainty that there is likely to be a difference by request category.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(RcmdrMisc)}
\FunctionTok{lm}\NormalTok{(WaitTime }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Category, }\AttributeTok{data=}\NormalTok{service\_requests) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{Anova}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Anova Table (Type II tests)

Response: WaitTime
            Sum Sq   Df F value Pr(>F)
Category     29404    4    0.42   0.79
Residuals 19942945 1147               
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lm}\NormalTok{(TaskTime }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Category, }\AttributeTok{data=}\NormalTok{service\_requests) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{Anova}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Anova Table (Type II tests)

Response: TaskTime
          Sum Sq   Df F value              Pr(>F)    
Category  665056    4    1742 <0.0000000000000002 ***
Residuals 109458 1147                                
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lm}\NormalTok{(TotalTime }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Category, }\AttributeTok{data=}\NormalTok{service\_requests) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{Anova}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Anova Table (Type II tests)

Response: TotalTime
            Sum Sq   Df F value      Pr(>F)    
Category    731473    4    10.5 0.000000025 ***
Residuals 20035691 1147                        
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

As well as statistical tests, we can apply quality control principles too. The \texttt{qcc} package allows us to use a number of relevant models and charts to understand what is happening.

Here we use the package to take a number of samples from the data and prepare a \texttt{qcc} base transformation containing information needed to make common charts. We use the \texttt{xbar.one} transformation to get the mean using one-at-time data of a continuous process variable.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(qcc)}

\NormalTok{service\_requests }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  \{}\FunctionTok{qcc.groups}\NormalTok{(.}\SpecialCharTok{$}\NormalTok{WaitTime, .}\SpecialCharTok{$}\NormalTok{RequestID)\} }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{qcc}\NormalTok{(}\AttributeTok{type=}\StringTok{"xbar.one"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-532-1} \end{center}

\begin{verbatim}
Call:
qcc(data = ., type = "xbar.one")

xbar.one chart for . 

Summary of group statistics:
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
      0       0       0      27       0    1910 

Group sample size:  1152
Number of groups:  1152
Center of group statistics:  27.1
Standard deviation:  42.9 

Control limits:
  LCL UCL
 -102 156
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{service\_requests }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  \{}\FunctionTok{qcc.groups}\NormalTok{(.}\SpecialCharTok{$}\NormalTok{TaskTime, .}\SpecialCharTok{$}\NormalTok{RequestID)\} }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{qcc}\NormalTok{(}\AttributeTok{type=}\StringTok{"xbar.one"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-533-1} \end{center}

\begin{verbatim}
Call:
qcc(data = ., type = "xbar.one")

xbar.one chart for . 

Summary of group statistics:
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
    0.0     6.0    30.0    31.9    52.0   113.0 

Group sample size:  1152
Number of groups:  1152
Center of group statistics:  31.9
Standard deviation:  25.2 

Control limits:
   LCL UCL
 -43.7 107
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{service\_requests }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  \{}\FunctionTok{qcc.groups}\NormalTok{(.}\SpecialCharTok{$}\NormalTok{TotalTime, .}\SpecialCharTok{$}\NormalTok{RequestID)\} }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{qcc}\NormalTok{(}\AttributeTok{type=}\StringTok{"xbar.one"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summary}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-534-1} \end{center}

\begin{verbatim}
Call:
qcc(data = ., type = "xbar.one")

xbar.one chart for . 

Summary of group statistics:
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
      0      11      37      59      60    1926 

Group sample size:  1152
Number of groups:  1152
Center of group statistics:  59
Standard deviation:  63.4 

Control limits:
  LCL UCL
 -131 249
\end{verbatim}

These show overall patterns. What if we wanted one per category?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Need to get categories being added as titles}
\NormalTok{service\_requests }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{  \{}\FunctionTok{split}\NormalTok{(., .}\SpecialCharTok{$}\NormalTok{Category)\} }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{map}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\FunctionTok{qcc.groups}\NormalTok{(.}\SpecialCharTok{$}\NormalTok{TotalTime, .}\SpecialCharTok{$}\NormalTok{RequestID)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{map}\NormalTok{(qcc, }\AttributeTok{type =}\StringTok{"xbar.one"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-535-1} \end{center}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-535-2} \end{center}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-535-3} \end{center}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-535-4} \end{center}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-535-5} \end{center}

\begin{verbatim}
$`Grievance Resolution`
List of 11
 $ call      : language .f(data = .x[[i]], type = "xbar.one")
 $ type      : chr "xbar.one"
 $ data.name : chr ".x[[i]]"
 $ data      : num [1, 1:45] 93 66 84 71 64 86 76 70 113 74 ...
  ..- attr(*, "dimnames")=List of 2
 $ statistics: Named num [1:45] 93 66 84 71 64 86 76 70 113 74 ...
  ..- attr(*, "names")= chr [1:45] "1" NA NA NA ...
 $ sizes     : int 45
 $ center    : num 83
 $ std.dev   : num 31.3
 $ nsigmas   : num 3
 $ limits    : num [1, 1:2] -11 177
  ..- attr(*, "dimnames")=List of 2
 $ violations:List of 2
 - attr(*, "class")= chr "qcc"

$`HR Report`
List of 11
 $ call      : language .f(data = .x[[i]], type = "xbar.one")
 $ type      : chr "xbar.one"
 $ data.name : chr ".x[[i]]"
 $ data      : num [1, 1:441] 400 205 0 14 0 ...
  ..- attr(*, "dimnames")=List of 2
 $ statistics: Named num [1:441] 400 205 0 14 0 ...
  ..- attr(*, "names")= chr [1:441] "1" NA NA NA ...
 $ sizes     : int 441
 $ center    : num 28.6
 $ std.dev   : num 40.5
 $ nsigmas   : num 3
 $ limits    : num [1, 1:2] -92.8 150
  ..- attr(*, "dimnames")=List of 2
 $ violations:List of 2
 - attr(*, "class")= chr "qcc"

$`Job Classification`
List of 11
 $ call      : language .f(data = .x[[i]], type = "xbar.one")
 $ type      : chr "xbar.one"
 $ data.name : chr ".x[[i]]"
 $ data      : num [1, 1:355] 22 30 43 40 166 ...
  ..- attr(*, "dimnames")=List of 2
 $ statistics: Named num [1:355] 22 30 43 40 166 ...
  ..- attr(*, "names")= chr [1:355] "1" NA NA NA ...
 $ sizes     : int 355
 $ center    : num 68.4
 $ std.dev   : num 59.8
 $ nsigmas   : num 3
 $ limits    : num [1, 1:2] -111 248
  ..- attr(*, "dimnames")=List of 2
 $ violations:List of 2
 - attr(*, "class")= chr "qcc"

$Recruitment
List of 11
 $ call      : language .f(data = .x[[i]], type = "xbar.one")
 $ type      : chr "xbar.one"
 $ data.name : chr ".x[[i]]"
 $ data      : num [1, 1:80] 31 73 69 526 64 ...
  ..- attr(*, "dimnames")=List of 2
 $ statistics: Named num [1:80] 31 73 69 526 64 ...
  ..- attr(*, "names")= chr [1:80] "1" NA NA NA ...
 $ sizes     : int 80
 $ center    : num 96.1
 $ std.dev   : num 64.2
 $ nsigmas   : num 3
 $ limits    : num [1, 1:2] -96.5 288.7
  ..- attr(*, "dimnames")=List of 2
 $ violations:List of 2
 - attr(*, "class")= chr "qcc"

$`Training Delivery`
List of 11
 $ call      : language .f(data = .x[[i]], type = "xbar.one")
 $ type      : chr "xbar.one"
 $ data.name : chr ".x[[i]]"
 $ data      : num [1, 1:231] 59 59 82.8 65 57 ...
  ..- attr(*, "dimnames")=List of 2
 $ statistics: Named num [1:231] 59 59 82.8 65 57 ...
  ..- attr(*, "names")= chr [1:231] "1" NA NA NA ...
 $ sizes     : int 231
 $ center    : num 85
 $ std.dev   : num 55.3
 $ nsigmas   : num 3
 $ limits    : num [1, 1:2] -80.9 251
  ..- attr(*, "dimnames")=List of 2
 $ violations:List of 2
 - attr(*, "class")= chr "qcc"
\end{verbatim}

5 Valuable Service Desk Metrics

Source: \url{https://www.ibm.com/communities/analytics/watson-analytics-blog/it-help-desk/}

Number of tickets processed and ticket/service agent ratio --Two simple metrics that add up the number of tickets submitted during specific times (i.e.~shift, hour, day, week, etc.) and create a ratio of tickets/available service agents during those times. This is a key KPI that speaks to staffing levels and informs other Service Desk metrics.

Wait times -- How long after a customer submits a service request do they have to wait before Service Desk agents start working on the ticket? Your wait time metrics also speak to Service Desk staffing levels. Once you identify whether your Service Desk has excessive wait times, you can drill down to see what might be causing wait times to run long (i.e.~low staff levels at certain times of the day or week; not enough service agents trained for a specific service; processing issues; etc.) and create a remedy that applies to your entire Service Desk organization or to an individual IT service.

Transfer analysis (tickets solved on first-touch versus multi-touch tickets) -- Number of tickets that are solved by the first agent to handle the ticket (first-touch) versus the number of tickets that are assigned to one or more groups through the ticket's lifespan. Great for determining which tickets need special attention, particularly those tickets where automation might reduce the amount of ticket passing between technical groups.

Ticket growth over time and backlog -- Trending data showing the increase (or decrease) in the number of Service Desk tickets over time. It can help spot unexpected changes in user requests that may indicate a need for more Service Desk staff or more automation. Or, it may identify that a specific change resulted in increased Service Desk resources. You also want to check the trends for your backlog of tickets in progress and the number of unresolved tickets. A growth in backlogged tickets can indicate a change in service desk demand or problems with service deployment.

Top IT services with the most incidents -- Spotlights which services are failing, causing the most Service Desk support. Helpful for spotting problem IT services that need modification.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{it\_helpdesk }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"https://hranalytics.netlify.com/data/WA\_Fn{-}UseC\_{-}IT{-}Help{-}Desk.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{it\_helpdesk }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{ITOwner) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{"IT Owner"}\NormalTok{, }
       \AttributeTok{y=}\StringTok{"Number of tickets"}\NormalTok{, }
       \AttributeTok{title=}\StringTok{"Tickets by IT Owner"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-537-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{it\_helpdesk }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{daysOpen) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{"Number of days ticket was open for"}\NormalTok{, }
       \AttributeTok{y=}\StringTok{"Number of tickets"}\NormalTok{, }
       \AttributeTok{title=}\StringTok{"Time to resolve/close tickets"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-538-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{it\_helpdesk }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{count}\NormalTok{(Requestor) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{n) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{"Number of tickets raised per person"}\NormalTok{, }
       \AttributeTok{y=}\StringTok{"Density"}\NormalTok{, }
       \AttributeTok{title=}\StringTok{"Distribution of tickets per person"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-539-1} \end{center}

\hypertarget{personality}{%
\chapter{Personality insights}\label{personality}}

The IBM Watson Personality Insights service uses linguistic analysis to extract cognitive and social characteristics from input text such as email, text messages, tweets, forum posts, and more. By deriving cognitive and social preferences, the service helps users to understand, connect to, and communicate with other people on a more personalized level.

\begin{itemize}
\tightlist
\item
  \href{https://console.bluemix.net/docs/services/personality-insights/getting-started.html\#getting-started-tutorial}{Getting started guide}
\item
  \href{https://github.com/personality-insights/sunburst-chart}{IBM's open source plotting library}
\item
  \href{https://itsalocke.com/ibmsunburst/}{The ibmsunburst R documentation}
\item
  \href{https://www.ibm.com/watson/developercloud/personality-insights/api/v3/curl.html?curl\#introduction}{Working with curl and personality insights}
\end{itemize}

We can use communications from a person to analyse their personality.

We first need to be able to talk to Personality Insights. We get most of this information off the IBM site once we've made an account.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{key }\OtherTok{=} \StringTok{"aOWMNztQ\_VVlz9fINhc3v67rtnJqcN6JuubQorAvhq"}
\NormalTok{url }\OtherTok{=} \StringTok{"https://gateway.watsonplatform.net/personality{-}insights/api/v3/profile?version=2017{-}10{-}13"}
\NormalTok{uname}\OtherTok{=}\StringTok{"a4a4ea65{-}e8e7{-}492c{-}a95e{-}128f10fc5f"}
\NormalTok{pword}\OtherTok{=}\StringTok{"LuFm4BELs"}
\end{Highlighting}
\end{Shaded}

We can then talk to the API using the \texttt{httr} package. We can send a body of text to be analysed. The guidance for the API says you should send it more than 600 words to get a robust results.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(httr)}
\FunctionTok{library}\NormalTok{(janeaustenr)}
\NormalTok{cr}\OtherTok{=}\FunctionTok{POST}\NormalTok{(url,}
    \FunctionTok{authenticate}\NormalTok{(uname, pword),}
    \FunctionTok{content\_type}\NormalTok{(}\StringTok{"text/plain;charset=utf{-}8"}\NormalTok{),}
    \FunctionTok{accept\_json}\NormalTok{(),}
    \AttributeTok{body=}\FunctionTok{paste}\NormalTok{(janeaustenr}\SpecialCharTok{::}\NormalTok{emma, }\AttributeTok{collapse =} \StringTok{" "}\NormalTok{)}
\NormalTok{)}
\FunctionTok{status\_code}\NormalTok{(cr)}
\end{Highlighting}
\end{Shaded}

We can then visualise the results using the \texttt{ibmsunburst} package.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ibmsunburst)}
\FunctionTok{ibmsunburst}\NormalTok{(}\FunctionTok{content}\NormalTok{(cr), }\AttributeTok{version =} \StringTok{"v3"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{commuting-time}{%
\chapter{Commuting time}\label{commuting-time}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{do\_eval}\OtherTok{=}\FunctionTok{file.exists}\NormalTok{(}\StringTok{"commute\_times.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Commuting time is often mentioned as a valid reason for leaving one employer for another.

You are asked to calculate the commuting time for the employee working at your company. You decide that hyphotetically they must all arrive in the office at 8.30 am. In the Google you identified that there is google api for doing this, called \href{https://developers.google.com/maps/documentation/distance-matrix/start?csw=1}{Distance Matrix API}, which works for a matrix of origins and destinations.

The information returned is based on the recommended route between start and end points and consists of rows containing duration and distance values for each pair." To use the Distance Matrix API, you must first activate the API in the Google Cloud Platform Console and obtain the proper authentication credentials. You need to provide your own API key in each request.

The documentation on how to do this is located here: \url{https://developers.google.com/maps/documentation/distance-matrix/intro}

A file with 200 fake names and addresses has been put together for you. The task is to add the commuting time next to each staff member. Normally google will charge us one dollar for checking 200 commuting times.

Please note that the departure\_time specifies the desired time of departure must be in POSIXct. format and must be in the future (i.e.greater than sys.time()). If no value is specified it defaults to Sys.time().

Please note you can only specify one of arrival\_time or departure\_time, not both. If both are supplied, departure\_time will be used.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(googleway)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(lubridate)}

\CommentTok{\# Set up here your api key key=}
\NormalTok{key }\OtherTok{=} \StringTok{\textquotesingle{}AIzaSyCOly69PDrlPlM42I378p2lmvNs8I2w\textquotesingle{}}

\NormalTok{d }\OtherTok{\textless{}{-}} \FunctionTok{dmy\_hms}\NormalTok{(}\StringTok{"10/09/2018 08:30"}\NormalTok{)}

\NormalTok{arrival }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(d)}


\NormalTok{from }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"SE3+8UQ,UK"}\NormalTok{)}
\NormalTok{to }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"E14+5EU,UK"}\NormalTok{)}

\NormalTok{test }\OtherTok{\textless{}{-}} \FunctionTok{google\_distance}\NormalTok{(}\AttributeTok{origins =}\NormalTok{ from,}
                        \AttributeTok{destinations =}\NormalTok{ to,}
                        \AttributeTok{mode =} \StringTok{"walking"}\NormalTok{,}
                        \AttributeTok{arrival\_time =} \FunctionTok{as.POSIXct}\NormalTok{(arrival, }\AttributeTok{origin =} \StringTok{"1970{-}01{-}01"}\NormalTok{, }\AttributeTok{tz =} \StringTok{"UTC"}\NormalTok{),}
                        \AttributeTok{key=}\NormalTok{key)}

\NormalTok{test}\SpecialCharTok{$}\NormalTok{rows}\SpecialCharTok{$}\NormalTok{elements[[}\DecValTok{1}\NormalTok{]]}\SpecialCharTok{$}\NormalTok{distance}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
NULL
\end{verbatim}

To make all this simpler, we put together an R function which can be used to analyse many results. Note this calls all modes of transport for every employee making it more costly than just running one mode per employee.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#\textquotesingle{} Get distance data between two points based on all the travel mode options. Works for many origin points.}
\CommentTok{\#\textquotesingle{}}
\CommentTok{\#\textquotesingle{} @param x A vector of origins in address or postcode format}
\CommentTok{\#\textquotesingle{} @param dest A single destinationin address or postocde format}
\CommentTok{\#\textquotesingle{} @param arrival\_time A POSIXct datetime that folks need to arrive by}
\CommentTok{\#\textquotesingle{} @param key A google distance API key}
\CommentTok{\#\textquotesingle{} @param ... Additional options to pass to \textasciigrave{}google\_distance()\textasciigrave{}}
\CommentTok{\#\textquotesingle{}}
\CommentTok{\#\textquotesingle{} @return Data.frame containing (typically) 4 rows per input element}

\NormalTok{google\_distance\_all }\OtherTok{=}  \ControlFlowTok{function}\NormalTok{(x, dest, arrival\_time, key, ...)\{}
  
  \CommentTok{\# simple hygeine stuff}
\NormalTok{  gd }\OtherTok{=}\NormalTok{ purrr}\SpecialCharTok{::}\FunctionTok{possibly}\NormalTok{(}
\NormalTok{    memoise}\SpecialCharTok{::}\FunctionTok{memoise}\NormalTok{(}
\NormalTok{      google\_distance)}
\NormalTok{    , }\StringTok{"Fail"}
\NormalTok{  )}
  
  \CommentTok{\# Prep dataset}
\NormalTok{   interested\_in }\OtherTok{=} \FunctionTok{expand.grid}\NormalTok{(}\AttributeTok{from=}\NormalTok{x, }
     \AttributeTok{mode=}\FunctionTok{c}\NormalTok{(}\StringTok{"driving"}\NormalTok{, }\StringTok{"walking"}\NormalTok{, }\StringTok{"bicycling"}\NormalTok{, }\StringTok{"transit"}\NormalTok{), }
      \AttributeTok{stringsAsFactors =} \ConstantTok{FALSE}\NormalTok{)}
   \CommentTok{\# Perform google\_distance calls for all combos}
\NormalTok{  purrr}\SpecialCharTok{::}\FunctionTok{map2}\NormalTok{(interested\_in}\SpecialCharTok{$}\NormalTok{from,interested\_in}\SpecialCharTok{$}\NormalTok{mode, }
     \SpecialCharTok{\textasciitilde{}}\FunctionTok{gd}\NormalTok{(.x, dest, }\AttributeTok{mode=}\NormalTok{.y,}
                        \AttributeTok{arrival\_time =}\NormalTok{ arrival\_time,}
                        \AttributeTok{key=}\NormalTok{key)}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%} 
    \CommentTok{\# Extract relevant section}
\NormalTok{    purrr}\SpecialCharTok{::}\FunctionTok{map}\NormalTok{(}\StringTok{"rows"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{    purrr}\SpecialCharTok{::}\FunctionTok{map}\NormalTok{(}\StringTok{"elements"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{    purrr}\SpecialCharTok{::}\FunctionTok{flatten}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
    \CommentTok{\# Simplify the data.frames}
\NormalTok{    purrr}\SpecialCharTok{::}\FunctionTok{map}\NormalTok{(unclass) }\SpecialCharTok{\%\textgreater{}\%} 
\NormalTok{    purrr}\SpecialCharTok{::}\FunctionTok{map\_df}\NormalTok{(purrr}\SpecialCharTok{::}\NormalTok{flatten) }\SpecialCharTok{\%\textgreater{}\%} 
    \CommentTok{\# Add original lookup values}
    \FunctionTok{cbind}\NormalTok{(interested\_in)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

This needs some packages installed to work

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(purrr)) }\FunctionTok{install.packages}\NormalTok{(}\StringTok{"purrr"}\NormalTok{)}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(memoise)) }\FunctionTok{install.packages}\NormalTok{(}\StringTok{"memoise"}\NormalTok{)}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(googleway)) }\FunctionTok{install.packages}\NormalTok{(}\StringTok{"googleway"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We need some information about our google API account, where our office is, and when we're testing commute times for.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{office }\OtherTok{=} \StringTok{"E14 5EU"}
\NormalTok{monday\_9am }\OtherTok{=} \FunctionTok{as.POSIXct}\NormalTok{(}\StringTok{"2018{-}12{-}03 09:00"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Then we can use this function to get data for our 200 employees

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{the\_200 }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"https://hranalytics.netlify.com/data/200\_staff\_members.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results }\OtherTok{=} \FunctionTok{google\_distance\_all}\NormalTok{(}
\NormalTok{  the\_200}\SpecialCharTok{$}\NormalTok{Postcode,}
\NormalTok{  office,}
  \AttributeTok{arrival\_time =}\NormalTok{ monday\_9am,}
  \AttributeTok{key =}\NormalTok{ key}
\NormalTok{)}

\FunctionTok{write\_csv}\NormalTok{(results, }\FunctionTok{file.path}\NormalTok{(}\StringTok{"data"}\NormalTok{), }\StringTok{"commute\_times.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 800 x 5
   text    value status from     mode   
   <chr>   <dbl> <chr>  <chr>    <chr>  
 1 16 mins   963 OK     E15 1NS  driving
 2 57 mins  3396 OK     W12 8ET  driving
 3 41 mins  2469 OK     SW1V 1AP driving
 4 19 mins  1123 OK     SE1 2RE  driving
 5 37 mins  2206 OK     WC1A 2QS driving
 6 43 mins  2604 OK     N6 5JW   driving
 7 31 mins  1871 OK     EC1N 8PN driving
 8 36 mins  2158 OK     WC2B 6TE driving
 9 31 mins  1841 OK     N1 9AZ   driving
10 24 mins  1423 OK     WC2R 3LD driving
# ... with 790 more rows
\end{verbatim}

Now that we have this data we can answer questions about our employee's commute times.

Who has the longest commutes by car?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(mode }\SpecialCharTok{==} \StringTok{"driving"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{hours=}\NormalTok{value}\SpecialCharTok{/}\DecValTok{60}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{top\_n}\NormalTok{(}\DecValTok{10}\NormalTok{, hours) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(hours))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 10 x 6
   text             value status from     mode       hours
   <chr>            <dbl> <chr>  <chr>    <chr>      <dbl>
 1 10 hours 46 mins 38745 OK     BT49 0BX driving 10.7625 
 2 10 hours 33 mins 37986 OK     BT51 3SQ driving 10.5517 
 3 6 hours 45 mins  24274 OK     G4 0JY   driving  6.74278
 4 5 hours 19 mins  19145 OK     CA1 1BG  driving  5.31806
 5 4 hours 35 mins  16519 OK     PL1 4GP  driving  4.58861
 6 4 hours 17 mins  15411 OK     BB5 3EZ  driving  4.28083
 7 3 hours 57 mins  14239 OK     OL10 2TL driving  3.95528
 8 3 hours 30 mins  12585 OK     LS23 7BA driving  3.49583
 9 3 hours 26 mins  12341 OK     LS24 9QW driving  3.42806
10 3 hours 18 mins  11882 OK     S10 3TR  driving  3.30056
\end{verbatim}

We can look at the distributions overall too.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{hours=}\NormalTok{value}\SpecialCharTok{/}\DecValTok{60}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{hours) }\SpecialCharTok{+}
  \FunctionTok{geom\_density}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_log10}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{mode) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-554-1} \end{center}

\hypertarget{orgnisational-network}{%
\chapter{Organisational network analysis}\label{orgnisational-network}}

Visualizing and analysing formal and informal relationships in your organization can help you shape business strategy that maximizes organic exchange of information, thereby helping your business become more sustainable and effective.

For example in Organisational network analysis (ONA), we can ask employees three simple questions: 1) who is important to your ability to accomplish work priorities? 2) who is important for you to have greater access to, and 3) who provides you with career-related advice?

Nowadays, HR professionals use Organizational Network Analysis (ONA) use to their advantage. A whole new skill set is develping. HR professionals need to develop a structured way to visualise how communications, information, and decisions flow through an organization.

Organizational networks consist of nodes and edges.

In the following example, we will use the character interaction network for George R. R. Martin's ``A Song of Ice and Fire'' saga.

These networks were created by connecting two characters whenever their names (or nicknames) appeared within 15 words of one another in one of the books in ``A Song of Ice and Fire.'' The edge weight corresponds to the number of interactions. A Song of Ice and Fire is an ongoing a series of epic fantasy novels.

You can use this data to explore the dynamics of the Seven Kingdoms using network science techniques. For example, community detection finds coherent plotlines. Centrality measures uncover the multiple ways in which characters play important roles in the saga.

This is the data for the work presented here: \url{https://networkofthrones.wordpress.com} by Andrew Beveridge.

Source: \url{https://github.com/mathbeveridge/asoiaf}
Source code: \url{https://shirinsplayground.netlify.com/2018/03/got_network/}
Source code: \url{https://shiring.github.io/networks/2017/05/15/got_final}

With the following we ensure that all needed libraries are installed.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse) }\CommentTok{\# tidy data analysis}
\FunctionTok{library}\NormalTok{(tidygraph) }\CommentTok{\# tidy graph analysis}
\FunctionTok{library}\NormalTok{(ggraph)    }\CommentTok{\# for plotting}
\FunctionTok{library}\NormalTok{(igraph)    }\CommentTok{\# for plotting}
\FunctionTok{library}\NormalTok{(visNetwork) }\CommentTok{\# for visualising graph}
\end{Highlighting}
\end{Shaded}

First, let's get the data from the characters from the ``Song of Ice and Fire'' novels.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cooc\_all\_edges }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"https://hranalytics.netlify.com/data/asoiaf{-}all{-}edges.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let us identify first the main characters contained either as Source or as a target and later the 50 most important charcters:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{main\_ch }\OtherTok{\textless{}{-}}\NormalTok{ cooc\_all\_edges }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{Type) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{gather}\NormalTok{(x, name, Source}\SpecialCharTok{:}\NormalTok{Target) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(name) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{sum\_weight =} \FunctionTok{sum}\NormalTok{(weight)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ungroup}\NormalTok{()}

\NormalTok{main\_ch\_l }\OtherTok{\textless{}{-}}\NormalTok{ main\_ch }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(sum\_weight)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{top\_n}\NormalTok{(}\DecValTok{50}\NormalTok{, sum\_weight)}
\NormalTok{main\_ch\_l}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 50 x 2
   name               sum_weight
   <chr>                   <dbl>
 1 Tyrion-Lannister         2873
 2 Jon-Snow                 2757
 3 Cersei-Lannister         2232
 4 Joffrey-Baratheon        1762
 5 Eddard-Stark             1649
 6 Daenerys-Targaryen       1608
 7 Jaime-Lannister          1569
 8 Sansa-Stark              1547
 9 Bran-Stark               1508
10 Robert-Baratheon         1488
# ... with 40 more rows
\end{verbatim}

In the following we select the relationships of the top 50 characters. The edges are undirected, therefore there are no redundant Source-Target combinations; because of this, Source and Target data have been gathered before summing up the weights.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cooc\_all\_f }\OtherTok{\textless{}{-}}\NormalTok{ cooc\_all\_edges }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(Source }\SpecialCharTok{\%in\%}\NormalTok{ main\_ch\_l}\SpecialCharTok{$}\NormalTok{name }\SpecialCharTok{\&}\NormalTok{ Target }\SpecialCharTok{\%in\%}\NormalTok{ main\_ch\_l}\SpecialCharTok{$}\NormalTok{name)}
\end{Highlighting}
\end{Shaded}

The first step is to convert our edge table into a tbl\_graph object structure. Here, we use the as\_tbl\_graph() function from tidygraph; it can take many different types of input data, like data.frame, matrix, dendrogram, igraph, etc.

A central aspect of tidygraph is that you can directly manipulate node and edge data from this tbl\_graph object by activating nodes or edges. When we first create a tbl\_graph object, the nodes will be activated. We can then directly calculate node or edge metrics, like centrality, using tidyverse functions.

We can change that with the activate() function. We can now, for example, remove multiple edges.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{as\_tbl\_graph}\NormalTok{(cooc\_all\_f, }\AttributeTok{directed =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tbl_graph: 50 nodes and 402 edges
#
# An undirected simple graph with 1 component
#
# Node Data: 50 x 1 (active)
  name                           
  <chr>                          
1 Aemon-Targaryen-(Maester-Aemon)
2 Arya-Stark                     
3 Barristan-Selmy                
4 Bran-Stark                     
5 Brienne-of-Tarth               
6 Bronn                          
# ... with 44 more rows
#
# Edge Data: 402 x 5
   from    to Type          id weight
  <int> <int> <chr>      <dbl>  <dbl>
1     1    15 Undirected    54      5
2     1    20 Undirected    57     25
3     1    23 Undirected    58    110
# ... with 399 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{as\_tbl\_graph}\NormalTok{(cooc\_all\_f, }\AttributeTok{directed =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{activate}\NormalTok{(edges) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{edge\_is\_multiple}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tbl_graph: 50 nodes and 402 edges
#
# An undirected simple graph with 1 component
#
# Edge Data: 402 x 5 (active)
   from    to Type          id weight
  <int> <int> <chr>      <dbl>  <dbl>
1     1    15 Undirected    54      5
2     1    20 Undirected    57     25
3     1    23 Undirected    58    110
4     1    28 Undirected    60      5
5     1    39 Undirected    63      5
6     1    41 Undirected    64     99
# ... with 396 more rows
#
# Node Data: 50 x 1
  name                           
  <chr>                          
1 Aemon-Targaryen-(Maester-Aemon)
2 Arya-Stark                     
3 Barristan-Selmy                
# ... with 47 more rows
\end{verbatim}

Node ranking

There are many options for node ranking (go to ?node\_rank for a full list); let's try out Minimize hamiltonian path length using a travelling salesperson solver.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{as\_tbl\_graph}\NormalTok{(cooc\_all\_f, }\AttributeTok{directed =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{activate}\NormalTok{(nodes) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{n\_rank\_trv =} \FunctionTok{node\_rank\_traveller}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(n\_rank\_trv)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tbl_graph: 50 nodes and 402 edges
#
# An undirected simple graph with 1 component
#
# Node Data: 50 x 2 (active)
  name             n_rank_trv
  <chr>                 <int>
1 Hizdahr-zo-Loraq          1
2 Quentyn-Martell           2
3 Barristan-Selmy           3
4 Meryn-Trant               4
5 Jaime-Lannister           5
6 Arya-Stark                6
# ... with 44 more rows
#
# Edge Data: 402 x 5
   from    to Type          id weight
  <int> <int> <chr>      <dbl>  <dbl>
1    28    29 Undirected    54      5
2    28    30 Undirected    57     25
3    25    28 Undirected    58    110
# ... with 399 more rows
\end{verbatim}

Centrality

Centrality describes the number of edges that are in- or outgoing to/from nodes. High centrality networks have few nodes with many connections, low centrality networks have many nodes with similar numbers of edges. The centrality of a node measures the importance of it in the network.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Centrality}

\FunctionTok{as\_tbl\_graph}\NormalTok{(cooc\_all\_f, }\AttributeTok{directed =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{activate}\NormalTok{(nodes) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{neighbors =} \FunctionTok{centrality\_degree}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{neighbors)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tbl_graph: 50 nodes and 402 edges
#
# An undirected simple graph with 1 component
#
# Node Data: 50 x 2 (active)
  name              neighbors
  <chr>                 <dbl>
1 Tyrion-Lannister         36
2 Robert-Baratheon         33
3 Joffrey-Baratheon        32
4 Cersei-Lannister         30
5 Eddard-Stark             30
6 Jaime-Lannister          29
# ... with 44 more rows
#
# Edge Data: 402 x 5
   from    to Type          id weight
  <int> <int> <chr>      <dbl>  <dbl>
1    41    47 Undirected    54      5
2    38    41 Undirected    57     25
3    14    41 Undirected    58    110
# ... with 399 more rows
\end{verbatim}

Grouping and clustering

Another common operation is to group nodes based on the graph topology, sometimes referred to as community detection based on its commonality in social network analysis. All clustering algorithms from igraph is available in tidygraph using the group\_* prefix. All of these functions return an integer vector with nodes (or edges) sharing the same integer being grouped together. \url{https://www.data-imaginist.com/2017/introducing-tidygraph/}

We can use ?group\_graph for an overview about all possible ways to cluster and group nodes. Here I am using group\_infomap(): Group nodes by minimizing description length using.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Grouping and clustering}

\FunctionTok{as\_tbl\_graph}\NormalTok{(cooc\_all\_f, }\AttributeTok{directed =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{activate}\NormalTok{(nodes) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{group =} \FunctionTok{group\_infomap}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{group)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tbl_graph: 50 nodes and 402 edges
#
# An undirected simple graph with 1 component
#
# Node Data: 50 x 2 (active)
  name                            group
  <chr>                           <int>
1 Aemon-Targaryen-(Maester-Aemon)     1
2 Arya-Stark                          1
3 Barristan-Selmy                     1
4 Bran-Stark                          1
5 Brienne-of-Tarth                    1
6 Bronn                               1
# ... with 44 more rows
#
# Edge Data: 402 x 5
   from    to Type          id weight
  <int> <int> <chr>      <dbl>  <dbl>
1     1    15 Undirected    54      5
2     1    20 Undirected    57     25
3     1    23 Undirected    58    110
# ... with 399 more rows
\end{verbatim}

Querying node types

We can also query different node types (?node\_types gives us a list of options):

These functions all lets the user query whether each node is of a certain type. All of the functions returns a logical vector indicating whether the node is of the type in question. Do note that the types are not mutually exclusive and that nodes can thus be of multiple types.

Here, I am trying out node\_is\_center() (does the node have the minimal eccentricity in the graph) and node\_is\_keyplayer() to identify the top 10 key-players in the network.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Querying node types}

\FunctionTok{as\_tbl\_graph}\NormalTok{(cooc\_all\_f, }\AttributeTok{directed =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{activate}\NormalTok{(nodes) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{center =} \FunctionTok{node\_is\_center}\NormalTok{(),}
         \AttributeTok{keyplayer =} \FunctionTok{node\_is\_keyplayer}\NormalTok{(}\AttributeTok{k =} \DecValTok{10}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tbl_graph: 50 nodes and 402 edges
#
# An undirected simple graph with 1 component
#
# Node Data: 50 x 3 (active)
  name                            center keyplayer
  <chr>                           <lgl>  <lgl>    
1 Aemon-Targaryen-(Maester-Aemon) FALSE  FALSE    
2 Arya-Stark                      FALSE  TRUE     
3 Barristan-Selmy                 FALSE  FALSE    
4 Bran-Stark                      FALSE  FALSE    
5 Brienne-of-Tarth                FALSE  FALSE    
6 Bronn                           FALSE  FALSE    
# ... with 44 more rows
#
# Edge Data: 402 x 5
   from    to Type          id weight
  <int> <int> <chr>      <dbl>  <dbl>
1     1    15 Undirected    54      5
2     1    20 Undirected    57     25
3     1    23 Undirected    58    110
# ... with 399 more rows
\end{verbatim}

Node pairs

Some statistics are a measure between two nodes, such as distance or similarity between nodes. In a tidy context one of the ends must always be the node defined by the row, while the other can be any other node. All of the node pair functions are prefixed with node\_* and ends with \_from/\_to if the measure is not symmetric and \_with if it is; e.g.~there's both a node\_max\_flow\_to() and node\_max\_flow\_from() function while only a single node\_cocitation\_with() function. The other part of the node pair can be specified as an integer vector that will get recycled if needed, or a logical vector which will get recycled and converted to indexes with which(). This means that output from node type functions can be used directly in the calls. \url{https://www.data-imaginist.com/2017/introducing-tidygraph/}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Node pairs}

\FunctionTok{as\_tbl\_graph}\NormalTok{(cooc\_all\_f, }\AttributeTok{directed =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{activate}\NormalTok{(nodes) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{dist\_to\_center =} \FunctionTok{node\_distance\_to}\NormalTok{(}\FunctionTok{node\_is\_center}\NormalTok{()))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tbl_graph: 50 nodes and 402 edges
#
# An undirected simple graph with 1 component
#
# Node Data: 50 x 2 (active)
  name                            dist_to_center
  <chr>                                    <dbl>
1 Aemon-Targaryen-(Maester-Aemon)              2
2 Arya-Stark                                   1
3 Barristan-Selmy                              1
4 Bran-Stark                                   1
5 Brienne-of-Tarth                             1
6 Bronn                                        1
# ... with 44 more rows
#
# Edge Data: 402 x 5
   from    to Type          id weight
  <int> <int> <chr>      <dbl>  <dbl>
1     1    15 Undirected    54      5
2     1    20 Undirected    57     25
3     1    23 Undirected    58    110
# ... with 399 more rows
\end{verbatim}

Edge betweenness

Similarly to node metrics, we can calculate all kinds of edge metrics. Betweenness, for example, describes the shortest paths between nodes.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Edge betweenness}

\FunctionTok{as\_tbl\_graph}\NormalTok{(cooc\_all\_f, }\AttributeTok{directed =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{activate}\NormalTok{(edges) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{centrality\_e =} \FunctionTok{centrality\_edge\_betweenness}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tbl_graph: 50 nodes and 402 edges
#
# An undirected simple graph with 1 component
#
# Edge Data: 402 x 6 (active)
   from    to Type          id weight centrality_e
  <int> <int> <chr>      <dbl>  <dbl>        <dbl>
1     1    15 Undirected    54      5      4.53552
2     1    20 Undirected    57     25      2.28205
3     1    23 Undirected    58    110      8.13462
4     1    28 Undirected    60      5      2.76619
5     1    39 Undirected    63      5     23.0559 
6     1    41 Undirected    64     99      4.63597
# ... with 396 more rows
#
# Node Data: 50 x 1
  name                           
  <chr>                          
1 Aemon-Targaryen-(Maester-Aemon)
2 Arya-Stark                     
3 Barristan-Selmy                
# ... with 47 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#The complete code}

\NormalTok{cooc\_all\_f\_graph }\OtherTok{\textless{}{-}} \FunctionTok{as\_tbl\_graph}\NormalTok{(cooc\_all\_f, }\AttributeTok{directed =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{n\_rank\_trv =} \FunctionTok{node\_rank\_traveller}\NormalTok{(),}
         \AttributeTok{neighbors =} \FunctionTok{centrality\_degree}\NormalTok{(),}
         \AttributeTok{group =} \FunctionTok{group\_infomap}\NormalTok{(),}
         \AttributeTok{center =} \FunctionTok{node\_is\_center}\NormalTok{(),}
         \AttributeTok{dist\_to\_center =} \FunctionTok{node\_distance\_to}\NormalTok{(}\FunctionTok{node\_is\_center}\NormalTok{()),}
         \AttributeTok{keyplayer =} \FunctionTok{node\_is\_keyplayer}\NormalTok{(}\AttributeTok{k =} \DecValTok{10}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{activate}\NormalTok{(edges) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{edge\_is\_multiple}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{centrality\_e =} \FunctionTok{centrality\_edge\_betweenness}\NormalTok{())}

\NormalTok{cooc\_all\_f\_graph }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{activate}\NormalTok{(nodes) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# \%N\textgreater{}\%}
  \FunctionTok{as\_tibble}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 50 x 7
   name          n_rank_trv neighbors group center dist_to_center keyplayer
   <chr>              <int>     <dbl> <int> <lgl>           <dbl> <lgl>    
 1 Aemon-Targar~         15         7     1 FALSE               2 FALSE    
 2 Arya-Stark             4        24     1 FALSE               1 FALSE    
 3 Barristan-Se~         49        17     1 FALSE               1 FALSE    
 4 Bran-Stark            12        22     1 FALSE               1 FALSE    
 5 Brienne-of-T~         33        16     1 FALSE               1 FALSE    
 6 Bronn                 28        10     1 FALSE               1 FALSE    
 7 Catelyn-Stark         22        26     1 FALSE               1 FALSE    
 8 Cersei-Lanni~         40        30     1 TRUE                1 FALSE    
 9 Daenerys-Tar~         46        10     1 FALSE               2 TRUE     
10 Davos-Seawor~         10         6     1 FALSE               2 FALSE    
# ... with 40 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cooc\_all\_f\_graph }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{activate}\NormalTok{(edges) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# \%E\textgreater{}\%}
  \FunctionTok{as\_tibble}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 402 x 6
    from    to Type          id weight centrality_e
   <int> <int> <chr>      <dbl>  <dbl>        <dbl>
 1     1    15 Undirected    54      5      4.53552
 2     1    20 Undirected    57     25      2.28205
 3     1    23 Undirected    58    110      8.13462
 4     1    28 Undirected    60      5      2.76619
 5     1    39 Undirected    63      5     23.0559 
 6     1    41 Undirected    64     99      4.63597
 7     1    44 Undirected    65     12     12.8433 
 8     2     4 Undirected   299     40      5.05995
 9     2     7 Undirected   303      8      2.99633
10     2     8 Undirected   304     25      3.30954
# ... with 392 more rows
\end{verbatim}

Plotting with the package ``ggraph''

First, I am going to define a layout. There are lots of options for layouts, here I am using a Fruchterman-Reingold algorithm.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Plotting}

\NormalTok{layout }\OtherTok{\textless{}{-}} \FunctionTok{create\_layout}\NormalTok{(cooc\_all\_f\_graph, }
                        \AttributeTok{layout =} \StringTok{"fr"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The rest works like any ggplot2 function call, just that we use special geoms for our network, like geom\_edge\_density() to draw a shadow where the edge density is higher, geom\_edge\_link() to connect edges with a straight line, geom\_node\_point() to draw node points and geom\_node\_text() to draw the labels.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggraph}\NormalTok{(layout) }\SpecialCharTok{+} 
  \FunctionTok{geom\_edge\_density}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill =}\NormalTok{ weight)) }\SpecialCharTok{+}
  \FunctionTok{geom\_edge\_link}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{width =}\NormalTok{ weight), }\AttributeTok{alpha =} \FloatTok{0.2}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_node\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color =} \FunctionTok{factor}\NormalTok{(group)), }\AttributeTok{size =} \DecValTok{5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_node\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ name), }\AttributeTok{size =} \DecValTok{3}\NormalTok{, }\AttributeTok{repel =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_brewer}\NormalTok{(}\AttributeTok{palette =} \StringTok{"Set1"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_graph}\NormalTok{(}\AttributeTok{base\_family=}\StringTok{"sans"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"A Song of Ice and Fire character network"}\NormalTok{,}
       \AttributeTok{subtitle =} \StringTok{"Nodes are colored by group"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-568-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggsave}\NormalTok{(}\StringTok{"plotshiringroup.pdf"}\NormalTok{, }\AttributeTok{width =} \DecValTok{21}\NormalTok{, }\AttributeTok{height =} \FloatTok{29.7}\NormalTok{, }\AttributeTok{units =} \StringTok{"cm"}\NormalTok{)}

\NormalTok{cols }\OtherTok{\textless{}{-}}\NormalTok{ RColorBrewer}\SpecialCharTok{::}\FunctionTok{brewer.pal}\NormalTok{(}\DecValTok{3}\NormalTok{, }\StringTok{"Set1"}\NormalTok{)}

\FunctionTok{ggraph}\NormalTok{(layout) }\SpecialCharTok{+} 
  \FunctionTok{geom\_edge\_density}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill =}\NormalTok{ weight)) }\SpecialCharTok{+}
  \FunctionTok{geom\_edge\_link}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{width =}\NormalTok{ weight), }\AttributeTok{alpha =} \FloatTok{0.2}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_node\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color =} \FunctionTok{factor}\NormalTok{(center), }\AttributeTok{size =}\NormalTok{ dist\_to\_center)) }\SpecialCharTok{+}
  \FunctionTok{geom\_node\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ name), }\AttributeTok{size =} \DecValTok{3}\NormalTok{, }\AttributeTok{repel =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_colour\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(cols[}\DecValTok{2}\NormalTok{], cols[}\DecValTok{1}\NormalTok{])) }\SpecialCharTok{+}
  \FunctionTok{theme\_graph}\NormalTok{(}\AttributeTok{base\_family=}\StringTok{"sans"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"A Song of Ice and Fire character network"}\NormalTok{,}
       \AttributeTok{subtitle =} \StringTok{"Nodes are colored by centeredness"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-568-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggsave}\NormalTok{(}\StringTok{"plotshirin.pdf"}\NormalTok{, }\AttributeTok{width =} \DecValTok{21}\NormalTok{, }\AttributeTok{height =} \FloatTok{29.7}\NormalTok{, }\AttributeTok{units =} \StringTok{"cm"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Another visualisation, this time with the package ``visNetwork''

Graph-based analyses are many and diverse: whenever you can describe your data in terms of ``outgoing'' and ``receiving'' entities, a graph-based analysis and/or visualisation is possible. Let us try visualising previous results with another package called ``visNetwork''.

visNetwork is an R package for network visualization, using vis.js javascript library. Being based on htmlwidgets, it is compatible with shiny, R Markdown documents, and RStudio viewer. It is particularly easy to use, one can customise shapes, styles, colors, size. It works smoothly on any modern browser for up to a few thousand nodes and edges.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nodes2 }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{id =}\NormalTok{ (}\FunctionTok{unique}\NormalTok{(}\FunctionTok{c}\NormalTok{(cooc\_all\_f}\SpecialCharTok{$}\NormalTok{Source, cooc\_all\_f}\SpecialCharTok{$}\NormalTok{Target))), }\AttributeTok{group =}\NormalTok{ layout}\SpecialCharTok{$}\NormalTok{group)}
                               
\NormalTok{edges2 }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{from =}\NormalTok{ cooc\_all\_f}\SpecialCharTok{$}\NormalTok{Source,}
                    \AttributeTok{to =}\NormalTok{ (cooc\_all\_f}\SpecialCharTok{$}\NormalTok{Target))}

\FunctionTok{visNetwork}\NormalTok{(nodes2, edges2, }\AttributeTok{height =} \StringTok{"1000px"}\NormalTok{, }\AttributeTok{width =} \StringTok{"100\%"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{visLayout}\NormalTok{(}\AttributeTok{randomSeed =} \DecValTok{12}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# to have always the same network }
  \FunctionTok{visGroups}\NormalTok{(}\AttributeTok{groupname =} \StringTok{"1"}\NormalTok{, }\AttributeTok{color =} \StringTok{"red"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{visGroups}\NormalTok{(}\AttributeTok{groupname =} \StringTok{"2"}\NormalTok{, }\AttributeTok{color =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{visGroups}\NormalTok{(}\AttributeTok{groupname =} \StringTok{"3"}\NormalTok{, }\AttributeTok{color =} \StringTok{"green"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{visGroups}\NormalTok{(}\AttributeTok{groupname =} \StringTok{"4"}\NormalTok{, }\AttributeTok{color =} \StringTok{"purple"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{visGroups}\NormalTok{(}\AttributeTok{groupname =} \StringTok{"5"}\NormalTok{, }\AttributeTok{color =} \StringTok{"orange"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{visIgraphLayout}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{visOptions}\NormalTok{(}\AttributeTok{highlightNearest =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{visNodes}\NormalTok{(}\AttributeTok{size =} \DecValTok{15}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-569-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{visNetwork}\NormalTok{(nodes2, edges2, }\AttributeTok{height =} \StringTok{"1000px"}\NormalTok{, }\AttributeTok{width =} \StringTok{"100\%"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{visLayout}\NormalTok{(}\AttributeTok{randomSeed =} \DecValTok{12}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# to have always the same network }
  \FunctionTok{visGroups}\NormalTok{(}\AttributeTok{groupname =} \StringTok{"1"}\NormalTok{, }\AttributeTok{color =} \StringTok{"red"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{visGroups}\NormalTok{(}\AttributeTok{groupname =} \StringTok{"2"}\NormalTok{, }\AttributeTok{color =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{visGroups}\NormalTok{(}\AttributeTok{groupname =} \StringTok{"3"}\NormalTok{, }\AttributeTok{color =} \StringTok{"green"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{visGroups}\NormalTok{(}\AttributeTok{groupname =} \StringTok{"4"}\NormalTok{, }\AttributeTok{color =} \StringTok{"purple"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{visGroups}\NormalTok{(}\AttributeTok{groupname =} \StringTok{"5"}\NormalTok{, }\AttributeTok{color =} \StringTok{"orange"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{visIgraphLayout}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{visOptions}\NormalTok{(}\AttributeTok{highlightNearest =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{visNodes}\NormalTok{(}\AttributeTok{size =} \DecValTok{15}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{visEdges}\NormalTok{(}\AttributeTok{arrows =} \StringTok{"to"}\NormalTok{, }\AttributeTok{arrowStrikethrough =}\NormalTok{ F) }\SpecialCharTok{\%\textgreater{}\%}
\FunctionTok{visSave}\NormalTok{(}\AttributeTok{file =} \StringTok{"transfers2.html"}\NormalTok{, }\AttributeTok{selfcontained =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\hypertarget{job-classification}{%
\chapter{Job classification analysis}\label{job-classification}}

Job classification is a way for objectively and accurately defining and evaluating the duties, responsibilities, tasks, and authority level of a job. When done correctly, the job classification is a thorough description of the job responsibilities of a position without regard to the knowledge, skills, experience, and education of the individuals currently performing the job.

Job classification is most frequently, formally performed in large companies, civil service and government employment, nonprofit organisations, colleges and universities. The approach used in these organizations is formal and structured.

One popular, commercial job classification system is the Hay Classification system. The Hay job classification system assigns points to evaluate job components to determine the relative value of a particular job to other jobs.

The primary goal of a job classification is to classify job descriptions into job classifications using the power of statistical algorithms to assist in predicting the best fit. The secondary goal can be to improve the design of our job classification framework.

\#\#2.Collect And Manage Data

For purposes of this application of People Analytics, this step in the data science process will take the longest initially. This is because in almost every organization, the existing job classifications or categories, and the job descriptions themselves are not typically represented in numerical format suitable for statistical analysis. Sometimes, that which we are predicting- the pay grade is numeric because point methods are used in evaluation and different paygrades have different point ranges. But more often the job descriptions are narrative as are the job classification specs or summaries. For this blog article, we will assume that and delineate the steps required.

\#\#\#Collecting The Data

The following are typical steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Gather together the entire set of narrative, written job classification specifications.
\item
  Review all of them to determine what the common denominators are- what the organization is paying attention to , to differentiate them from each other.
\item
  For each of the common denominators, pay attention to descriptions of how much of that common denominator exists in each narrative, writing down the phrases that are used.
\item
  For each common denominator, develop an ordinal scale which assigns numbers and places them in a `less to more' order
\item
  Create a datafile where each record (row) is one job classification, and where each column is either a common denominator or the job classification identifier or paygrade.
\item
  Code each job classification narrative into the datafile recording their common denominator information and other pertinent categorical information.
\end{enumerate}

\#\#\#\#Gather together the entire set of narrative, written job classification specifications.

This initially represents the `total' population of what will be a `known' population. Ones that by definition represent the prescribed intended categories and levels of paygrades. These are going to be used to compare an `unknown' population- unclassified job descriptions, to determine best fit. But before this can happen, we should have confidence that the job classifications themselves are well designed- since they will be the standard against which all job descriptions will be compared.

\#\#\#\#Review all of them to determine what the common denominators are

Technically speaking, anything that appears in the narrative could be considered a feature that is a common denominator including the tasks, knowledges described. But few organizations have that level of automation in their job descriptions. So generally broader features are used to describe common denominators. Often they may include the following:

\begin{itemize}
\tightlist
\item
  Education Level
\item
  Experience
\item
  Organizational Impact
\item
  Problem Solving
\item
  Supervision Received
\item
  Contact Level
\item
  Financial Budget Responsibility
\end{itemize}

To be a common denominator they need to be mentioned or discernable in every job classification specification

\#\#\#\#Pay attention to the descriptions of how much of that common denominator exists in each narrative

For each of the above common denominators ( if these are ones you use), go through each narrative identify where the common denominator is mentioned and write down the words used to describe how much of it exists. Go through you entire set of job classification specs and tabulate these for each common denominator and each class spec.

\#\#\#\#For each common denominator, develop an ordinal scale

Ordinal means in order. You order the descriptions from less than to more than. Then apply a numerical indicator to it. 0 might mean it doesnt exist in any significant way, 1 might mean something at a low or introductory level, higher numbers meaning more of it. The scale should have as many numbers as distinguishable descriptions.(You may have to merge or collapse descriptions if it's impossible to distinguish order)

\#\#\#\#Create a datafile

This might be a spreadsheet.

each record(row) will be one job classification, and each column will be either a common denominator or the job classification identifier or paygrade or other categorical information.

\#\#\#\#Code each job classification narrative into the datafile

Record their common denominator information and other pertinent categorical or identifying information. At the end of this task you will have as many records as you have written job classification specs.

At the end of this effort you will have something that looks like the data found at the following link:

\url{https://onedrive.live.com/redir?resid=4EF2CCBEDB98D0F5!6435\&authkey=!AL37Wt0sVLrsUYA\&ithint=file\%2ctxt}

Ensure all needed libraries are installed

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(caret)}
\FunctionTok{library}\NormalTok{(rattle)}
\FunctionTok{library}\NormalTok{(rpart)}
\FunctionTok{library}\NormalTok{(randomForest)}
\FunctionTok{library}\NormalTok{(kernlab)}
\FunctionTok{library}\NormalTok{(nnet)}
\FunctionTok{library}\NormalTok{(car)}
\FunctionTok{library}\NormalTok{(rpart.plot)}
\FunctionTok{library}\NormalTok{(pROC)}
\FunctionTok{library}\NormalTok{(ada)}
\end{Highlighting}
\end{Shaded}

\#\#\#Manage The Data

In this step we check the data for errors, organize the data for model building, and take an initial look at what the data is telling us.

\#\#\#\#Check the data for errors

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MYdataset }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"https://https://hranalytics.netlify.com/data/jobclassinfo2.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(MYdataset)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
tibble [66 x 14] (S3: spec_tbl_df/tbl_df/tbl/data.frame)
 $ ID                  : num [1:66] 1 2 3 4 5 6 7 8 9 10 ...
 $ JobFamily           : num [1:66] 1 1 1 1 2 2 2 2 2 3 ...
 $ JobFamilyDescription: chr [1:66] "Accounting And Finance" "Accounting And Finance" "Accounting And Finance" "Accounting And Finance" ...
 $ JobClass            : num [1:66] 1 2 3 4 5 6 7 8 9 10 ...
 $ JobClassDescription : chr [1:66] "Accountant I" "Accountant II" "Accountant III" "Accountant IV" ...
 $ PayGrade            : num [1:66] 5 6 8 10 1 2 3 4 5 4 ...
 $ EducationLevel      : num [1:66] 3 4 4 5 1 1 1 4 4 2 ...
 $ Experience          : num [1:66] 1 1 2 5 0 1 2 0 0 0 ...
 $ OrgImpact           : num [1:66] 3 5 6 6 1 1 1 1 4 1 ...
 $ ProblemSolving      : num [1:66] 3 4 5 6 1 1 2 2 3 4 ...
 $ Supervision         : num [1:66] 4 5 6 7 1 1 1 1 5 1 ...
 $ ContactLevel        : num [1:66] 3 7 7 8 1 2 3 3 7 1 ...
 $ FinancialBudget     : num [1:66] 5 7 10 11 1 3 3 5 7 2 ...
 $ PG                  : chr [1:66] "PG05" "PG06" "PG08" "PG10" ...
 - attr(*, "spec")=
  .. cols(
  ..   ID = col_double(),
  ..   JobFamily = col_double(),
  ..   JobFamilyDescription = col_character(),
  ..   JobClass = col_double(),
  ..   JobClassDescription = col_character(),
  ..   PayGrade = col_double(),
  ..   EducationLevel = col_double(),
  ..   Experience = col_double(),
  ..   OrgImpact = col_double(),
  ..   ProblemSolving = col_double(),
  ..   Supervision = col_double(),
  ..   ContactLevel = col_double(),
  ..   FinancialBudget = col_double(),
  ..   PG = col_character()
  .. )
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(MYdataset)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       ID         JobFamily     JobFamilyDescription    JobClass   
 Min.   : 1.0   Min.   : 1.00   Length:66            Min.   : 1.0  
 1st Qu.:17.2   1st Qu.: 4.00   Class :character     1st Qu.:17.2  
 Median :33.5   Median : 7.00   Mode  :character     Median :33.5  
 Mean   :33.5   Mean   : 7.61                        Mean   :33.5  
 3rd Qu.:49.8   3rd Qu.:11.00                        3rd Qu.:49.8  
 Max.   :66.0   Max.   :15.00                        Max.   :66.0  
 JobClassDescription    PayGrade    EducationLevel   Experience   
 Length:66           Min.   : 1.0   Min.   :1.00   Min.   : 0.00  
 Class :character    1st Qu.: 4.0   1st Qu.:2.00   1st Qu.: 0.00  
 Mode  :character    Median : 5.0   Median :4.00   Median : 1.00  
                     Mean   : 5.7   Mean   :3.17   Mean   : 1.76  
                     3rd Qu.: 8.0   3rd Qu.:4.00   3rd Qu.: 2.75  
                     Max.   :10.0   Max.   :6.00   Max.   :10.00  
   OrgImpact    ProblemSolving  Supervision    ContactLevel 
 Min.   :1.00   Min.   :1.00   Min.   :1.00   Min.   :1.00  
 1st Qu.:2.00   1st Qu.:3.00   1st Qu.:1.00   1st Qu.:3.00  
 Median :3.00   Median :4.00   Median :4.00   Median :6.00  
 Mean   :3.35   Mean   :3.61   Mean   :3.86   Mean   :4.76  
 3rd Qu.:4.00   3rd Qu.:5.00   3rd Qu.:5.75   3rd Qu.:7.00  
 Max.   :6.00   Max.   :6.00   Max.   :7.00   Max.   :8.00  
 FinancialBudget      PG           
 Min.   : 1.00   Length:66         
 1st Qu.: 2.00   Class :character  
 Median : 5.00   Mode  :character  
 Mean   : 5.30                     
 3rd Qu.: 7.75                     
 Max.   :11.00                     
\end{verbatim}

On the surface there doesn't seem to be any issues with data. This gives a summary of the layout of the data and the likely values we can expect. PG is the category we will predict. It's a categorical representation of the numeric paygrade. Education level through Financial Budgeting Responsibility will be the independent variables/measures we will use to predict. The other columns in file will be ignored.

\#\#\#\#Organize the data

Let's narrow down the information to just the data used in the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MYnobs }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(MYdataset) }\CommentTok{\# The data set is made of 66 observations}

\NormalTok{MYsample }\OtherTok{\textless{}{-}}\NormalTok{ MYtrain }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(MYdataset), }\FloatTok{0.7}\SpecialCharTok{*}\NormalTok{MYnobs) }\CommentTok{\# 70\% of those 66 observations (i.e. 46 observations) will form our training dataset.}

\NormalTok{MYvalidate }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\FunctionTok{setdiff}\NormalTok{(}\FunctionTok{seq\_len}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(MYdataset)), MYtrain), }\FloatTok{0.14}\SpecialCharTok{*}\NormalTok{MYnobs) }\CommentTok{\# 14\% of those 66 observations (i.e. 9 observations) will form our validation dataset.}

\NormalTok{MYtest }\OtherTok{\textless{}{-}} \FunctionTok{setdiff}\NormalTok{(}\FunctionTok{setdiff}\NormalTok{(}\FunctionTok{seq\_len}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(MYdataset)), MYtrain), MYvalidate) }\CommentTok{\# \# The remaining observations (i.e. 11 observations) will form our test dataset.}


\CommentTok{\# The following variable selections have been noted.}
\NormalTok{MYinput }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"EducationLevel"}\NormalTok{, }\StringTok{"Experience"}\NormalTok{, }\StringTok{"OrgImpact"}\NormalTok{, }\StringTok{"ProblemSolving"}\NormalTok{,}
     \StringTok{"Supervision"}\NormalTok{, }\StringTok{"ContactLevel"}\NormalTok{, }\StringTok{"FinancialBudget"}\NormalTok{)}

\NormalTok{MYnumeric }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"EducationLevel"}\NormalTok{, }\StringTok{"Experience"}\NormalTok{, }\StringTok{"OrgImpact"}\NormalTok{, }\StringTok{"ProblemSolving"}\NormalTok{,}
     \StringTok{"Supervision"}\NormalTok{, }\StringTok{"ContactLevel"}\NormalTok{, }\StringTok{"FinancialBudget"}\NormalTok{)}

\NormalTok{MYcategoric }\OtherTok{\textless{}{-}} \ConstantTok{NULL}

\NormalTok{MYtarget  }\OtherTok{\textless{}{-}} \StringTok{"PG"}
\NormalTok{MYrisk    }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\NormalTok{MYident   }\OtherTok{\textless{}{-}} \StringTok{"ID"}
\NormalTok{MYignore  }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"JobFamily"}\NormalTok{, }\StringTok{"JobFamilyDescription"}\NormalTok{, }\StringTok{"JobClass"}\NormalTok{, }\StringTok{"JobClassDescription"}\NormalTok{, }\StringTok{"PayGrade"}\NormalTok{)}
\NormalTok{MYweights }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\end{Highlighting}
\end{Shaded}

We are predominantly interested in MYinput and MYtarget because they represent the predictors and what needs to be predicted respectively. You will notice for the time being that we are not partitioning the data. This will be elaborated upon in model building.

\#\#\#What the data is initially telling us

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MYdataset }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{factor}\NormalTok{(PG)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"count"}\NormalTok{, }\AttributeTok{width =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{fill =} \StringTok{"steelblue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Number of job classifications per PG category"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-575-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MYdataset }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{factor}\NormalTok{(JobFamilyDescription)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"count"}\NormalTok{, }\AttributeTok{width =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{fill =} \StringTok{"steelblue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Number of job classifications per job family"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-575-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MYdataset }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{aes}\NormalTok{(EducationLevel) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"count"}\NormalTok{, }\AttributeTok{width =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{fill =} \StringTok{"steelblue"}\NormalTok{)  }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Number of job classifications per Education level"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-575-3} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MYdataset }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{aes}\NormalTok{(Experience) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"count"}\NormalTok{, }\AttributeTok{width =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{fill =} \StringTok{"steelblue"}\NormalTok{)  }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Number of job classifications per experience"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-575-4} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MYdataset }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{aes}\NormalTok{(OrgImpact) }\SpecialCharTok{+} 
    \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"count"}\NormalTok{, }\AttributeTok{width =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{fill =} \StringTok{"steelblue"}\NormalTok{)  }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Number of job classifications per organisational impact"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-575-5} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MYdataset }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{aes}\NormalTok{(ProblemSolving) }\SpecialCharTok{+} 
    \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"count"}\NormalTok{, }\AttributeTok{width =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{fill =} \StringTok{"steelblue"}\NormalTok{)  }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Number of job classifications per problem solving"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-575-6} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MYdataset }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{aes}\NormalTok{(Supervision) }\SpecialCharTok{+} 
    \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"count"}\NormalTok{, }\AttributeTok{width =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{fill =} \StringTok{"steelblue"}\NormalTok{)  }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Number of job classifications per supervision"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-575-7} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MYdataset }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{aes}\NormalTok{(ContactLevel) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"count"}\NormalTok{, }\AttributeTok{width =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{fill =} \StringTok{"steelblue"}\NormalTok{)  }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Number of job classifications per contact level"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-575-8} \end{center}

Let's use the caret library again for some graphical representations of this data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(caret)}

\NormalTok{MYdataset}\SpecialCharTok{$}\NormalTok{PG }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(MYdataset}\SpecialCharTok{$}\NormalTok{PG)}

\FunctionTok{featurePlot}\NormalTok{(}\AttributeTok{x =}\NormalTok{ MYdataset[,}\DecValTok{7}\SpecialCharTok{:}\DecValTok{13}\NormalTok{], }
            \AttributeTok{y =}\NormalTok{ MYdataset}\SpecialCharTok{$}\NormalTok{PG, }
            \AttributeTok{plot =} \StringTok{"density"}\NormalTok{, }
            \AttributeTok{auto.key =} \FunctionTok{list}\NormalTok{(}\AttributeTok{columns =} \DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-576-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{featurePlot}\NormalTok{(}\AttributeTok{x =}\NormalTok{ MYdataset[,}\DecValTok{7}\SpecialCharTok{:}\DecValTok{13}\NormalTok{], }
            \AttributeTok{y =}\NormalTok{ MYdataset}\SpecialCharTok{$}\NormalTok{PG, }
            \AttributeTok{plot =} \StringTok{"box"}\NormalTok{, }
            \AttributeTok{auto.key =} \FunctionTok{list}\NormalTok{(}\AttributeTok{columns =} \DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-576-2} \end{center}

The first set of charts shows the distribution of the independent variable values (predictors) by PG.

The second set of charts show the range of values of the predictors by PG. PG is ordered left to right in ascending order from PG1 to PG10. In each of the predictors we would expect increasing levels as we move up the paygrades and from left to right (or at least not dropping from previous paygrade).

This is the first indication by a graphic `visual' that we `may' have problems in the data or the interpretation of the coding of the information. Then again the coding may be accurate based on our descriptions and our assumptions false. We will probably want to recheck our coding from the job description to make sure.

\#\#3.Build The Model

Let's use the rattle library to efficiently generate the code to run the following classification algorithms against our data:

\begin{itemize}
\tightlist
\item
  Decision Tree
\item
  Random Forest
\item
  Support Vector Machines
\item
  Linear Regression Model
\end{itemize}

Please note that in our case we want to use the job description to predict the payscale grade (PG), so we make the formula (PG \textasciitilde{} .). In other words, we're representing the relationship between payscale grades (PG) and the remaining variables (.).

\#\#\#Decision Tree

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# The \textquotesingle{}rattle\textquotesingle{} package provides a graphical user interface to very many other packages that provide functionality for data mining.}

\FunctionTok{library}\NormalTok{(rattle)}

\CommentTok{\# The \textquotesingle{}rpart\textquotesingle{} package provides the \textquotesingle{}rpart\textquotesingle{} function.}

\FunctionTok{library}\NormalTok{(rpart, }\AttributeTok{quietly=}\ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\# Reset the random number seed to obtain the same results each time.}
\NormalTok{crv}\SpecialCharTok{$}\NormalTok{seed }\OtherTok{\textless{}{-}} \DecValTok{42} 
\FunctionTok{set.seed}\NormalTok{(crv}\SpecialCharTok{$}\NormalTok{seed)}

\CommentTok{\# Build the Decision Tree model.}

\NormalTok{MYrpart }\OtherTok{\textless{}{-}} \FunctionTok{rpart}\NormalTok{(PG }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .,}
    \AttributeTok{data=}\NormalTok{MYdataset[, }\FunctionTok{c}\NormalTok{(MYinput, MYtarget)],}
    \AttributeTok{method=}\StringTok{"class"}\NormalTok{,}
    \AttributeTok{parms=}\FunctionTok{list}\NormalTok{(}\AttributeTok{split=}\StringTok{"information"}\NormalTok{),}
      \AttributeTok{control=}\FunctionTok{rpart.control}\NormalTok{(}\AttributeTok{minsplit=}\DecValTok{10}\NormalTok{,}
           \AttributeTok{minbucket=}\DecValTok{2}\NormalTok{,}
           \AttributeTok{maxdepth=}\DecValTok{10}\NormalTok{,}
        \AttributeTok{usesurrogate=}\DecValTok{0}\NormalTok{, }
        \AttributeTok{maxsurrogate=}\DecValTok{0}\NormalTok{))}

\CommentTok{\# Generate a textual view of the Decision Tree model.}

\FunctionTok{print}\NormalTok{(MYrpart)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
n= 66 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

 1) root 66 51 PG05 (0.03 0.076 0.11 0.11 0.23 0.11 0.061 0.11 0.091 0.091)  
   2) ProblemSolving< 4.5 47 32 PG05 (0.043 0.11 0.15 0.15 0.32 0.15 0.085 0 0 0)  
     4) ContactLevel< 5.5 32 21 PG05 (0.062 0.16 0.22 0.22 0.34 0 0 0 0 0)  
       8) EducationLevel< 1.5 15  9 PG03 (0.13 0.33 0.4 0.13 0 0 0 0 0 0)  
        16) ProblemSolving< 1.5 5  2 PG02 (0.4 0.6 0 0 0 0 0 0 0 0) *
        17) ProblemSolving>=1.5 10  4 PG03 (0 0.2 0.6 0.2 0 0 0 0 0 0)  
          34) Experience< 0.5 3  1 PG02 (0 0.67 0 0.33 0 0 0 0 0 0) *
          35) Experience>=0.5 7  1 PG03 (0 0 0.86 0.14 0 0 0 0 0 0) *
       9) EducationLevel>=1.5 17  6 PG05 (0 0 0.059 0.29 0.65 0 0 0 0 0)  
        18) Experience< 0.5 8  3 PG04 (0 0 0 0.62 0.37 0 0 0 0 0) *
        19) Experience>=0.5 9  1 PG05 (0 0 0.11 0 0.89 0 0 0 0 0) *
     5) ContactLevel>=5.5 15  8 PG06 (0 0 0 0 0.27 0.47 0.27 0 0 0)  
      10) Experience< 2.5 12  5 PG06 (0 0 0 0 0.33 0.58 0.083 0 0 0)  
        20) ContactLevel>=6.5 8  4 PG05 (0 0 0 0 0.5 0.38 0.12 0 0 0) *
        21) ContactLevel< 6.5 4  0 PG06 (0 0 0 0 0 1 0 0 0 0) *
      11) Experience>=2.5 3  0 PG07 (0 0 0 0 0 0 1 0 0 0) *
   3) ProblemSolving>=4.5 19 12 PG08 (0 0 0 0 0 0 0 0.37 0.32 0.32)  
     6) ProblemSolving< 5.5 13  6 PG08 (0 0 0 0 0 0 0 0.54 0.46 0)  
      12) ContactLevel>=6.5 10  3 PG08 (0 0 0 0 0 0 0 0.7 0.3 0) *
      13) ContactLevel< 6.5 3  0 PG09 (0 0 0 0 0 0 0 0 1 0) *
     7) ProblemSolving>=5.5 6  0 PG10 (0 0 0 0 0 0 0 0 0 1) *
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{printcp}\NormalTok{(MYrpart)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Classification tree:
rpart(formula = PG ~ ., data = MYdataset[, c(MYinput, MYtarget)], 
    method = "class", parms = list(split = "information"), control = rpart.control(minsplit = 10, 
        minbucket = 2, maxdepth = 10, usesurrogate = 0, maxsurrogate = 0))

Variables actually used in tree construction:
[1] ContactLevel   EducationLevel Experience     ProblemSolving

Root node error: 51/66 = 0.8

n= 66 

    CP nsplit rel error xerror xstd
1 0.14      0       1.0    1.0 0.07
2 0.12      1       0.9    0.9 0.08
3 0.09      2       0.7    0.8 0.08
4 0.06      4       0.6    0.7 0.08
5 0.04      7       0.4    0.7 0.08
6 0.02      9       0.3    0.6 0.08
7 0.01     10       0.3    0.6 0.08
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\#\#\#Random Forest

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# The \textquotesingle{}randomForest\textquotesingle{} package provides the \textquotesingle{}randomForest\textquotesingle{} function.}

\FunctionTok{library}\NormalTok{(randomForest, }\AttributeTok{quietly=}\ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\# Build the Random Forest model.}

\FunctionTok{set.seed}\NormalTok{(crv}\SpecialCharTok{$}\NormalTok{seed)}

\NormalTok{MYrf }\OtherTok{\textless{}{-}}\NormalTok{ randomForest}\SpecialCharTok{::}\FunctionTok{randomForest}\NormalTok{(PG }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\CommentTok{\# PG \textasciitilde{} .}
      \AttributeTok{data=}\NormalTok{MYdataset[,}\FunctionTok{c}\NormalTok{(MYinput, MYtarget)], }
      \AttributeTok{ntree=}\DecValTok{500}\NormalTok{,}
      \AttributeTok{mtry=}\DecValTok{2}\NormalTok{,}
      \AttributeTok{importance=}\ConstantTok{TRUE}\NormalTok{,}
      \AttributeTok{na.action=}\NormalTok{randomForest}\SpecialCharTok{::}\NormalTok{na.roughfix,}
      \AttributeTok{replace=}\ConstantTok{FALSE}\NormalTok{)}

\CommentTok{\# Generate textual output of \textquotesingle{}Random Forest\textquotesingle{} model.}

\NormalTok{MYrf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Call:
 randomForest(formula = PG ~ ., data = MYdataset[, c(MYinput,      MYtarget)], ntree = 500, mtry = 2, importance = TRUE, replace = FALSE,      na.action = randomForest::na.roughfix) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 2

        OOB estimate of  error rate: 42.4%
Confusion matrix:
     PG01 PG02 PG03 PG04 PG05 PG06 PG07 PG08 PG09 PG10 class.error
PG01    0    2    0    0    0    0    0    0    0    0       1.000
PG02    0    4    1    0    0    0    0    0    0    0       0.200
PG03    0    0    4    2    1    0    0    0    0    0       0.429
PG04    0    0    1    2    4    0    0    0    0    0       0.714
PG05    0    0    0    3   10    2    0    0    0    0       0.333
PG06    0    0    0    0    1    5    1    0    0    0       0.286
PG07    0    0    0    0    0    2    2    0    0    0       0.500
PG08    0    0    0    0    0    0    0    4    2    1       0.429
PG09    0    0    0    0    0    0    0    5    1    0       0.833
PG10    0    0    0    0    0    0    0    0    0    6       0.000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# List the importance of the variables.}

\NormalTok{rn }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(randomForest}\SpecialCharTok{::}\FunctionTok{importance}\NormalTok{(MYrf), }\DecValTok{2}\NormalTok{)}
\NormalTok{rn[}\FunctionTok{order}\NormalTok{(rn[,}\DecValTok{3}\NormalTok{], }\AttributeTok{decreasing=}\ConstantTok{TRUE}\NormalTok{),]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                 PG01  PG02  PG03  PG04  PG05  PG06  PG07  PG08  PG09
EducationLevel   2.85 14.92 11.36  5.05  7.93  1.73  5.19  3.56 -5.47
ProblemSolving   3.65  5.98  8.96  1.51  8.91 13.08  7.66 12.60 10.04
Experience      -4.68  9.85  8.70  5.32  0.66  5.99  8.10 -5.70  4.12
Supervision     -2.86  9.78  5.45  2.44  9.26  3.57  0.77  3.33  2.03
ContactLevel     2.46 13.33  3.70  2.45  4.03 10.09  4.65 11.16 -0.97
OrgImpact       -2.70 11.72  3.65  0.58 12.85  2.37  5.22  1.86  6.10
FinancialBudget  1.74  6.18  2.05 -1.03  9.37  0.87 -0.52 -4.66 10.82
                 PG10 MeanDecreaseAccuracy MeanDecreaseGini
EducationLevel   8.60                 18.4             4.33
ProblemSolving  19.57                 24.2             6.26
Experience       3.11                 13.8             4.30
Supervision     16.36                 17.5             4.01
ContactLevel    12.24                 19.1             4.97
OrgImpact       10.58                 15.8             3.57
FinancialBudget 15.78                 15.5             5.17
\end{verbatim}

\#\#\#Support Vector Machine

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# The \textquotesingle{}kernlab\textquotesingle{} package provides the \textquotesingle{}ksvm\textquotesingle{} function.}

\FunctionTok{library}\NormalTok{(kernlab, }\AttributeTok{quietly=}\ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\# Build a Support Vector Machine model.}

\CommentTok{\#set.seed(crv$seed)}
\NormalTok{MYksvm }\OtherTok{\textless{}{-}} \FunctionTok{ksvm}\NormalTok{(}\FunctionTok{as.factor}\NormalTok{(PG) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .,}
      \AttributeTok{data=}\NormalTok{MYdataset[,}\FunctionTok{c}\NormalTok{(MYinput, MYtarget)],}
      \AttributeTok{kernel=}\StringTok{"rbfdot"}\NormalTok{,}
      \AttributeTok{prob.model=}\ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\# Generate a textual view of the SVM model.}

\NormalTok{MYksvm}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Support Vector Machine object of class "ksvm" 

SV type: C-svc  (classification) 
 parameter : cost C = 1 

Gaussian Radial Basis kernel function. 
 Hyperparameter : sigma =  0.124564242327033 

Number of Support Vectors : 64 

Objective Function Value : -3.87 -3.73 -3.11 -2.49 -1.6 -1.34 -1.33 -1.48 -1.33 -8.1 -4.95 -2.89 -1.74 -1.34 -1.32 -1.48 -1.32 -9.18 -6.9 -3.32 -2.46 -1.81 -2.08 -1.61 -10.7 -4.24 -2.67 -2.06 -2.47 -1.74 -10.7 -6.73 -4.03 -4 -2.23 -7.76 -5.98 -5.13 -2.4 -5.1 -4.41 -2.21 -10.1 -6.83 -7 
Training error : 0.333333 
Probability model included. 
\end{verbatim}

\#\#\#Linear Regression Model

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Build a multinomial model using the nnet package.}

\FunctionTok{library}\NormalTok{(nnet, }\AttributeTok{quietly=}\ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\# Summarise multinomial model using Anova from the car package.}

\FunctionTok{library}\NormalTok{(car, }\AttributeTok{quietly=}\ConstantTok{TRUE}\NormalTok{)}

\CommentTok{\# Build a Regression model.}

\NormalTok{MYglm }\OtherTok{\textless{}{-}} \FunctionTok{multinom}\NormalTok{(PG }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data=}\NormalTok{MYdataset[,}\FunctionTok{c}\NormalTok{(MYinput, MYtarget)], }\AttributeTok{trace=}\ConstantTok{FALSE}\NormalTok{, }\AttributeTok{maxit=}\DecValTok{1000}\NormalTok{)}

\CommentTok{\# Generate a textual view of the Linear model.}

\FunctionTok{rattle.print.summary.multinom}\NormalTok{(}\FunctionTok{summary}\NormalTok{(MYglm,}
                              \AttributeTok{Wald.ratios=}\ConstantTok{TRUE}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Call:
multinom(formula = PG ~ ., data = MYdataset[, c(MYinput, MYtarget)], 
    trace = FALSE, maxit = 1000)

n=66

Coefficients:
     (Intercept) EducationLevel Experience OrgImpact ProblemSolving
PG02       31812         -29773     -10132    -31986          23063
PG03        5213         -14099      17747    -23614          27792
PG04      -12736           3771      -5808    -12061          18671
PG05      -29448           5204       1463    -12676          21706
PG06      -65435           5423       1343    -10231          28060
PG07      -55610           5421       1344    -10234          25601
PG08     -125872           6245        577    -14007          42812
PG09      -98509         -14539       8765     -8749         -22069
PG10     -203005           9258        771    -15602          55341
     Supervision ContactLevel FinancialBudget
PG02       -3856        -3493           14234
PG03       -8041         4086           -2148
PG04       -1212        -4092            6203
PG05       -3161         -994            5801
PG06       -3568         -752            6308
PG07       -3567         -749            6308
PG08       -1877        -1738            7323
PG09       21230       -12837           34381
PG10       -2794         1162            6006

Std. Errors:
     (Intercept)      EducationLevel          Experience
PG02       0.175 0.17496351272064589 0.00000000000000000
PG03       0.000 0.00000000000000000 0.00000000000000000
PG04       0.000 0.00000000000000257                 NaN
PG05         NaN                 NaN 0.00000000000000156
PG06       0.386 1.39726829398874242 0.66504625210025126
PG07       0.386 1.39726829398858898 0.66504625210022916
PG08       0.000 0.00000000000000000 0.00000000000000000
PG09       0.000 0.00000000000000000 0.00000000000000000
PG10       0.000 0.00000000000000000 0.00000000000000000
              OrgImpact      ProblemSolving Supervision
PG02 0.1749635127206459 0.17496351272064589       0.175
PG03 0.0000000000000000 0.00000000000000000       0.000
PG04 0.0000000000000108 0.00000000000000756         NaN
PG05                NaN                 NaN         NaN
PG06 1.3664268009899736 1.54374337492899638       0.881
PG07 1.3664268009899854 1.54374337492892133       0.881
PG08 0.0000000000000000 0.00000000000000000       0.000
PG09 0.0000000000000000 0.00000000000000000       0.000
PG10 0.0000000000000000 0.00000000000000000       0.000
            ContactLevel FinancialBudget
PG02 0.17496351272064589           0.175
PG03 0.00000000000000000           0.000
PG04 0.00000000000000144             NaN
PG05                 NaN             NaN
PG06 1.29803445271060580           0.322
PG07 1.29803445271063556           0.322
PG08 0.00000000000000000           0.000
PG09 0.00000000000000000           0.000
PG10 0.00000000000000000           0.000

Value/SE (Wald statistics):
     (Intercept)      EducationLevel         Experience
PG02      181823             -170166               -Inf
PG03         Inf                -Inf                Inf
PG04        -Inf 1467533350215317248                NaN
PG05         NaN                 NaN 935764317710622208
PG06     -169549                3881               2019
PG07     -144092                3880               2022
PG08        -Inf                 Inf                Inf
PG09        -Inf                -Inf                Inf
PG10        -Inf                 Inf                Inf
                OrgImpact      ProblemSolving Supervision
PG02              -182818              131814      -22037
PG03                 -Inf                 Inf        -Inf
PG04 -1121500571223810048 2468598833059940864         NaN
PG05                  NaN                 NaN         NaN
PG06                -7487               18177       -4050
PG07                -7489               16584       -4049
PG08                 -Inf                 Inf        -Inf
PG09                 -Inf                -Inf         Inf
PG10                 -Inf                 Inf        -Inf
             ContactLevel FinancialBudget
PG02               -19964           81352
PG03                  Inf            -Inf
PG04 -2833778518965696000             NaN
PG05                  NaN             NaN
PG06                 -579           19610
PG07                 -577           19610
PG08                 -Inf             Inf
PG09                 -Inf             Inf
PG10                  Inf             Inf

Residual Deviance: 13.1 
AIC: 157 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\FunctionTok{sprintf}\NormalTok{(}\StringTok{"Log likelihood: \%.3f (\%d df)}
\StringTok{"}\NormalTok{, }\FunctionTok{logLik}\NormalTok{(MYglm)[}\DecValTok{1}\NormalTok{], }\FunctionTok{attr}\NormalTok{(}\FunctionTok{logLik}\NormalTok{(MYglm), }\StringTok{"df"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Log likelihood: -6.545 (72 df)
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{ (}\FunctionTok{is.null}\NormalTok{(MYglm}\SpecialCharTok{$}\NormalTok{na.action)) omitted }\OtherTok{\textless{}{-}} \ConstantTok{TRUE} \ControlFlowTok{else}\NormalTok{ omitted }\OtherTok{\textless{}{-}} \SpecialCharTok{{-}}\NormalTok{MYglm}\SpecialCharTok{$}\NormalTok{na.action}
\FunctionTok{cat}\NormalTok{(}\FunctionTok{sprintf}\NormalTok{(}\StringTok{"Pseudo R{-}Square: \%.8f}

\StringTok{"}\NormalTok{,}\FunctionTok{cor}\NormalTok{(}\FunctionTok{apply}\NormalTok{(MYglm}\SpecialCharTok{$}\NormalTok{fitted.values, }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) }\FunctionTok{which}\NormalTok{(x }\SpecialCharTok{==} \FunctionTok{max}\NormalTok{(x))),}
\FunctionTok{as.integer}\NormalTok{(MYdataset[omitted,]}\SpecialCharTok{$}\NormalTok{PG))))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Pseudo R-Square: 0.99516038
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{\textquotesingle{}==== ANOVA ====\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
==== ANOVA ====
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(}\FunctionTok{Anova}\NormalTok{(MYglm))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Deviance Table (Type II tests)

Response: PG
                LR Chisq Df Pr(>Chisq)   
EducationLevel     14.34  9      0.111   
Experience         24.21  9      0.004 **
OrgImpact           1.61  9      0.996   
ProblemSolving     21.11  9      0.012 * 
Supervision         2.92  9      0.967   
ContactLevel        4.86  9      0.847   
FinancialBudget     5.55  9      0.784   
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Now let's plot the Decision Tree

\#\#\#Decision Tree Plot

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Plot the resulting Decision Tree. }

\CommentTok{\# We use the rpart.plot package.}

\FunctionTok{fancyRpartPlot}\NormalTok{(MYrpart, }\AttributeTok{main=}\StringTok{"Decision Tree MYdataset $ PG"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-581-1} \end{center}

A readable view of the decision tree can be found at the following pdf:

\url{https://onedrive.live.com/redir?resid=4EF2CCBEDB98D0F5!6449\&authkey=!ACgJAX951UZuo4s\&ithint=file\%2cpdf}

\#\#4.Evaluation of the best fitting model

\#\#\#Evaluate

In the following we will evaluate the best fitting model creating a confusion matrix. A confusion matrix is a specific table layout that allows visualization of the performance of an algorithm. Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa). The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e.~commonly mislabeling one as another).

\#\#\#\#Decision Tree

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Predict new job classsifications utilising the Decision Tree model.}

\NormalTok{MYpr }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(MYrpart, }\AttributeTok{newdata=}\NormalTok{MYdataset[,}\FunctionTok{c}\NormalTok{(MYinput, MYtarget)], }\AttributeTok{type=}\StringTok{"class"}\NormalTok{)}

\CommentTok{\# Generate the confusion matrix showing counts.}

\FunctionTok{table}\NormalTok{(MYdataset[,}\FunctionTok{c}\NormalTok{(MYinput, MYtarget)]}\SpecialCharTok{$}\NormalTok{PG, MYpr,}
        \AttributeTok{dnn=}\FunctionTok{c}\NormalTok{(}\StringTok{"Actual"}\NormalTok{, }\StringTok{"Predicted"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      Predicted
Actual PG01 PG02 PG03 PG04 PG05 PG06 PG07 PG08 PG09 PG10
  PG01    0    2    0    0    0    0    0    0    0    0
  PG02    0    5    0    0    0    0    0    0    0    0
  PG03    0    0    6    0    1    0    0    0    0    0
  PG04    0    1    1    5    0    0    0    0    0    0
  PG05    0    0    0    3   12    0    0    0    0    0
  PG06    0    0    0    0    3    4    0    0    0    0
  PG07    0    0    0    0    1    0    3    0    0    0
  PG08    0    0    0    0    0    0    0    7    0    0
  PG09    0    0    0    0    0    0    0    3    3    0
  PG10    0    0    0    0    0    0    0    0    0    6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Generate the confusion matrix showing proportions and misclassification error in the last column. Misclassification error, represents how often is the prediction wrong,}

\NormalTok{pcme }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(actual, cl)}
\NormalTok{\{}
\NormalTok{  x }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(actual, cl)}
\NormalTok{  nc }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(x)}
\NormalTok{  tbl }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(x}\SpecialCharTok{/}\FunctionTok{length}\NormalTok{(actual),}
               \AttributeTok{Error=}\FunctionTok{sapply}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{nc,}
                 \ControlFlowTok{function}\NormalTok{(r) }\FunctionTok{round}\NormalTok{(}\FunctionTok{sum}\NormalTok{(x[r,}\SpecialCharTok{{-}}\NormalTok{r])}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(x[r,]), }\DecValTok{2}\NormalTok{)))}
  \FunctionTok{names}\NormalTok{(}\FunctionTok{attr}\NormalTok{(tbl, }\StringTok{"dimnames"}\NormalTok{)) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Actual"}\NormalTok{, }\StringTok{"Predicted"}\NormalTok{)}
  \FunctionTok{return}\NormalTok{(tbl)}
\NormalTok{\}}
\NormalTok{per }\OtherTok{\textless{}{-}} \FunctionTok{pcme}\NormalTok{(MYdataset[,}\FunctionTok{c}\NormalTok{(MYinput, MYtarget)]}\SpecialCharTok{$}\NormalTok{PG, MYpr)}
\FunctionTok{round}\NormalTok{(per, }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      Predicted
Actual PG01 PG02 PG03 PG04 PG05 PG06 PG07 PG08 PG09 PG10 Error
  PG01    0 0.03 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  1.00
  PG02    0 0.08 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.00
  PG03    0 0.00 0.09 0.00 0.02 0.00 0.00 0.00 0.00 0.00  0.14
  PG04    0 0.02 0.02 0.08 0.00 0.00 0.00 0.00 0.00 0.00  0.29
  PG05    0 0.00 0.00 0.05 0.18 0.00 0.00 0.00 0.00 0.00  0.20
  PG06    0 0.00 0.00 0.00 0.05 0.06 0.00 0.00 0.00 0.00  0.43
  PG07    0 0.00 0.00 0.00 0.02 0.00 0.05 0.00 0.00 0.00  0.25
  PG08    0 0.00 0.00 0.00 0.00 0.00 0.00 0.11 0.00 0.00  0.00
  PG09    0 0.00 0.00 0.00 0.00 0.00 0.00 0.05 0.05 0.00  0.50
  PG10    0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.09  0.00
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# First we calculate the overall miscalculation rate (also known as error rate or percentage error).}
\CommentTok{\#Please note that diag(per) extracts the diagonal of confusion matrix.}

\FunctionTok{cat}\NormalTok{(}\DecValTok{100}\SpecialCharTok{*}\FunctionTok{round}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\FunctionTok{sum}\NormalTok{(}\FunctionTok{diag}\NormalTok{(per), }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{), }\DecValTok{2}\NormalTok{)) }\CommentTok{\# 23\%}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
23
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate the averaged miscalculation rate for each job classification. }
\CommentTok{\# per[,"Error"] extracts the last column, which represents the miscalculation rate per.}

\FunctionTok{cat}\NormalTok{(}\DecValTok{100}\SpecialCharTok{*}\FunctionTok{round}\NormalTok{(}\FunctionTok{mean}\NormalTok{(per[,}\StringTok{"Error"}\NormalTok{], }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{), }\DecValTok{2}\NormalTok{))  }\CommentTok{\# 28\%}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
28
\end{verbatim}

\#\#\#\#Random Forest

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Generate the Confusion Matrix for the Random Forest model.}

\CommentTok{\# Obtain the response from the Random Forest model.}

\NormalTok{MYpr }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(MYrf, }\AttributeTok{newdata=}\FunctionTok{na.omit}\NormalTok{(MYdataset[,}\FunctionTok{c}\NormalTok{(MYinput, MYtarget)]))}

\CommentTok{\# Generate the confusion matrix showing counts.}

\FunctionTok{table}\NormalTok{(}\FunctionTok{na.omit}\NormalTok{(MYdataset[,}\FunctionTok{c}\NormalTok{(MYinput, MYtarget)])}\SpecialCharTok{$}\NormalTok{PG, MYpr,}
        \AttributeTok{dnn=}\FunctionTok{c}\NormalTok{(}\StringTok{"Actual"}\NormalTok{, }\StringTok{"Predicted"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      Predicted
Actual PG01 PG02 PG03 PG04 PG05 PG06 PG07 PG08 PG09 PG10
  PG01    1    1    0    0    0    0    0    0    0    0
  PG02    0    5    0    0    0    0    0    0    0    0
  PG03    0    0    7    0    0    0    0    0    0    0
  PG04    0    0    0    7    0    0    0    0    0    0
  PG05    0    0    0    0   15    0    0    0    0    0
  PG06    0    0    0    0    0    7    0    0    0    0
  PG07    0    0    0    0    0    1    3    0    0    0
  PG08    0    0    0    0    0    0    0    7    0    0
  PG09    0    0    0    0    0    0    0    1    5    0
  PG10    0    0    0    0    0    0    0    0    0    6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Generate the confusion matrix showing proportions.}

\NormalTok{pcme }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(actual, cl)}
\NormalTok{\{}
\NormalTok{  x }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(actual, cl)}
\NormalTok{  nc }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(x)}
\NormalTok{  tbl }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(x}\SpecialCharTok{/}\FunctionTok{length}\NormalTok{(actual),}
               \AttributeTok{Error=}\FunctionTok{sapply}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{nc,}
                 \ControlFlowTok{function}\NormalTok{(r) }\FunctionTok{round}\NormalTok{(}\FunctionTok{sum}\NormalTok{(x[r,}\SpecialCharTok{{-}}\NormalTok{r])}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(x[r,]), }\DecValTok{2}\NormalTok{)))}
  \FunctionTok{names}\NormalTok{(}\FunctionTok{attr}\NormalTok{(tbl, }\StringTok{"dimnames"}\NormalTok{)) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Actual"}\NormalTok{, }\StringTok{"Predicted"}\NormalTok{)}
  \FunctionTok{return}\NormalTok{(tbl)}
\NormalTok{\}}
\NormalTok{per }\OtherTok{\textless{}{-}} \FunctionTok{pcme}\NormalTok{(}\FunctionTok{na.omit}\NormalTok{(MYdataset[,}\FunctionTok{c}\NormalTok{(MYinput, MYtarget)])}\SpecialCharTok{$}\NormalTok{PG, MYpr)}
\FunctionTok{round}\NormalTok{(per, }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      Predicted
Actual PG01 PG02 PG03 PG04 PG05 PG06 PG07 PG08 PG09 PG10 Error
  PG01 0.02 0.02 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.50
  PG02 0.00 0.08 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.00
  PG03 0.00 0.00 0.11 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.00
  PG04 0.00 0.00 0.00 0.11 0.00 0.00 0.00 0.00 0.00 0.00  0.00
  PG05 0.00 0.00 0.00 0.00 0.23 0.00 0.00 0.00 0.00 0.00  0.00
  PG06 0.00 0.00 0.00 0.00 0.00 0.11 0.00 0.00 0.00 0.00  0.00
  PG07 0.00 0.00 0.00 0.00 0.00 0.02 0.05 0.00 0.00 0.00  0.25
  PG08 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.11 0.00 0.00  0.00
  PG09 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.02 0.08 0.00  0.17
  PG10 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.09  0.00
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate the overall error percentage.}

\FunctionTok{cat}\NormalTok{(}\DecValTok{100}\SpecialCharTok{*}\FunctionTok{round}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\FunctionTok{sum}\NormalTok{(}\FunctionTok{diag}\NormalTok{(per), }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{), }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate the averaged class error percentage.}

\FunctionTok{cat}\NormalTok{(}\DecValTok{100}\SpecialCharTok{*}\FunctionTok{round}\NormalTok{(}\FunctionTok{mean}\NormalTok{(per[,}\StringTok{"Error"}\NormalTok{], }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{), }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
9
\end{verbatim}

\#\#\#\#Support Vector Machine

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Generate the Confusion Matrix for the SVM model.}

\CommentTok{\# Obtain the response from the SVM model.}

\NormalTok{MYpr }\OtherTok{\textless{}{-}}\NormalTok{ kernlab}\SpecialCharTok{::}\FunctionTok{predict}\NormalTok{(MYksvm, }\AttributeTok{newdata=}\FunctionTok{na.omit}\NormalTok{(MYdataset[,}\FunctionTok{c}\NormalTok{(MYinput, MYtarget)]))}

\CommentTok{\# Generate the confusion matrix showing counts.}

\FunctionTok{table}\NormalTok{(}\FunctionTok{na.omit}\NormalTok{(MYdataset[,}\FunctionTok{c}\NormalTok{(MYinput, MYtarget)])}\SpecialCharTok{$}\NormalTok{PG, MYpr,}
        \AttributeTok{dnn=}\FunctionTok{c}\NormalTok{(}\StringTok{"Actual"}\NormalTok{, }\StringTok{"Predicted"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      Predicted
Actual PG01 PG02 PG03 PG04 PG05 PG06 PG07 PG08 PG09 PG10
  PG01    0    1    1    0    0    0    0    0    0    0
  PG02    0    4    1    0    0    0    0    0    0    0
  PG03    0    0    5    0    2    0    0    0    0    0
  PG04    0    0    0    4    3    0    0    0    0    0
  PG05    0    0    0    1   12    2    0    0    0    0
  PG06    0    0    0    0    2    5    0    0    0    0
  PG07    0    0    0    0    0    4    0    0    0    0
  PG08    0    0    0    0    0    1    0    6    0    0
  PG09    0    0    0    0    0    0    0    4    2    0
  PG10    0    0    0    0    0    0    0    0    0    6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Generate the confusion matrix showing proportions.}

\NormalTok{pcme }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(actual, cl)}
\NormalTok{\{}
\NormalTok{  x }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(actual, cl)}
\NormalTok{  nc }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(x)}
\NormalTok{  tbl }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(x}\SpecialCharTok{/}\FunctionTok{length}\NormalTok{(actual),}
               \AttributeTok{Error=}\FunctionTok{sapply}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{nc,}
                 \ControlFlowTok{function}\NormalTok{(r) }\FunctionTok{round}\NormalTok{(}\FunctionTok{sum}\NormalTok{(x[r,}\SpecialCharTok{{-}}\NormalTok{r])}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(x[r,]), }\DecValTok{2}\NormalTok{)))}
  \FunctionTok{names}\NormalTok{(}\FunctionTok{attr}\NormalTok{(tbl, }\StringTok{"dimnames"}\NormalTok{)) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Actual"}\NormalTok{, }\StringTok{"Predicted"}\NormalTok{)}
  \FunctionTok{return}\NormalTok{(tbl)}
\NormalTok{\}}
\NormalTok{per }\OtherTok{\textless{}{-}} \FunctionTok{pcme}\NormalTok{(}\FunctionTok{na.omit}\NormalTok{(MYdataset[,}\FunctionTok{c}\NormalTok{(MYinput, MYtarget)])}\SpecialCharTok{$}\NormalTok{PG, MYpr)}
\FunctionTok{round}\NormalTok{(per, }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      Predicted
Actual PG01 PG02 PG03 PG04 PG05 PG06 PG07 PG08 PG09 PG10 Error
  PG01    0 0.02 0.02 0.00 0.00 0.00    0 0.00 0.00 0.00  1.00
  PG02    0 0.06 0.02 0.00 0.00 0.00    0 0.00 0.00 0.00  0.20
  PG03    0 0.00 0.08 0.00 0.03 0.00    0 0.00 0.00 0.00  0.29
  PG04    0 0.00 0.00 0.06 0.05 0.00    0 0.00 0.00 0.00  0.43
  PG05    0 0.00 0.00 0.02 0.18 0.03    0 0.00 0.00 0.00  0.20
  PG06    0 0.00 0.00 0.00 0.03 0.08    0 0.00 0.00 0.00  0.29
  PG07    0 0.00 0.00 0.00 0.00 0.06    0 0.00 0.00 0.00  1.00
  PG08    0 0.00 0.00 0.00 0.00 0.02    0 0.09 0.00 0.00  0.14
  PG09    0 0.00 0.00 0.00 0.00 0.00    0 0.06 0.03 0.00  0.67
  PG10    0 0.00 0.00 0.00 0.00 0.00    0 0.00 0.00 0.09  0.00
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate the overall error percentage.}

\FunctionTok{cat}\NormalTok{(}\DecValTok{100}\SpecialCharTok{*}\FunctionTok{round}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\FunctionTok{sum}\NormalTok{(}\FunctionTok{diag}\NormalTok{(per), }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{), }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
33
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate the averaged class error percentage.}

\FunctionTok{cat}\NormalTok{(}\DecValTok{100}\SpecialCharTok{*}\FunctionTok{round}\NormalTok{(}\FunctionTok{mean}\NormalTok{(per[,}\StringTok{"Error"}\NormalTok{], }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{), }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
42
\end{verbatim}

\#\#\#\#Linear regression model

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Generate the confusion matrix for the linear regression model.}

\CommentTok{\# Obtain the response from the Linear model.}

\NormalTok{MYpr }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(MYglm, }\AttributeTok{newdata=}\NormalTok{MYdataset[,}\FunctionTok{c}\NormalTok{(MYinput, MYtarget)])}

\CommentTok{\# Generate the confusion matrix showing counts.}

\FunctionTok{table}\NormalTok{(MYdataset[,}\FunctionTok{c}\NormalTok{(MYinput, MYtarget)]}\SpecialCharTok{$}\NormalTok{PG, MYpr,}
        \AttributeTok{dnn=}\FunctionTok{c}\NormalTok{(}\StringTok{"Actual"}\NormalTok{, }\StringTok{"Predicted"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      Predicted
Actual PG01 PG02 PG03 PG04 PG05 PG06 PG07 PG08 PG09 PG10
  PG01    1    1    0    0    0    0    0    0    0    0
  PG02    0    5    0    0    0    0    0    0    0    0
  PG03    0    0    7    0    0    0    0    0    0    0
  PG04    0    0    0    7    0    0    0    0    0    0
  PG05    0    0    0    0   15    0    0    0    0    0
  PG06    0    0    0    0    0    6    1    0    0    0
  PG07    0    0    0    0    0    2    2    0    0    0
  PG08    0    0    0    0    0    0    0    7    0    0
  PG09    0    0    0    0    0    0    0    0    6    0
  PG10    0    0    0    0    0    0    0    0    0    6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Generate the confusion matrix showing proportions.}

\NormalTok{pcme }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(actual, cl)}
\NormalTok{\{}
\NormalTok{  x }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(actual, cl)}
\NormalTok{  nc }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(x)}
\NormalTok{  tbl }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(x}\SpecialCharTok{/}\FunctionTok{length}\NormalTok{(actual),}
               \AttributeTok{Error=}\FunctionTok{sapply}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{nc,}
                 \ControlFlowTok{function}\NormalTok{(r) }\FunctionTok{round}\NormalTok{(}\FunctionTok{sum}\NormalTok{(x[r,}\SpecialCharTok{{-}}\NormalTok{r])}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(x[r,]), }\DecValTok{2}\NormalTok{)))}
  \FunctionTok{names}\NormalTok{(}\FunctionTok{attr}\NormalTok{(tbl, }\StringTok{"dimnames"}\NormalTok{)) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Actual"}\NormalTok{, }\StringTok{"Predicted"}\NormalTok{)}
  \FunctionTok{return}\NormalTok{(tbl)}
\NormalTok{\}}
\NormalTok{per }\OtherTok{\textless{}{-}} \FunctionTok{pcme}\NormalTok{(MYdataset[,}\FunctionTok{c}\NormalTok{(MYinput, MYtarget)]}\SpecialCharTok{$}\NormalTok{PG, MYpr)}
\FunctionTok{round}\NormalTok{(per, }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      Predicted
Actual PG01 PG02 PG03 PG04 PG05 PG06 PG07 PG08 PG09 PG10 Error
  PG01 0.02 0.02 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.50
  PG02 0.00 0.08 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.00
  PG03 0.00 0.00 0.11 0.00 0.00 0.00 0.00 0.00 0.00 0.00  0.00
  PG04 0.00 0.00 0.00 0.11 0.00 0.00 0.00 0.00 0.00 0.00  0.00
  PG05 0.00 0.00 0.00 0.00 0.23 0.00 0.00 0.00 0.00 0.00  0.00
  PG06 0.00 0.00 0.00 0.00 0.00 0.09 0.02 0.00 0.00 0.00  0.14
  PG07 0.00 0.00 0.00 0.00 0.00 0.03 0.03 0.00 0.00 0.00  0.50
  PG08 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.11 0.00 0.00  0.00
  PG09 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.09 0.00  0.00
  PG10 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.09  0.00
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate the overall error percentage.}

\FunctionTok{cat}\NormalTok{(}\DecValTok{100}\SpecialCharTok{*}\FunctionTok{round}\NormalTok{(}\DecValTok{1}\SpecialCharTok{{-}}\FunctionTok{sum}\NormalTok{(}\FunctionTok{diag}\NormalTok{(per), }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{), }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate the averaged class error percentage.}

\FunctionTok{cat}\NormalTok{(}\DecValTok{100}\SpecialCharTok{*}\FunctionTok{round}\NormalTok{(}\FunctionTok{mean}\NormalTok{(per[,}\StringTok{"Error"}\NormalTok{], }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{), }\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
11
\end{verbatim}

\hypertarget{final-evaluation-of-the-various-models}{%
\subsection{Final evaluation of the various models}\label{final-evaluation-of-the-various-models}}

It turns out that:

\begin{itemize}
\tightlist
\item
  The model that performed best was Random Forests at 2\% error
\item
  The linear model was next at 6\% error.\\
\item
  Support Vector Machines performed less well at 18\% error.
\item
  Decision trees, while being able to give us a `visual' representation of what rules are being used, performed worst of all at 23\% error.
\end{itemize}

While the above results were found on just the job classification specs, it would be wise to have a much larger population before deciding which model to deploy in real life.

Another observation: You noticed in the results of the various models, that some model had predictions that were one or two paygrades off `higher or lower' than the actual existing paygrade.

In a practical sense, this might mean:

\begin{itemize}
\tightlist
\item
  these might be candidates for determining whether criteria/features for these pay grades should be redefined
\item
  and or whether there are, in reality, fewer categories needed.
\end{itemize}

We could extend our analysis and modelling to `cluster' analysis. This would create a newer grouping based on the existing characteristics, and then the classification algorithms could be rerun to see if there was any improvement.

Some articles on People Analytics suggest that on a `maturity level' basis, the step/stage beyond prediction is `experimental design'. If we are using our results to modify our design of our systems to predict better, that might be an example of this.

\#\#5.Deploy the model

The easiest way to deploying our model, is to run your unknown data with the model.

Here, is the unclassified dataset:

\url{https://onedrive.live.com/redir?resid=4EF2CCBEDB98D0F5!6478\&authkey=!ALYidIIpaCrfnf4\&ithint=file\%2ccsv}

Put the data in a separate dataset and run the following R commands:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{DeployDataset }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"https://hranalytics.netlify.com/data/Deploydata.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{DeployDataset}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 7
  EducationLevel Experience OrgImpact ProblemSolving Supervision
           <dbl>      <dbl>     <dbl>          <dbl>       <dbl>
1              2          0         3              4           1
# ... with 2 more variables: ContactLevel <dbl>, FinancialBudget <dbl>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PredictedJobGrade }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(MYrf, }\AttributeTok{newdata=}\NormalTok{DeployDataset)}
\NormalTok{PredictedJobGrade}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   1 
PG05 
Levels: PG01 PG02 PG03 PG04 PG05 PG06 PG07 PG08 PG09 PG10
\end{verbatim}

The DeployDataset represents the information coded from a single job description (paygrade not known). PredictedJobGrade compares the coded values against the MYrf (random forest model) and the prediction is determined. In this case, the job description predicts PG05.

\hypertarget{masking-data}{%
\chapter{Masking HR data}\label{masking-data}}

In the toolbox of every HR Analytics expert, the ability to quickly anonymize data for test and development environments finds an important place. Surely you want to be able to protect confidential data from inappropriate views.

Ensure all needed libraries are installed

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(randNames)}
\FunctionTok{library}\NormalTok{(smoothmest)}
\end{Highlighting}
\end{Shaded}

\hypertarget{whitehouse-dataset}{%
\subsection{Whitehouse dataset}\label{whitehouse-dataset}}

Load a data set with first and last names and let us preview the dataset

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{whitehouse }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"https://hranalytics.netlify.com/data/2016{-}Report{-}White{-}House{-}Staff.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let us replace original names with fake names

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Create a set of new names of equal length as the original dataset}

\NormalTok{fake }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(whitehouse) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rand\_names}\NormalTok{ (}\AttributeTok{nat=}\StringTok{"US"}\NormalTok{)}

\CommentTok{\#Function to capitalise the first letter}
\NormalTok{firstup }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{}
  \FunctionTok{substr}\NormalTok{(x, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{) }\OtherTok{\textless{}{-}} \FunctionTok{toupper}\NormalTok{(}\FunctionTok{substr}\NormalTok{(x, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\NormalTok{  x}
\NormalTok{\}}

\CommentTok{\#Create a dataframe without the original names columns}
\NormalTok{whitehouse\_with\_no\_names }\OtherTok{\textless{}{-}}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(whitehouse, }\SpecialCharTok{{-}}\NormalTok{Name)}

\CommentTok{\#Create a column of fake last and first names}
\NormalTok{fake}\SpecialCharTok{$}\NormalTok{newnames }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\FunctionTok{firstup}\NormalTok{(fake}\SpecialCharTok{$}\NormalTok{name.last), }\StringTok{", "}\NormalTok{, }\FunctionTok{firstup}\NormalTok{(fake}\SpecialCharTok{$}\NormalTok{name.first))}

\CommentTok{\#Create a column of fake last and first names of exactly the length of the data frame without the original names}
\NormalTok{result }\OtherTok{\textless{}{-}}\NormalTok{ fake[}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(whitehouse\_with\_no\_names),]}

\CommentTok{\#Bind the dataset without names with the new dataset containing fake names}
\NormalTok{whitehouse\_masked }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(result}\SpecialCharTok{$}\NormalTok{newnames, whitehouse\_with\_no\_names)}

\CommentTok{\#Rename the column title to "Name"}
\FunctionTok{colnames}\NormalTok{(whitehouse\_masked)[}\DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"Name"}
\end{Highlighting}
\end{Shaded}

Let us replace original names with random numbers

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Set seed for reproducibility of the random numbers}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}

\CommentTok{\# Replace names with random numbers from 1 to 1000}
\NormalTok{whitehouse\_no\_names }\OtherTok{\textless{}{-}}\NormalTok{ whitehouse\_masked }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Name =} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{1000}\NormalTok{, }\FunctionTok{nrow}\NormalTok{(whitehouse\_masked)))}
\end{Highlighting}
\end{Shaded}

Let us anonymise the salary data

We can use four different methods to anonymise data:
* Rounding Salary to the nearest ten thousand
* Top coding consists of bringing data to an upper limit.
* Bottom coding consists of bringing data to a lower limit.

In data analysis exists various types of transformations, reciprocal, logarithm, cube root, square root and square, however in the following we will demonstrate only the square root transformation.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Rounding Salary to the nearest ten thousand}
\NormalTok{whitehouse\_no\_identifiers }\OtherTok{\textless{}{-}}\NormalTok{ whitehouse\_no\_names }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Salary =} \FunctionTok{round}\NormalTok{(Salary, }\AttributeTok{digits =} \SpecialCharTok{{-}}\DecValTok{4}\NormalTok{))}

\CommentTok{\# Top coding convert the salaries into three categories}
\NormalTok{whitehouse.gen }\OtherTok{\textless{}{-}}\NormalTok{ whitehouse\_masked }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Salary =} \FunctionTok{ifelse}\NormalTok{(Salary }\SpecialCharTok{\textless{}} \DecValTok{50000}\NormalTok{, }\DecValTok{0}\NormalTok{, }
                         \FunctionTok{ifelse}\NormalTok{(Salary }\SpecialCharTok{\textgreater{}=} \DecValTok{50000} \SpecialCharTok{\&}\NormalTok{ Salary }\SpecialCharTok{\textless{}} \DecValTok{100000}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{)))}

\CommentTok{\# Bottom Coding}
\NormalTok{whitehouse.bottom }\OtherTok{\textless{}{-}}\NormalTok{ whitehouse\_masked }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Salary =} \FunctionTok{ifelse}\NormalTok{(Salary}\SpecialCharTok{\textless{}=}\DecValTok{45000}\NormalTok{, }\DecValTok{45000}\NormalTok{, Salary))}
\end{Highlighting}
\end{Shaded}

\hypertarget{fertility-dataset}{%
\subsection{Fertility dataset}\label{fertility-dataset}}

Let us explore deeper anonymisation option of generating synthetic data. We will do on the basis of the fertility dataset available from UCI website:
\url{https://archive.ics.uci.edu/ml/datasets/Fertility}

\begin{figure}
\centering
\includegraphics{images/UCI_logo.jpg}
\caption{UCI logo}
\end{figure}

It needs to be loaded first and let us preview it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fertility }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"https://https://hranalytics.netlify.com/data/fertility\_Diagnosis.txt"}\NormalTok{, }\AttributeTok{col\_names =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Let us assign significant column titles}
\FunctionTok{colnames}\NormalTok{(fertility) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Season"}\NormalTok{, }\StringTok{"Age"}\NormalTok{, }\StringTok{"Child\_Disease"}\NormalTok{, }\StringTok{"Accident\_Trauma"}\NormalTok{, }\StringTok{"Surgical\_Intervention"}\NormalTok{,}\StringTok{"High\_Fevers"}\NormalTok{, }\StringTok{"Alcohol\_Consumption"}\NormalTok{,}\StringTok{"Smoking\_Habit"}\NormalTok{,}\StringTok{"Hours\_Sitting"}\NormalTok{,}\StringTok{"Diagnosis"}\NormalTok{)}

\CommentTok{\# View fertility data}
\NormalTok{fertility}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 100 x 10
   Season     Age Child_Disease Accident_Trauma Surgical_Interv~
    <dbl>   <dbl>         <dbl>           <dbl>            <dbl>
 1  -0.33 0.69000             0               1                1
 2  -0.33 0.94                1               0                1
 3  -0.33 0.5                 1               0                0
 4  -0.33 0.75                0               1                1
 5  -0.33 0.67                1               1                0
 6  -0.33 0.67                1               0                1
 7  -0.33 0.67                0               0                0
 8  -0.33 1                   1               1                1
 9   1    0.64                0               0                1
10   1    0.61                1               0                0
# ... with 90 more rows, and 5 more variables: High_Fevers <dbl>,
#   Alcohol_Consumption <dbl>, Smoking_Habit <dbl>, Hours_Sitting <dbl>,
#   Diagnosis <chr>
\end{verbatim}

Let us examine the dataset.

More on the data attributes is availalble here:
\url{https://archive.ics.uci.edu/ml/datasets/Fertility}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Number of participants with Surgical\_Intervention and Diagnosis}
\NormalTok{fertility }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(Diagnosis) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise\_at}\NormalTok{(}\FunctionTok{vars}\NormalTok{(Surgical\_Intervention), sum)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 2
  Diagnosis Surgical_Intervention
  <chr>                     <dbl>
1 N                            44
2 O                             7
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Number of participants with Surgical\_Intervention and Diagnosis}
\NormalTok{fertility }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise\_at}\NormalTok{(}\FunctionTok{vars}\NormalTok{(Age), }\FunctionTok{funs}\NormalTok{(mean, sd))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 2
   mean       sd
  <dbl>    <dbl>
1 0.669 0.121319
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Counts of the Groups in High\_Fevers}
\NormalTok{fertility }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(High\_Fevers)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 2
  High_Fevers     n
        <dbl> <int>
1          -1     9
2           0    63
3           1    28
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Counts of the Groups in Child\_Disease and Accident\_Trauma }
\NormalTok{fertility }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(Child\_Disease,Accident\_Trauma)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 3
  Child_Disease Accident_Trauma     n
          <dbl>           <dbl> <int>
1             0               0    10
2             0               1     3
3             1               0    46
4             1               1    41
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate the average of Child\_Disease}
\NormalTok{fertility }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise\_at}\NormalTok{(}\FunctionTok{vars}\NormalTok{(Child\_Disease), mean)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 1
  Child_Disease
          <dbl>
1          0.87
\end{verbatim}

In the following we will generate synthetic data sampling from a normal distribution through the subsequent steps:
1. First create a new dataset called ``fert'', after applying a log transformation on the hours sitting variable.
2. Calculate the average and the standard deviation
3. Set a seed for reproducibility
4. Generate new data normally distributed for the hours sitting variable
5. Retransform back the log variable using exponential.
6. Hard bound data not falling in the right range
7. Recheck the range.
8. Substitute the synthtic data back into the initial fert dataset.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fert }\OtherTok{\textless{}{-}}\NormalTok{ fertility }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Hours\_Sitting =} \FunctionTok{log}\NormalTok{(Hours\_Sitting))}

\NormalTok{fert }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{summarise\_at}\NormalTok{(}\FunctionTok{vars}\NormalTok{(Hours\_Sitting), }\FunctionTok{funs}\NormalTok{(mean, sd))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 2
      mean       sd
     <dbl>    <dbl>
1 -1.01224 0.504779
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}

\NormalTok{hours.sit }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\SpecialCharTok{{-}}\FloatTok{1.01}\NormalTok{, }\FloatTok{0.50}\NormalTok{)}

\NormalTok{hours.sit }\OtherTok{\textless{}{-}} \FunctionTok{exp}\NormalTok{(hours.sit)}

\NormalTok{hours.sit[hours.sit }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{hours.sit[hours.sit }\SpecialCharTok{\textgreater{}} \DecValTok{1}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{1}

\FunctionTok{range}\NormalTok{(hours.sit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.0815 1.0000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fert}\SpecialCharTok{$}\NormalTok{Hours\_Sitting }\OtherTok{\textless{}{-}}\NormalTok{ hours.sit}
\end{Highlighting}
\end{Shaded}

In data analysis exists various types of transformations, reciprocal, logarithm, cube root, square root and square, however in the following we will demonstrate only the square root transformation.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Square root Transformation of Salary}
\NormalTok{whitehouse.salary }\OtherTok{\textless{}{-}}\NormalTok{ whitehouse\_masked }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Salary =} \FunctionTok{sqrt}\NormalTok{(Salary))}

\CommentTok{\# Calculate the mean and standard deviation}
\NormalTok{stats }\OtherTok{\textless{}{-}}\NormalTok{ whitehouse.salary }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise\_at}\NormalTok{(}\FunctionTok{vars}\NormalTok{(Salary), }\FunctionTok{funs}\NormalTok{(mean, sd))}

\CommentTok{\# Generate Synthetic data with the same mean and standard deviation}
\NormalTok{salary\_transformed }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(whitehouse\_masked), }\FunctionTok{mean}\NormalTok{(whitehouse.salary}\SpecialCharTok{$}\NormalTok{Salary), }\FunctionTok{sd}\NormalTok{(whitehouse.salary}\SpecialCharTok{$}\NormalTok{Salary))}

\CommentTok{\# Power transformation}
\NormalTok{salary\_original }\OtherTok{\textless{}{-}}\NormalTok{ salary\_transformed}\SpecialCharTok{\^{}}\DecValTok{2}

\CommentTok{\# Hard bound}
\NormalTok{salary }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(salary\_original }\SpecialCharTok{\textless{}} \DecValTok{0}\NormalTok{, }\DecValTok{0}\NormalTok{, salary\_original)}
\end{Highlighting}
\end{Shaded}

Let us introduce now the concept of differential privacy, a mathematical concept used by big names like Google, Census Bureau and Apple.
Why Differential Privacy? It quantifies the privacy loss via a privacy budget, called epsilon. It assumes the worst-case scenario about the data intruder. Smaller privacy budget means less information or a noiser answer, however epsilon cannot be zero or lower.

Global Sensitivity of Other Queries
n is total number of observations
a is the lower bound of the data
b is the upper bound of the data
Counting: 1
Proportion: 1 / n
Mean: (b - a) / n

small global sensitivity results in less noise
large global sensitivity results in more noise

Number of observations
n \textless- nrow(fertility)

Global sensitivity of counts
gs.count \textless- 1

Global sensitivity of proportions
gs.prop \textless- 1/n

Lower bound
a \textless- 0

Upper bound
b \textless- 1

Global sensitivity of mean
gs.mean \textless- (b - a) / n

Global sensitivity of proportions
gs.var \textless- (b - a)\^{}2 / n

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fertility }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{summarise\_at}\NormalTok{(}\FunctionTok{vars}\NormalTok{(Child\_Disease), sum)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 1
  Child_Disease
          <dbl>
1            87
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(smoothmest)}

\CommentTok{\#rdoublex(draws, mean, shaping) }

\CommentTok{\#rdoublex is a random number generator. It creates a vector of random numbers generated by the double exponential distribution.}
\end{Highlighting}
\end{Shaded}

The double exponential distribution is also commonly referred to as the Laplace distribution. The following is the plot of the double exponential probability density function.

\begin{figure}
\centering
\includegraphics{images/dexpdf.jpg}
\caption{plot of double exponential probability density function}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}

\FunctionTok{rdoublex}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{87}\NormalTok{, }\DecValTok{1} \SpecialCharTok{/} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 87
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#[1] 87.01983}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)    }
\FunctionTok{rdoublex}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{87}\NormalTok{, }\DecValTok{1} \SpecialCharTok{/} \FloatTok{0.1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 85
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#[1] 88.98337}
\end{Highlighting}
\end{Shaded}

Sequential Composition

Suppose a set of privacy mechanisms M are sequentially performed on a dataset, and each M provides the max epsilon privacy guarantee.

The sequential composition undertakes the privacy guarantee for a sequence of differentially private computations. When a set of randomized mechanisms has been performed sequentially on a dataset, the final privacy guarantee is determined by the summation of total privacy budgets.

The privacy budget must be divided by two.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Set Value of Epsilon}
\NormalTok{eps }\OtherTok{\textless{}{-}} \FloatTok{0.1} \SpecialCharTok{/} \DecValTok{2}

\CommentTok{\# GS of Mean and Variance}
\NormalTok{gs.mean }\OtherTok{\textless{}{-}} \FloatTok{0.01}
\NormalTok{gs.var }\OtherTok{\textless{}{-}} \FloatTok{0.01}

\CommentTok{\# Apply the Laplace mechanism}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}
\FunctionTok{rdoublex}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.41}\NormalTok{, gs.mean }\SpecialCharTok{/}\NormalTok{ eps)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.37
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#[1] 0.4496674}
\FunctionTok{rdoublex}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.19}\NormalTok{, gs.var }\SpecialCharTok{/}\NormalTok{ eps)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.247
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#[1] 0.2466982}
\end{Highlighting}
\end{Shaded}

Parallel Composition

Suppose a set of privacy mechanisms M are sequentially performed on a dataset, and each M provides the sum epsilon privacy guarantee.

The sequential composition undertakes the privacy guarantee for a sequence of differentially private computations. When a set of randomized mechanisms has been performed sequentially on a dataset, the final privacy guarantee is determined by the summation of total privacy budgets.

The privacy budget does not need to be divided.
The query with the most epsilon is the budget for the data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#High\_Fevers and Mean of Hours\_Sitting}

\NormalTok{fertility }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(High\_Fevers }\SpecialCharTok{\textgreater{}=} \DecValTok{0}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise\_at}\NormalTok{(}\FunctionTok{vars}\NormalTok{(Hours\_Sitting), mean)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 1
  Hours_Sitting
          <dbl>
1      0.393297
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Hours\_Sitting 0.3932967}

\CommentTok{\# No High\_Fevers and Mean of Hours\_Sitting}
\NormalTok{fertility }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(High\_Fevers }\SpecialCharTok{==} \SpecialCharTok{{-}}\DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise\_at}\NormalTok{(}\FunctionTok{vars}\NormalTok{(Hours\_Sitting), mean)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 1
  Hours_Sitting
          <dbl>
1      0.543333
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Hours\_Sitting 0.5433333}

\CommentTok{\#Set Value of Epsilon}
\NormalTok{eps }\OtherTok{\textless{}{-}} \FloatTok{0.1}

\CommentTok{\# GS of mean for Hours\_Sitting}
\NormalTok{gs.mean }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{/} \DecValTok{100}

\CommentTok{\# Apply the Laplace mechanism}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}
\FunctionTok{rdoublex}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.39}\NormalTok{, gs.mean }\SpecialCharTok{/}\NormalTok{ eps)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.37
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#[1] 0.4098337}
\FunctionTok{rdoublex}\NormalTok{(}\DecValTok{1}\NormalTok{, }\FloatTok{0.54}\NormalTok{, gs.mean }\SpecialCharTok{/}\NormalTok{ eps)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.568
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#[1] 0.5683491}
\end{Highlighting}
\end{Shaded}

Prepping up data

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Set Value of Epsilon}
\NormalTok{eps }\OtherTok{\textless{}{-}} \FloatTok{0.01}
\CommentTok{\# GS of counts}
\NormalTok{gs.count }\OtherTok{\textless{}{-}} \DecValTok{1}

\NormalTok{fertility }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(Smoking\_Habit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 2
  Smoking_Habit     n
          <dbl> <int>
1            -1    56
2             0    23
3             1    21
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Smoking Count}
\CommentTok{\#{-}1      56}
\CommentTok{\# 0      23}
\CommentTok{\# 1     21}

\CommentTok{\#Apply the Laplace mechanism}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}

\NormalTok{smoking1 }\OtherTok{\textless{}{-}} \FunctionTok{rdoublex}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{56}\NormalTok{, gs.count }\SpecialCharTok{/}\NormalTok{ eps }\SpecialCharTok{/} \DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{round}\NormalTok{()}

\NormalTok{smoking2 }\OtherTok{\textless{}{-}} \FunctionTok{rdoublex}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{23}\NormalTok{, gs.count }\SpecialCharTok{/}\NormalTok{ eps }\SpecialCharTok{/} \DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{round}\NormalTok{()}

\CommentTok{\# Post{-}process based on previous queries}
\NormalTok{smoking3 }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(fertility) }\SpecialCharTok{{-}}\NormalTok{ smoking1 }\SpecialCharTok{{-}}\NormalTok{ smoking2}

\CommentTok{\# Checking the noisy answers}
\NormalTok{smoking1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 46
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#[1] 60}
\NormalTok{smoking2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 37
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#[1] 29}
\NormalTok{smoking3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 17
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#[1] 11}
\end{Highlighting}
\end{Shaded}

Impossible and
Inconsistent Answers

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Set Value of Epsilon}
\NormalTok{eps }\OtherTok{\textless{}{-}} \FloatTok{0.01}
\CommentTok{\# GS of counts}
\NormalTok{gs.count }\OtherTok{\textless{}{-}} \DecValTok{1}

\CommentTok{\# Display Participants with Abnormal Diagnosis}
\NormalTok{Number\_abnormal }\OtherTok{\textless{}{-}}\NormalTok{ fertility }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(Diagnosis}\SpecialCharTok{==}\StringTok{"O"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summarise}\NormalTok{(}\AttributeTok{sum\_x1 =} \FunctionTok{sum}\NormalTok{(Diagnosis}\SpecialCharTok{==}\StringTok{"O"}\NormalTok{))}

\CommentTok{\#Negative Counts: Applying the Laplace mechanism}
\CommentTok{\# Apply the Laplace mechanism and set.seed(22)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{22}\NormalTok{)}
\FunctionTok{rdoublex}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{12}\NormalTok{, gs.count }\SpecialCharTok{/}\NormalTok{ eps) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{round}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -79
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#[1] {-}79}

\CommentTok{\# Apply the Laplace mechanism and set.seed(22)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{22}\NormalTok{)}
\FunctionTok{rdoublex}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{12}\NormalTok{, gs.count }\SpecialCharTok{/}\NormalTok{ eps) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{round}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{max}\NormalTok{(}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#[1] 0}

\CommentTok{\# Suppose we set a different seed}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{12}\NormalTok{)}
\NormalTok{noisy\_answer }\OtherTok{\textless{}{-}} \FunctionTok{rdoublex}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{12}\NormalTok{, gs.count }\SpecialCharTok{/}\NormalTok{ eps) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{round}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{max}\NormalTok{(}\DecValTok{0}\NormalTok{)}

\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(fertility)}

\CommentTok{\# ifelse example}
\FunctionTok{ifelse}\NormalTok{(noisy\_answer }\SpecialCharTok{\textgreater{}}\NormalTok{ n, n, noisy\_answer)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 100
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#[1] 100}

\CommentTok{\#Normalising}
\CommentTok{\# Set Value of Epsilon}
\NormalTok{eps }\OtherTok{\textless{}{-}} \FloatTok{0.01}
\CommentTok{\# GS of Counts}
\NormalTok{gs.count }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{fertility }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(Smoking\_Habit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 2
  Smoking_Habit     n
          <dbl> <int>
1            -1    56
2             0    23
3             1    21
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Smoking Count}
\CommentTok{\# {-}1     56}
\CommentTok{\#  0     23}
\CommentTok{\#  1     21}

\CommentTok{\# Apply the Laplace mechanism and set.seed(42)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{42}\NormalTok{)}
\NormalTok{smoking1 }\OtherTok{\textless{}{-}} \FunctionTok{rdoublex}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{56}\NormalTok{, gs.count }\SpecialCharTok{/}\NormalTok{ eps }\SpecialCharTok{/} \DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{max}\NormalTok{(}\DecValTok{0}\NormalTok{)}
\NormalTok{smoking2 }\OtherTok{\textless{}{-}} \FunctionTok{rdoublex}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{23}\NormalTok{, gs.count }\SpecialCharTok{/}\NormalTok{ eps }\SpecialCharTok{/} \DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{max}\NormalTok{(}\DecValTok{0}\NormalTok{)}
\NormalTok{smoking3 }\OtherTok{\textless{}{-}} \FunctionTok{rdoublex}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{21}\NormalTok{, gs.count }\SpecialCharTok{/}\NormalTok{ eps }\SpecialCharTok{/} \DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{max}\NormalTok{(}\DecValTok{0}\NormalTok{)}

\CommentTok{\# Checking the noisy answers}
\NormalTok{smoking }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(smoking1, smoking2, smoking3)}
\NormalTok{smoking}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 46.1 37.2 44.7
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#[1] 65.91684 37.17455 0.00000}

\CommentTok{\# Normalize smoking}
\NormalTok{normalized }\OtherTok{\textless{}{-}}\NormalTok{ (smoking}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(smoking)) }\SpecialCharTok{*}\NormalTok{ (}\FunctionTok{nrow}\NormalTok{(fertility))}

\CommentTok{\# Round the values}
\FunctionTok{round}\NormalTok{(normalized)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 36 29 35
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#[1] 64 36 0}
\end{Highlighting}
\end{Shaded}

\hypertarget{absenteeism-MFG}{%
\chapter{Absenteeism in manufacturing}\label{absenteeism-MFG}}

Employee absenteeism is a significant problem for most organizations. Absenteeism excludes paid leave and occasions where an employer has granted an employee time off.

If you're a manager, supervisor, or team leader, you'll have likely experienced cases of absenteeism. You should know that staff will be absent from time to time, such as for illness, jury duty, or bereavement. However, when absenteeism is frequent and excessive it begins to become a problem.

Absenteeism at the workplace affects both employers and employees alike.

Some consequences for employers include:
* Reduced productivity levels.
* High administration costs.
* Increased labour costs if you hire replacement temporary workers.
* Understaffing which could lead to poor customer service.
* Poor morale among colleagues. This will be particularly prevalent if employees constantly have to fill in for absent staff and if they don't see any retributions for absence.

To address this issue, HR professionals are encouraged to develop a business case and to devise and implement plans for managing employee attendance in their organizations. The following topics related to managing employee attendance are presented in this article:

The direct and indirect costs of absenteeism.
Collecting attendance data for the organization.
Measuring the cost of absenteeism.
Calculating rates of absenteeism.
Strategies aimed at addressing the organization's particular attendance issues.
Legal issues that may be implicated in designing and implementing absence control strategies.

\#\#1.Define the goal (or HR business problem/issue).

A hypothetical company MFG has decided that it needs to do something about absenteeism.
To start with, it wants answers to the following questions:

\begin{itemize}
\tightlist
\item
  \textbf{What is its rate of absenteeism?}
\item
  \textbf{Does anyone have excessive absenteeism?}
\item
  \textbf{Is is the same across the organization?}
\item
  \textbf{Does it vary by gender?}
\item
  \textbf{Does it vary by length of service or age?} Its guesses are that initially age and length of service may be related to absenteeism rates.
\item
  \textbf{Can it predict next year's absenteeism?}
\item
  \textbf{If so, how well can it predict?}
\item
  \textbf{Can we reduce our absenteeism?}
\end{itemize}

If they can make future People Management decisions ``driven'' by what the data is telling them, then they will feel they have started the People Analytics journey.

\#\#2.Collect and Manage Data.

Let us suppose this is a skunkworks project. Formal separate resources have not be identified for this initiative.
Only an initial look at recent data is possible. The HRIS system is able to provide some rudimentary information covering absences only for 2015
It was able to generate the following information as a CSV file (comma separated values):

\begin{itemize}
\tightlist
\item
  EmployeeNumber
\item
  Surname
\item
  GivenName
\item
  Gender
\item
  City
\item
  JobTitle
\item
  DepartmentName
\item
  StoreLocation
\item
  Division
\item
  Age
\item
  LengthService
\item
  AbsentHours
\item
  BusinessUnit
\end{itemize}

Ensure all needed libraries are installed

\#\#\#Let's read in the data provided

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MFGEmployees }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"https://hranalytics.netlify.com/data/MFGEmployees4.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The first thing we should do is check on quality of data. Data will rarely be clean or perfect when we receive it. Either questionnable data should be corrected (preferrred) or deleted.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(MFGEmployees)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 EmployeeNumber   Surname           GivenName            Gender         
 Min.   :   1   Length:8336        Length:8336        Length:8336       
 1st Qu.:2085   Class :character   Class :character   Class :character  
 Median :4168   Mode  :character   Mode  :character   Mode  :character  
 Mean   :4168                                                           
 3rd Qu.:6252                                                           
 Max.   :8336                                                           
     City             JobTitle         DepartmentName    
 Length:8336        Length:8336        Length:8336       
 Class :character   Class :character   Class :character  
 Mode  :character   Mode  :character   Mode  :character  
                                                         
                                                         
                                                         
 StoreLocation        Division              Age       LengthService 
 Length:8336        Length:8336        Min.   : 3.5   Min.   : 0.0  
 Class :character   Class :character   1st Qu.:35.3   1st Qu.: 3.6  
 Mode  :character   Mode  :character   Median :42.1   Median : 4.6  
                                       Mean   :42.0   Mean   : 4.8  
                                       3rd Qu.:48.7   3rd Qu.: 5.6  
                                       Max.   :77.9   Max.   :43.7  
  AbsentHours    BusinessUnit      
 Min.   :  0.0   Length:8336       
 1st Qu.: 19.1   Class :character  
 Median : 56.0   Mode  :character  
 Mean   : 61.3                     
 3rd Qu.: 94.3                     
 Max.   :272.5                     
\end{verbatim}

The only thing that stands out initially is that age has some questionable data- some one who is 3 and someone who is 77. The range for purposes of this example should be 18 to 65. Normally you would want to clean the data by getting the correct information and then changing it.

\hypertarget{for-expediency-we-will-delete-the-problem-records-in-the-dataset.}{%
\subsection{For expediency we will delete the problem records in the dataset.}\label{for-expediency-we-will-delete-the-problem-records-in-the-dataset.}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MFGEmployees}\OtherTok{\textless{}{-}}
\NormalTok{  MFGEmployees }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(Age}\SpecialCharTok{\textgreater{}=}\DecValTok{18} \SpecialCharTok{\&}\NormalTok{ Age}\SpecialCharTok{\textless{}=}\DecValTok{65}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now let's summarize again with cleaned up data

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(MFGEmployees)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 EmployeeNumber   Surname           GivenName            Gender         
 Min.   :   1   Length:8165        Length:8165        Length:8165       
 1st Qu.:2081   Class :character   Class :character   Class :character  
 Median :4166   Mode  :character   Mode  :character   Mode  :character  
 Mean   :4165                                                           
 3rd Qu.:6245                                                           
 Max.   :8336                                                           
     City             JobTitle         DepartmentName    
 Length:8165        Length:8165        Length:8165       
 Class :character   Class :character   Class :character  
 Mode  :character   Mode  :character   Mode  :character  
                                                         
                                                         
                                                         
 StoreLocation        Division              Age       LengthService 
 Length:8165        Length:8165        Min.   :18.2   Min.   : 0.1  
 Class :character   Class :character   1st Qu.:35.5   1st Qu.: 3.6  
 Mode  :character   Mode  :character   Median :42.1   Median : 4.6  
                                       Mean   :42.0   Mean   : 4.8  
                                       3rd Qu.:48.5   3rd Qu.: 5.6  
                                       Max.   :65.0   Max.   :43.7  
  AbsentHours    BusinessUnit      
 Min.   :  0.0   Length:8165       
 1st Qu.: 20.1   Class :character  
 Median : 55.9   Mode  :character  
 Mean   : 60.5                     
 3rd Qu.: 93.4                     
 Max.   :252.2                     
\end{verbatim}

\#\#\#Transform the data

Now calculate absenteeism rate by dividing the absent hours by total standard hours for the year (52 weeks/year * 40 hours/week = 2080 hours/year)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MFGEmployees }\OtherTok{\textless{}{-}} 
\NormalTok{  MFGEmployees }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{AbsenceRate =}\NormalTok{ AbsentHours }\SpecialCharTok{/}\DecValTok{2080}\SpecialCharTok{*}\DecValTok{100}\NormalTok{)}

\FunctionTok{str}\NormalTok{(MFGEmployees)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
tibble [8,165 x 14] (S3: spec_tbl_df/tbl_df/tbl/data.frame)
 $ EmployeeNumber: num [1:8165] 1 2 3 4 5 6 7 8 9 10 ...
 $ Surname       : chr [1:8165] "Gutierrez" "Hardwick" "Delgado" "Simon" ...
 $ GivenName     : chr [1:8165] "Molly" "Stephen" "Chester" "Irene" ...
 $ Gender        : chr [1:8165] "F" "M" "M" "F" ...
 $ City          : chr [1:8165] "Burnaby" "Courtenay" "Richmond" "Victoria" ...
 $ JobTitle      : chr [1:8165] "Baker" "Baker" "Baker" "Baker" ...
 $ DepartmentName: chr [1:8165] "Bakery" "Bakery" "Bakery" "Bakery" ...
 $ StoreLocation : chr [1:8165] "Burnaby" "Nanaimo" "Richmond" "Victoria" ...
 $ Division      : chr [1:8165] "Stores" "Stores" "Stores" "Stores" ...
 $ Age           : num [1:8165] 32 40.3 48.8 44.6 35.7 ...
 $ LengthService : num [1:8165] 6.02 5.53 4.39 3.08 3.62 ...
 $ AbsentHours   : num [1:8165] 36.6 30.2 83.8 70 0 ...
 $ BusinessUnit  : chr [1:8165] "Stores" "Stores" "Stores" "Stores" ...
 $ AbsenceRate   : num [1:8165] 1.76 1.45 4.03 3.37 0 ...
 - attr(*, "spec")=
  .. cols(
  ..   EmployeeNumber = col_double(),
  ..   Surname = col_character(),
  ..   GivenName = col_character(),
  ..   Gender = col_character(),
  ..   City = col_character(),
  ..   JobTitle = col_character(),
  ..   DepartmentName = col_character(),
  ..   StoreLocation = col_character(),
  ..   Division = col_character(),
  ..   Age = col_double(),
  ..   LengthService = col_double(),
  ..   AbsentHours = col_double(),
  ..   BusinessUnit = col_character()
  .. )
\end{verbatim}

We can now see our metric AbsenceRate has been calculated and created.

\#\#\#Explore The Data

Part of collecting and managing data is the Exploratory Data Analysis.

Let's start with bar graphs of some of the categorical data

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MFGEmployees }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{BusinessUnit) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{"Business Units"}\NormalTok{,}
       \AttributeTok{y=}\StringTok{"Count"}\NormalTok{,}
       \AttributeTok{title=}\StringTok{"Employee Count By Business Units"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{coord\_flip}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-611-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MFGEmployees }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Gender) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{"Business Units"}\NormalTok{,}
       \AttributeTok{y=}\StringTok{"Count"}\NormalTok{,}
       \AttributeTok{title=}\StringTok{"Employee Count By Gender"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{coord\_flip}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-611-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MFGEmployees }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Division) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{"Division"}\NormalTok{,}
       \AttributeTok{y=}\StringTok{"Count"}\NormalTok{,}
       \AttributeTok{title=}\StringTok{"Employee Count By Division"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{coord\_flip}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-611-3} \end{center}

Let's ask some of our questions answered through this exploratory analysis.

\textbf{First of all, what is our absenteeism rate?}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MFGEmployees }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{median =} \FunctionTok{median}\NormalTok{(AbsenceRate), }\AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(AbsenceRate)) }\CommentTok{\#Average absence rate and number of observations}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 2
   median    mean
    <dbl>   <dbl>
1 2.68572 2.90726
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{means }\OtherTok{\textless{}{-}} \FunctionTok{aggregate}\NormalTok{(AbsenceRate }\SpecialCharTok{\textasciitilde{}} \DecValTok{1}\NormalTok{, MFGEmployees, mean)}

\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_boxplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ AbsenceRate, }\AttributeTok{x =}\DecValTok{1}\NormalTok{), }\AttributeTok{data =}\NormalTok{ MFGEmployees) }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_text}\NormalTok{(}\AttributeTok{data =} \FunctionTok{round}\NormalTok{(means, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{), }\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ AbsenceRate, }\AttributeTok{x =}\DecValTok{1}\NormalTok{, }\AttributeTok{label =}\NormalTok{ AbsenceRate),  }\AttributeTok{check\_overlap=}\ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\#Add average absence rate as a text}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-612-1} \end{center}

The average absence rate is `r scales::percent(mean(MFGEmployees\$AbsenceRate)'.

\textbf{Does anyone have excessive absenteeism?}

The boxplot shows the average (three digit number), the mean and standard deviation (through lines) of the data. Any observations beyond 3 standard deviations shows up as dots. So at least under that definition of outliers, some people show way more absenteeism than 99\% of employees.

\textbf{Does it vary across the organization?}

When you conduct a piece of quantitative research, you are inevitably attempting to answer a research question or hypothesis that you have set. One method of evaluating this research question is via a process called hypothesis testing, which is sometimes also referred to as significance testing.

In order to undertake hypothesis testing you need to express your research hypothesis as a null and alternative hypothesis. The null hypothesis and alternative hypothesis are statements regarding the differences or effects that occur in the population. You will use your sample to test which statement (i.e., the null hypothesis or alternative hypothesis) is most likely (although technically, you test the evidence against the null hypothesis). So, with respect to our example, the null and alternative hypothesis will reflect statements about absences among the organization.

In the following we will use ANOVA, as statistical test.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_boxplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ AbsenceRate, }\AttributeTok{x =}\NormalTok{ Gender), }\AttributeTok{data =}\NormalTok{ MFGEmployees) }\SpecialCharTok{+} 
  \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-613-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AnovaModel}\FloatTok{.1} \OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(AbsenceRate }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Gender, }\AttributeTok{data=}\NormalTok{MFGEmployees) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{Anova}\NormalTok{()}

\NormalTok{AnovaModel}\FloatTok{.1} 
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Anova Table (Type II tests)

Response: AbsenceRate
          Sum Sq   Df F value              Pr(>F)    
Gender       496    1    97.8 <0.0000000000000002 ***
Residuals  41379 8163                                
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#means}
\NormalTok{MFGEmployees }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{group\_by}\NormalTok{(Gender) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summarise}\NormalTok{(}\AttributeTok{avg=}\FunctionTok{mean}\NormalTok{(AbsenceRate))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 2
  Gender     avg
  <chr>    <dbl>
1 F      3.15762
2 M      2.66481
\end{verbatim}

The output of the function is a classical ANOVA table with the following data:
Df = degree of freedom
Sum Sq = deviance (within groups, and residual)
Mean Sq = variance (within groups, and residual)
F value = the value of the Fisher statistic test, so computed (variance within groups) / (variance residual)
Pr(\textgreater F) = p-value

Please note that the value 2.2e-16 actually means 2.2 X 10 \^{} -16. It is just a way R prints numbers that are either too big or too small.

Since the p-Value is much less than 0.05, we reject the null hypothesis and accept the alternative hypothesis, i.e.~the absence levels among are significantly different among genders.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_boxplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =}\NormalTok{ AbsenceRate, }\AttributeTok{x =}\NormalTok{ Division), }\AttributeTok{data =}\NormalTok{ MFGEmployees) }\SpecialCharTok{+} 
  \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-614-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AnovaModel}\FloatTok{.2} \OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(AbsenceRate }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Division, }\AttributeTok{data=}\NormalTok{MFGEmployees) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{Anova}\NormalTok{()}

\NormalTok{AnovaModel}\FloatTok{.2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Anova Table (Type II tests)

Response: AbsenceRate
          Sum Sq   Df F value Pr(>F)   
Division      91    5    3.56 0.0032 **
Residuals  41783 8159                  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# means}
\NormalTok{MFGEmployees }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{group\_by}\NormalTok{(Division) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summarise}\NormalTok{(}\AttributeTok{avg=}\FunctionTok{mean}\NormalTok{(AbsenceRate))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 2
  Division                 avg
  <chr>                  <dbl>
1 Executive            2.32358
2 FinanceAndAccounting 1.92189
3 HumanResources       2.65174
4 InfoTech             1.92600
5 Legal                2.47172
6 Stores               2.92086
\end{verbatim}

The absence levels among are significantly different among Division.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AnovaModel}\FloatTok{.3} \OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(AbsenceRate }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Division}\SpecialCharTok{*}\NormalTok{Gender, }\AttributeTok{data=}\NormalTok{MFGEmployees) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{Anova}\NormalTok{()}

\NormalTok{AnovaModel}\FloatTok{.3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Anova Table (Type II tests)

Response: AbsenceRate
                Sum Sq   Df F value              Pr(>F)    
Division            92    5    3.61              0.0029 ** 
Gender             496    1   97.94 <0.0000000000000002 ***
Division:Gender      5    5    0.18              0.9708    
Residuals        41283 8153                                
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#means}
\NormalTok{MFGEmployees }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{group\_by}\NormalTok{(Division, Gender) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{summarise}\NormalTok{(}\AttributeTok{avg=}\FunctionTok{mean}\NormalTok{(AbsenceRate))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 12 x 3
# Groups:   Division [6]
   Division             Gender     avg
   <chr>                <chr>    <dbl>
 1 Executive            F      2.97642
 2 Executive            M      1.77955
 3 FinanceAndAccounting F      2.17280
 4 FinanceAndAccounting M      1.63408
 5 HumanResources       F      3.01449
 6 HumanResources       M      2.21431
 7 InfoTech             F      3.29811
 8 InfoTech             M      1.77354
 9 Legal                F      3.29811
10 Legal                M      2.05853
11 Stores               F      3.16905
12 Stores               M      2.68079
\end{verbatim}

If varies significantly by the interaction of gender and division.

These are just a handful of the categorical summaries we could do.

\textbf{Does AbsenceRate vary by length of service and age?}

Scatterplots and correlations help answer this.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# basic scatterplot}
\NormalTok{MFGEmployees }\SpecialCharTok{\%\textgreater{}\%}
\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Age, }\AttributeTok{y=}\NormalTok{AbsenceRate) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{colour =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-616-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(MFGEmployees}\SpecialCharTok{$}\NormalTok{Age, MFGEmployees}\SpecialCharTok{$}\NormalTok{AbsenceRate)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.825
\end{verbatim}

There is a strong correlation of Age and Absence Rate

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MFGEmployees }\SpecialCharTok{\%\textgreater{}\%}
\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{LengthService, }\AttributeTok{y=}\NormalTok{AbsenceRate) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{colour =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method=}\StringTok{\textquotesingle{}lm\textquotesingle{}}\NormalTok{, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{color=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-617-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(MFGEmployees}\SpecialCharTok{$}\NormalTok{LengthService, MFGEmployees}\SpecialCharTok{$}\NormalTok{AbsenceRate)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -0.0467
\end{verbatim}

There is not a strong correlation between length of service and Absence Rate.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MFGEmployees }\SpecialCharTok{\%\textgreater{}\%}
\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{Age, }\AttributeTok{y=}\NormalTok{LengthService) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{colour =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method=}\StringTok{\textquotesingle{}lm\textquotesingle{}}\NormalTok{, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{color=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-618-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(MFGEmployees}\SpecialCharTok{$}\NormalTok{Age, MFGEmployees}\SpecialCharTok{$}\NormalTok{LengthService)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.0562
\end{verbatim}

There is not much correlation between age and length of service either.

So far, we have done data exploration, now we are going to start building a model and starting to answer more difficult questions.

\#\#3. Build The model
One of the questions asked in the defining the goal step was `whether it was possible to \textbf{predict} absenteeism?'

Absence Rate is a numeric continuous value. In the `Building a model' step we have to chose which models/statistical algorithms to use. Prediction of a numerics continous values suggests a couple of models that could be brought to bear: Regression trees and linear regression. There are many more but for purposes of this article we will look at these.

\#\#\#3.1 Regression Trees
Regression Trees will allow for use of both categorical and numeric values as predictors. Let's choose the following data as potential predictors in this analysis:

\begin{itemize}
\tightlist
\item
  Gender
\item
  Department Name
\item
  Store Location
\item
  Division
\item
  Age
\item
  Length of Service
\item
  Business Unit
\end{itemize}

\textbf{Absence Rate} will be the the `target' or thing to be predicted.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MYinput }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Gender"}\NormalTok{, }\StringTok{"DepartmentName"}\NormalTok{, }\StringTok{"StoreLocation"}\NormalTok{, }\StringTok{"Division"}\NormalTok{,}
             \StringTok{"Age"}\NormalTok{, }\StringTok{"LengthService"}\NormalTok{, }\StringTok{"BusinessUnit"}\NormalTok{)}
\NormalTok{MYtarget  }\OtherTok{\textless{}{-}} \StringTok{"AbsenceRate"}

\FunctionTok{library}\NormalTok{(rpart, }\AttributeTok{quietly=}\ConstantTok{TRUE}\NormalTok{)}

\FunctionTok{set.seed}\NormalTok{(crv}\SpecialCharTok{$}\NormalTok{seed)}
\NormalTok{MYrpart }\OtherTok{\textless{}{-}} \FunctionTok{rpart}\NormalTok{(AbsenceRate }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .,}
                 \AttributeTok{data=}\NormalTok{MFGEmployees[, }\FunctionTok{c}\NormalTok{(MYinput, MYtarget)],}
                 \AttributeTok{method=}\StringTok{"anova"}\NormalTok{,}
                 \AttributeTok{parms=}\FunctionTok{list}\NormalTok{(}\AttributeTok{split=}\StringTok{"information"}\NormalTok{),}
                 \AttributeTok{control=}\FunctionTok{rpart.control}\NormalTok{(}\AttributeTok{minsplit=}\DecValTok{10}\NormalTok{,}
                                       \AttributeTok{maxdepth=}\DecValTok{10}\NormalTok{,}
                                       \AttributeTok{usesurrogate=}\DecValTok{0}\NormalTok{, }
                                       \AttributeTok{maxsurrogate=}\DecValTok{0}\NormalTok{))}

\FunctionTok{fancyRpartPlot}\NormalTok{(MYrpart, }\AttributeTok{main=}\StringTok{"Decision Tree MFGEmployees and AbsenceRate"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-619-1} \end{center}

The regression decision tree shows that age is a big factor in determining absence rate with gender playing a small part in one of the age ranges: \textgreater43 and \textless52 with males having a lower absence rate in this group. Almost all categorical information other than gender doesn't look like to help in the prediction.

Now let's look at linear regression as another model. The restriction in linear regression is that it can only accept non-categorical variables. Categorical variables can sometimes be made numeric through transformation, but that is beyond the scope of this article.

\#\#\#3.2 Linear Regression

In linear regression, then, we will need to restrict it to numeric variables:

\begin{itemize}
\tightlist
\item
  Age
\item
  Length of Service
\end{itemize}

being used to predict absence rate.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Linear Regression Model}
\NormalTok{RegressionCurrentData }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(AbsenceRate}\SpecialCharTok{\textasciitilde{}}\NormalTok{Age}\SpecialCharTok{+}\NormalTok{LengthService, }\AttributeTok{data=}\NormalTok{MFGEmployees)}

\FunctionTok{summary}\NormalTok{(RegressionCurrentData)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Call:
lm(formula = AbsenceRate ~ Age + LengthService, data = MFGEmployees)

Residuals:
   Min     1Q Median     3Q    Max 
-5.358 -0.847 -0.023  0.852  5.103 

Coefficients:
              Estimate Std. Error t value            Pr(>|t|)    
(Intercept)   -5.19021    0.06896   -75.3 <0.0000000000000002 ***
Age            0.20259    0.00151   134.2 <0.0000000000000002 ***
LengthService -0.08531    0.00565   -15.1 <0.0000000000000002 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.26 on 8162 degrees of freedom
Multiple R-squared:  0.689, Adjusted R-squared:  0.689 
F-statistic: 9.03e+03 on 2 and 8162 DF,  p-value: <0.0000000000000002
\end{verbatim}

The summary shows an adjusted R-squared of 0.689 which means approximately 69\% of the variance is accounted by age and length of service.
The variables are both significant at Pr(\textgreater\textbar t\textbar) of 'r summary(RegressionCurrentData)\$coefficients{[}3,4{]}`. These results are using the entirety of the existing data to predict itself.

Graphically it look like this:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#2D plot of Age and AbsenceRate}

\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Age,}\AttributeTok{y =}\NormalTok{ AbsenceRate),}\AttributeTok{data=}\NormalTok{MFGEmployees) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Age,}\AttributeTok{y =}\NormalTok{ AbsenceRate),}\AttributeTok{data=}\NormalTok{MFGEmployees,}\AttributeTok{method =} \StringTok{\textquotesingle{}lm\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-621-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#3D Scatterplot  of Age and Length of Service with Absence Rate {-} with Coloring and Vertical Lines}
\CommentTok{\# and Regression Plane }
\FunctionTok{library}\NormalTok{(scatterplot3d) }

\NormalTok{s3d }\OtherTok{\textless{}{-}}\FunctionTok{scatterplot3d}\NormalTok{(MFGEmployees}\SpecialCharTok{$}\NormalTok{Age,MFGEmployees}\SpecialCharTok{$}\NormalTok{LengthService,MFGEmployees}\SpecialCharTok{$}\NormalTok{AbsenceRate, }\AttributeTok{pch=}\DecValTok{16}\NormalTok{, }\AttributeTok{highlight.3d=}\ConstantTok{TRUE}\NormalTok{,}
                    \AttributeTok{type=}\StringTok{"h"}\NormalTok{, }\AttributeTok{main=}\StringTok{"Absence Rate By Age And Length of Service"}\NormalTok{)}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(MFGEmployees}\SpecialCharTok{$}\NormalTok{AbsenceRate }\SpecialCharTok{\textasciitilde{}}\NormalTok{ MFGEmployees}\SpecialCharTok{$}\NormalTok{Age}\SpecialCharTok{+}\NormalTok{MFGEmployees}\SpecialCharTok{$}\NormalTok{LengthService) }

\NormalTok{s3d}\SpecialCharTok{$}\FunctionTok{plane3d}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-621-2} \end{center}

\#\#4.Evaluate and evaluate the model

Up till now we have concentrated on producing a couple of models. The effort so far has had one weakness. \textbf{We have used all of our data for 2015 to generate the models.} They can both predict, but the prediction are based on existing data - that are already known. We don't know how well it will predict on data it hasn't seen yet.

To evaluate and critique the models, we need to train the model using part of the data and hold out a portion to test on. We will divide the data into 10 parts- using 9 parts as training data and 1 part as testing data, and alternate which are the 9 and the 1, so that each of the 10 parts gets to be training data 9 times and testing data once.

The R ``caret'' library helps us do that. We will run both a regression tree and linear regression and compare how they do against each other.

First the Linear Regression

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(caret)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{998}\NormalTok{)}

\NormalTok{inTraining }\OtherTok{\textless{}{-}} \FunctionTok{createDataPartition}\NormalTok{(MFGEmployees}\SpecialCharTok{$}\NormalTok{BusinessUnit, }\AttributeTok{p =}\NormalTok{ .}\DecValTok{75}\NormalTok{, }\AttributeTok{list =} \ConstantTok{FALSE}\NormalTok{)}

\NormalTok{training }\OtherTok{\textless{}{-}}\NormalTok{ MFGEmployees[inTraining,]}
\NormalTok{testing }\OtherTok{\textless{}{-}}\NormalTok{ MFGEmployees[ }\SpecialCharTok{{-}}\NormalTok{ inTraining,]}

\NormalTok{fitControl }\OtherTok{\textless{}{-}} \FunctionTok{trainControl}\NormalTok{(}\DocumentationTok{\#\# 10{-}fold CV}
\AttributeTok{method =} \StringTok{"repeatedcv"}\NormalTok{, }\AttributeTok{number =} \DecValTok{10}\NormalTok{, }
\AttributeTok{repeats =} \DecValTok{10} \DocumentationTok{\#\# repeated ten times}
\NormalTok{)}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{825}\NormalTok{)}

\NormalTok{lmFit1 }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(AbsenceRate }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Age }\SpecialCharTok{+}\NormalTok{ LengthService, }\AttributeTok{data =}\NormalTok{ training,}
                 \AttributeTok{method =} \StringTok{"lm"}\NormalTok{,}
                 \AttributeTok{trControl =}\NormalTok{ fitControl)}
\NormalTok{lmFit1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear Regression 

6124 samples
   2 predictor

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 5511, 5511, 5512, 5512, 5511, 5512, ... 
Resampling results:

  RMSE  Rsquared  MAE 
  1.27  0.691     1.01

Tuning parameter 'intercept' was held constant at a value of TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Testingdatasetandpredictions }\OtherTok{\textless{}{-}}\NormalTok{ testing }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{add\_predictions}\NormalTok{(lmFit1, }\AttributeTok{type =} \StringTok{"raw"}\NormalTok{)}

\NormalTok{Testingdatasetandpredictions}\SpecialCharTok{$}\NormalTok{pred[Testingdatasetandpredictions}\SpecialCharTok{$}\NormalTok{pred}\SpecialCharTok{\textless{}}\DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0} \CommentTok{\#Put a zero to all negative predictions}
\end{Highlighting}
\end{Shaded}

The Rsquared shows a value of 0.688 which means even with sampling different parts of the data on 10 fold cross validation the use of age and length of service seems to be pretty robust so far.

Next the decision tree. The first time with just the numeric variables.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{825}\NormalTok{)}

\NormalTok{rpartFit1 }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(AbsenceRate }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Age }\SpecialCharTok{+}\NormalTok{ LengthService, }\AttributeTok{data =}\NormalTok{ training,}
                 \AttributeTok{method =} \StringTok{"rpart"}\NormalTok{,}
                 \AttributeTok{trControl =}\NormalTok{ fitControl,}
                 \AttributeTok{maxdepth =} \DecValTok{5}\NormalTok{)}
\NormalTok{rpartFit1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
CART 

6124 samples
   2 predictor

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 5511, 5511, 5512, 5512, 5511, 5512, ... 
Resampling results across tuning parameters:

  cp      RMSE  Rsquared  MAE 
  0.0633  1.43  0.606     1.14
  0.0945  1.56  0.534     1.26
  0.4892  1.96  0.474     1.61

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was cp = 0.0633.
\end{verbatim}

You will notice that the decision tree with 10 fold cross validation didn't perform as well with an RSquared of approximately 0.60

The second time with the original categorical and numeric varibles used.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{825}\NormalTok{)}

\NormalTok{rpartFit2 }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(AbsenceRate }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Gender }\SpecialCharTok{+}\NormalTok{ DepartmentName }\SpecialCharTok{+}\NormalTok{ StoreLocation }\SpecialCharTok{+}\NormalTok{ Division }\SpecialCharTok{+}\NormalTok{ Age }\SpecialCharTok{+}\NormalTok{ LengthService }\SpecialCharTok{+}\NormalTok{ BusinessUnit, }\AttributeTok{data =}\NormalTok{ training,}
                 \AttributeTok{method =} \StringTok{"rpart"}\NormalTok{,}
                 \AttributeTok{trControl =}\NormalTok{ fitControl,}
                 \AttributeTok{maxdepth =} \DecValTok{5}\NormalTok{)}

\NormalTok{rpartFit2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
CART 

6124 samples
   7 predictor

No pre-processing
Resampling: Cross-Validated (10 fold, repeated 10 times) 
Summary of sample sizes: 5511, 5511, 5512, 5512, 5511, 5512, ... 
Resampling results across tuning parameters:

  cp      RMSE  Rsquared  MAE 
  0.0633  1.43  0.606     1.14
  0.0945  1.56  0.534     1.26
  0.4892  1.96  0.474     1.61

RMSE was used to select the optimal model using the smallest value.
The final value used for the model was cp = 0.0633.
\end{verbatim}

Here when you include all originally used vaiables in 10 fold cross validation, the Rsquared changed little and is still around 0.60.

So far, the linear regression is performing better.

\#\#5.Present Results and Document

The presenting of results and documenting is something that R helps in. You may not have realized it, but the R Markdown language has been used to create the full layout of these two blog articles. HTML,PDF and Word formats can be produced.

R Markdown allows the reader to see exactly what you having been doing, so that an independent person can replicate your results, to confirm what you have done. It shows the R code/commands, the statistical results and graphics, all is included in the Absenteeism-Part.Rmd file.

\#\#6.Deploy Model

Once you have evaluated your model(s) and chosen to use them, they need to be deployed so that they can be used. At the simplest level, `deploy' can mean using the `predict' function in R (where applicable) in conjunction with your model.

\textbf{Can it predict next year absenteeism?}

Let's predict the 2016 Absenteeism from the 2015 model.

If we make the simplifying assumption that nobody quits and nobody new comes in, we can take the 2015 data and add 1 to age and 1 to years of service for an approximation of new 2016 data before we get to 2016.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Apply model}
\CommentTok{\#Generate 2016 data}
\NormalTok{Absence2016Data}\OtherTok{\textless{}{-}}\NormalTok{MFGEmployees}
\NormalTok{Absence2016Data}\SpecialCharTok{$}\NormalTok{Age}\OtherTok{\textless{}{-}}\NormalTok{Absence2016Data}\SpecialCharTok{$}\NormalTok{Age}\SpecialCharTok{+}\DecValTok{1}
\NormalTok{Absence2016Data}\SpecialCharTok{$}\NormalTok{LengthService}\OtherTok{\textless{}{-}}\NormalTok{Absence2016Data}\SpecialCharTok{$}\NormalTok{LengthService}\SpecialCharTok{+}\DecValTok{1}

\NormalTok{Absence2016Data }\OtherTok{\textless{}{-}}\NormalTok{ Absence2016Data }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{add\_predictions}\NormalTok{(lmFit1, }\AttributeTok{type =} \StringTok{"raw"}\NormalTok{)}
\NormalTok{Absence2016Data}\SpecialCharTok{$}\NormalTok{pred[Absence2016Data}\SpecialCharTok{$}\NormalTok{pred}\SpecialCharTok{\textless{}}\DecValTok{0}\NormalTok{] }\OtherTok{\textless{}{-}} \DecValTok{0} \CommentTok{\#Put a zero to all negative predictions}
\end{Highlighting}
\end{Shaded}

To get single estimate for 2016 we ask for mean of absence rate.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(Absence2016Data}\SpecialCharTok{$}\NormalTok{pred)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 3.08
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(MFGEmployees}\SpecialCharTok{$}\NormalTok{AbsenceRate)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2.91
\end{verbatim}

The first figure above is the 2016 prediction, the second is the 2015 actual for comparison.

\textbf{If so, how well can it predict?}

As mentioned previously, about `r scales::percent(mean(MFGEmployees\$AbsenceRate)'. of the variation is accounted for in a linear regression model using age and length of service.

\textbf{Can we reduce our absenteeism?}

On the surface, only getting the age reduced and length of service increased will reduce absensteeism with this model.

Obviously, absenteeism is much more complex that just the rudimentary data we have collected. A serious look at this metric and problem would require more and different kinds of data. As mentioned before, the raw data used in this article and analysis is totally contrived to illustrate an example.

\hypertarget{absenteeism-work}{%
\chapter{Absenteeism at work}\label{absenteeism-work}}

The high competitiveness in the market, professional development combined with the development of organizations and the pressure to reach increasingly audacious goals, create increasingly overburdened employees and end up acquiring some disturbance in the state of health related to the type of work activity, including depression considered the evil of the 21st century. Taking employees to absenteeism. Absenteeism is defined as absence to work as expected, represents for the company the loss of productivity and quality of work.

The data set has been taken from \href{https://archive.ics.uci.edu/ml/datasets/Absenteeism+at+work\#}{UCI - Absenteeism at work}. The database used has 21 attributes and 740 records from documents that prove that they are absent from work and was collected from January 2008 to December 2016.

\hypertarget{data-reading}{%
\section{Data reading}\label{data-reading}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(DataExplorer)}
\FunctionTok{library}\NormalTok{(ggthemes)}
\FunctionTok{library}\NormalTok{(grid)}
\FunctionTok{library}\NormalTok{(gridExtra)}
\FunctionTok{library}\NormalTok{(factoextra)}
\FunctionTok{library}\NormalTok{(FactoMineR)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{absenteeism\_at\_work }\OtherTok{\textless{}{-}} \FunctionTok{read\_delim}\NormalTok{(}\AttributeTok{na =} \StringTok{"NA"}\NormalTok{, }\AttributeTok{delim =} \StringTok{";"}\NormalTok{, }\AttributeTok{col\_types =} \FunctionTok{cols}\NormalTok{(}\AttributeTok{col =} \FunctionTok{col\_character}\NormalTok{()), }\StringTok{"https://hranalytics.netlify.com/data/Absenteeism\_at\_work.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{basic-statistics-and-data-preparation}{%
\section{Basic statistics and data preparation}\label{basic-statistics-and-data-preparation}}

\textbf{Factors are in interger format , so for the sake of analysis we have changed them to factor format.}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(absenteeism\_at\_work)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
tibble [740 x 21] (S3: spec_tbl_df/tbl_df/tbl/data.frame)
 $ ID                             : num [1:740] 11 36 3 7 11 3 10 20 14 1 ...
 $ Reason for absence             : num [1:740] 26 0 23 7 23 23 22 23 19 22 ...
 $ Month of absence               : num [1:740] 7 7 7 7 7 7 7 7 7 7 ...
 $ Day of the week                : num [1:740] 3 3 4 5 5 6 6 6 2 2 ...
 $ Seasons                        : num [1:740] 1 1 1 1 1 1 1 1 1 1 ...
 $ Transportation expense         : num [1:740] 289 118 179 279 289 179 361 260 155 235 ...
 $ Distance from Residence to Work: num [1:740] 36 13 51 5 36 51 52 50 12 11 ...
 $ Service time                   : num [1:740] 13 18 18 14 13 18 3 11 14 14 ...
 $ Age                            : num [1:740] 33 50 38 39 33 38 28 36 34 37 ...
 $ Work load Average/day          : num [1:740] 240 240 240 240 240 ...
 $ Hit target                     : num [1:740] 97 97 97 97 97 97 97 97 97 97 ...
 $ Disciplinary failure           : num [1:740] 0 1 0 0 0 0 0 0 0 0 ...
 $ Education                      : num [1:740] 1 1 1 1 1 1 1 1 1 3 ...
 $ Son                            : num [1:740] 2 1 0 2 2 0 1 4 2 1 ...
 $ Social drinker                 : num [1:740] 1 1 1 1 1 1 1 1 1 0 ...
 $ Social smoker                  : num [1:740] 0 0 0 1 0 0 0 0 0 0 ...
 $ Pet                            : num [1:740] 1 0 0 0 1 0 4 0 0 1 ...
 $ Weight                         : num [1:740] 90 98 89 68 90 89 80 65 95 88 ...
 $ Height                         : num [1:740] 172 178 170 168 172 170 172 168 196 172 ...
 $ Body mass index                : num [1:740] 30 31 31 24 30 31 27 23 25 29 ...
 $ Absenteeism time in hours      : num [1:740] 4 0 2 4 2 2 8 4 40 8 ...
 - attr(*, "spec")=
  .. cols(
  ..   ID = col_double(),
  ..   `Reason for absence` = col_double(),
  ..   `Month of absence` = col_double(),
  ..   `Day of the week` = col_double(),
  ..   Seasons = col_double(),
  ..   `Transportation expense` = col_double(),
  ..   `Distance from Residence to Work` = col_double(),
  ..   `Service time` = col_double(),
  ..   Age = col_double(),
  ..   `Work load Average/day ` = col_double(),
  ..   `Hit target` = col_double(),
  ..   `Disciplinary failure` = col_double(),
  ..   Education = col_double(),
  ..   Son = col_double(),
  ..   `Social drinker` = col_double(),
  ..   `Social smoker` = col_double(),
  ..   Pet = col_double(),
  ..   Weight = col_double(),
  ..   Height = col_double(),
  ..   `Body mass index` = col_double(),
  ..   `Absenteeism time in hours` = col_double()
  .. )
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(absenteeism\_at\_work)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       ID     Reason for absence Month of absence Day of the week
 Min.   : 1   Min.   : 0.0       Min.   : 0.00    Min.   :2.00   
 1st Qu.: 9   1st Qu.:13.0       1st Qu.: 3.00    1st Qu.:3.00   
 Median :18   Median :23.0       Median : 6.00    Median :4.00   
 Mean   :18   Mean   :19.2       Mean   : 6.32    Mean   :3.91   
 3rd Qu.:28   3rd Qu.:26.0       3rd Qu.: 9.00    3rd Qu.:5.00   
 Max.   :36   Max.   :28.0       Max.   :12.00    Max.   :6.00   
    Seasons     Transportation expense Distance from Residence to Work
 Min.   :1.00   Min.   :118            Min.   : 5.0                   
 1st Qu.:2.00   1st Qu.:179            1st Qu.:16.0                   
 Median :3.00   Median :225            Median :26.0                   
 Mean   :2.54   Mean   :221            Mean   :29.6                   
 3rd Qu.:4.00   3rd Qu.:260            3rd Qu.:50.0                   
 Max.   :4.00   Max.   :388            Max.   :52.0                   
  Service time       Age       Work load Average/day    Hit target   
 Min.   : 1.0   Min.   :27.0   Min.   :206            Min.   : 81.0  
 1st Qu.: 9.0   1st Qu.:31.0   1st Qu.:244            1st Qu.: 93.0  
 Median :13.0   Median :37.0   Median :264            Median : 95.0  
 Mean   :12.6   Mean   :36.5   Mean   :271            Mean   : 94.6  
 3rd Qu.:16.0   3rd Qu.:40.0   3rd Qu.:294            3rd Qu.: 97.0  
 Max.   :29.0   Max.   :58.0   Max.   :379            Max.   :100.0  
 Disciplinary failure   Education         Son       Social drinker 
 Min.   :0.000        Min.   :1.00   Min.   :0.00   Min.   :0.000  
 1st Qu.:0.000        1st Qu.:1.00   1st Qu.:0.00   1st Qu.:0.000  
 Median :0.000        Median :1.00   Median :1.00   Median :1.000  
 Mean   :0.054        Mean   :1.29   Mean   :1.02   Mean   :0.568  
 3rd Qu.:0.000        3rd Qu.:1.00   3rd Qu.:2.00   3rd Qu.:1.000  
 Max.   :1.000        Max.   :4.00   Max.   :4.00   Max.   :1.000  
 Social smoker        Pet           Weight        Height   
 Min.   :0.000   Min.   :0.00   Min.   : 56   Min.   :163  
 1st Qu.:0.000   1st Qu.:0.00   1st Qu.: 69   1st Qu.:169  
 Median :0.000   Median :0.00   Median : 83   Median :170  
 Mean   :0.073   Mean   :0.75   Mean   : 79   Mean   :172  
 3rd Qu.:0.000   3rd Qu.:1.00   3rd Qu.: 89   3rd Qu.:172  
 Max.   :1.000   Max.   :8.00   Max.   :108   Max.   :196  
 Body mass index Absenteeism time in hours
 Min.   :19.0    Min.   :  0.0            
 1st Qu.:24.0    1st Qu.:  2.0            
 Median :25.0    Median :  3.0            
 Mean   :26.7    Mean   :  6.9            
 3rd Qu.:31.0    3rd Qu.:  8.0            
 Max.   :38.0    Max.   :120.0            
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# converting variables to factors}

\NormalTok{col }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\SpecialCharTok{:}\DecValTok{5}\NormalTok{,}\DecValTok{12}\SpecialCharTok{:}\DecValTok{17}\NormalTok{)}
\NormalTok{absenteeism\_at\_work\_factored }\OtherTok{\textless{}{-}}\NormalTok{ absenteeism\_at\_work}
\NormalTok{absenteeism\_at\_work\_factored[col] }\OtherTok{\textless{}{-}} \FunctionTok{lapply}\NormalTok{(absenteeism\_at\_work\_factored[col], factor)}

\CommentTok{\# converting codes to meaningful information}

\NormalTok{absenteeism\_at\_work\_factored }\OtherTok{\textless{}{-}}\NormalTok{ absenteeism\_at\_work\_factored }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Reason for absence}\StringTok{\textasciigrave{}} \OtherTok{=} \FunctionTok{fct\_recode}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Reason for absence}\StringTok{\textasciigrave{}}\NormalTok{,}\StringTok{\textasciigrave{}}\AttributeTok{Infectious, parasitic diseases}\StringTok{\textasciigrave{}}\OtherTok{=}\StringTok{"0"}\NormalTok{, }\StringTok{\textasciigrave{}}\AttributeTok{Neoplasms}\StringTok{\textasciigrave{}}\OtherTok{=}\StringTok{"1"}\NormalTok{,}\StringTok{\textasciigrave{}}\AttributeTok{Diseases of the blood}\StringTok{\textasciigrave{}}\OtherTok{=}\StringTok{"2"}\NormalTok{,}\StringTok{\textasciigrave{}}\AttributeTok{Endocrine and metabolic diseases}\StringTok{\textasciigrave{}}\OtherTok{=}\StringTok{"3"}\NormalTok{,}\StringTok{\textasciigrave{}}\AttributeTok{Mental and behavioural disorders}\StringTok{\textasciigrave{}}\OtherTok{=}\StringTok{"4"}\NormalTok{,}\StringTok{\textasciigrave{}}\AttributeTok{Diseases of the nervous system}\StringTok{\textasciigrave{}}\OtherTok{=}\StringTok{"5"}\NormalTok{,}\StringTok{\textasciigrave{}}\AttributeTok{Diseases of the eye and adnexa}\StringTok{\textasciigrave{}}\OtherTok{=}\StringTok{"6"}\NormalTok{,}\StringTok{\textasciigrave{}}\AttributeTok{Diseases of the ear and mastoid process}\StringTok{\textasciigrave{}}\OtherTok{=}\StringTok{"7"}\NormalTok{,}\StringTok{\textasciigrave{}}\AttributeTok{Diseases of the circulatory system}\StringTok{\textasciigrave{}}\OtherTok{=}\StringTok{"8"}\NormalTok{,}\StringTok{\textasciigrave{}}\AttributeTok{Diseases of the respiratory system}\StringTok{\textasciigrave{}}\OtherTok{=}\StringTok{"9"}\NormalTok{,}\StringTok{\textasciigrave{}}\AttributeTok{Diseases of the digestive system}\StringTok{\textasciigrave{}}\OtherTok{=}\StringTok{"10"}\NormalTok{, }\StringTok{\textasciigrave{}}\AttributeTok{Diseases of the skin and subcutaneous tissue}\StringTok{\textasciigrave{}}\OtherTok{=}\StringTok{"11"}\NormalTok{,}\StringTok{\textasciigrave{}}\AttributeTok{Diseases of the musculoskeletal system and connective tissue}\StringTok{\textasciigrave{}}\OtherTok{=}\StringTok{"12"}\NormalTok{, }\StringTok{\textasciigrave{}}\AttributeTok{Diseases of the genitourinary system}\StringTok{\textasciigrave{}}\OtherTok{=}\StringTok{"13"}\NormalTok{,}\StringTok{\textasciigrave{}}\AttributeTok{Pregnancy, childbirth and the puerperium}\StringTok{\textasciigrave{}}\OtherTok{=}\StringTok{"14"}\NormalTok{,}\StringTok{\textasciigrave{}}\AttributeTok{Certain conditions originating in the perinatal}\StringTok{\textasciigrave{}}\OtherTok{=}\StringTok{"15"}\NormalTok{,  }\StringTok{\textasciigrave{}}\AttributeTok{Congenital malformations, deformations and chromosomal abnormalities}\StringTok{\textasciigrave{}}\OtherTok{=} \StringTok{"16"}\NormalTok{,}\StringTok{\textasciigrave{}}\AttributeTok{Symptoms, signs and abnormal clinical  findings}\StringTok{\textasciigrave{}}\OtherTok{=}\StringTok{"17"}\NormalTok{, }\StringTok{\textasciigrave{}}\AttributeTok{Injury, poisoning and certain other consequences of external causes}\StringTok{\textasciigrave{}}\OtherTok{=} \StringTok{"18"}\NormalTok{,}\StringTok{\textasciigrave{}}\AttributeTok{causes of morbidity and mortality}\StringTok{\textasciigrave{}}\OtherTok{=}\StringTok{"19"}\NormalTok{, }\StringTok{\textasciigrave{}}\AttributeTok{Factors influencing health status and contact with health services}\StringTok{\textasciigrave{}}\OtherTok{=}\StringTok{"21"}\NormalTok{,}\StringTok{\textasciigrave{}}\AttributeTok{patient follow{-}up}\StringTok{\textasciigrave{}}\OtherTok{=}\StringTok{"22"}\NormalTok{,}\StringTok{\textasciigrave{}}\AttributeTok{medical consultation}\StringTok{\textasciigrave{}}\OtherTok{=}\StringTok{"23"}\NormalTok{,}\StringTok{\textasciigrave{}}\AttributeTok{blood donation}\StringTok{\textasciigrave{}}\OtherTok{=}\StringTok{"24"}\NormalTok{, }\StringTok{\textasciigrave{}}\AttributeTok{laboratory examination}\StringTok{\textasciigrave{}}\OtherTok{=}\StringTok{"25"}\NormalTok{, }\StringTok{\textasciigrave{}}\AttributeTok{unjustified absence}\StringTok{\textasciigrave{}}\OtherTok{=}\StringTok{"26"}\NormalTok{, }\StringTok{\textasciigrave{}}\AttributeTok{physiotherapy}\StringTok{\textasciigrave{}}\OtherTok{=}\StringTok{"27"}\NormalTok{, }\StringTok{\textasciigrave{}}\AttributeTok{dental consultation}\StringTok{\textasciigrave{}}\OtherTok{=}\StringTok{"28"}\NormalTok{))}

\NormalTok{absenteeism\_at\_work\_factored }\OtherTok{\textless{}{-}}\NormalTok{ absenteeism\_at\_work\_factored }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Month of absence}\StringTok{\textasciigrave{}}\OtherTok{=} \FunctionTok{fct\_recode}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Month of absence}\StringTok{\textasciigrave{}}\NormalTok{,}\AttributeTok{None=}\StringTok{"0"}\NormalTok{,}\AttributeTok{Jan=}\StringTok{"1"}\NormalTok{,}\AttributeTok{Feb=}\StringTok{"2"}\NormalTok{,}\AttributeTok{Mar=}\StringTok{"3"}\NormalTok{,}\AttributeTok{Apr=}\StringTok{"4"}\NormalTok{,}\AttributeTok{May=}\StringTok{"5"}\NormalTok{, }\AttributeTok{Jun=}\StringTok{"6"}\NormalTok{,}\AttributeTok{Jul=}\StringTok{"7"}\NormalTok{,}\AttributeTok{Aug=}\StringTok{"8"}\NormalTok{,}\AttributeTok{Sep=}\StringTok{"9"}\NormalTok{,}\AttributeTok{Oct=}\StringTok{"10"}\NormalTok{,}\AttributeTok{Nov=}\StringTok{"11"}\NormalTok{,}\AttributeTok{Dec=}\StringTok{"12"}\NormalTok{) )}

\NormalTok{absenteeism\_at\_work\_factored }\OtherTok{\textless{}{-}}\NormalTok{ absenteeism\_at\_work\_factored }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Seasons=} \FunctionTok{fct\_recode}\NormalTok{(Seasons,}\AttributeTok{summer=}\StringTok{"1"}\NormalTok{,}\AttributeTok{autumn=}\StringTok{"2"}\NormalTok{,}\AttributeTok{winter=}\StringTok{"3"}\NormalTok{,}\AttributeTok{spring=}\StringTok{"4"}\NormalTok{))}

\NormalTok{absenteeism\_at\_work\_factored }\OtherTok{\textless{}{-}}\NormalTok{ absenteeism\_at\_work\_factored }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Education =} \FunctionTok{fct\_recode}\NormalTok{(Education,}\AttributeTok{highschool=}\StringTok{"1"}\NormalTok{,}\AttributeTok{graduate=}\StringTok{"2"}\NormalTok{,}\AttributeTok{postgraduate=}\StringTok{"3"}\NormalTok{,}\StringTok{\textasciigrave{}}\AttributeTok{master\& doctrate}\StringTok{\textasciigrave{}}\OtherTok{=}\StringTok{"4"}\NormalTok{))}

\NormalTok{absenteeism\_at\_work\_factored }\OtherTok{\textless{}{-}}\NormalTok{ absenteeism\_at\_work\_factored }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Disciplinary failure}\StringTok{\textasciigrave{}}\OtherTok{=} \FunctionTok{fct\_recode}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Disciplinary failure}\StringTok{\textasciigrave{}}\NormalTok{,}\AttributeTok{No=}\StringTok{"0"}\NormalTok{,}\AttributeTok{Yes=}\StringTok{"1"}\NormalTok{))}

\NormalTok{absenteeism\_at\_work\_factored }\OtherTok{\textless{}{-}}\NormalTok{ absenteeism\_at\_work\_factored }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Social drinker}\StringTok{\textasciigrave{}}\OtherTok{=} \FunctionTok{fct\_recode}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Social drinker}\StringTok{\textasciigrave{}}\NormalTok{,}\AttributeTok{No=}\StringTok{"0"}\NormalTok{,}\AttributeTok{Yes=}\StringTok{"1"}\NormalTok{))}

\NormalTok{absenteeism\_at\_work\_factored }\OtherTok{\textless{}{-}}\NormalTok{ absenteeism\_at\_work\_factored }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Social smoker}\StringTok{\textasciigrave{}}\OtherTok{=} \FunctionTok{fct\_recode}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Social smoker}\StringTok{\textasciigrave{}}\NormalTok{,}\AttributeTok{No=}\StringTok{"0"}\NormalTok{,}\AttributeTok{Yes=}\StringTok{"1"}\NormalTok{))}

\NormalTok{absenteeism\_at\_work\_factored }\OtherTok{\textless{}{-}}\NormalTok{ absenteeism\_at\_work\_factored }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Day of the week}\StringTok{\textasciigrave{}} \OtherTok{=} \FunctionTok{fct\_recode}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Day of the week}\StringTok{\textasciigrave{}}\NormalTok{, }\AttributeTok{Monday=}\StringTok{"2"}\NormalTok{, }\AttributeTok{Tuesday=}\StringTok{"3"}\NormalTok{, }\AttributeTok{Wednesday=}\StringTok{"4"}\NormalTok{, }\AttributeTok{Thursday=}\StringTok{"5"}\NormalTok{, }\AttributeTok{Friday=}\StringTok{"6"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{data-exploration}{%
\section{Data exploration}\label{data-exploration}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OtherTok{\textless{}{-}}\NormalTok{ absenteeism\_at\_work\_factored }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Pet, }\AttributeTok{fill =}\NormalTok{ Pet) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{() }

\NormalTok{s }\OtherTok{\textless{}{-}}\NormalTok{ absenteeism\_at\_work\_factored }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Son, }\AttributeTok{fill =}\NormalTok{ Son) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{()}

\NormalTok{SS }\OtherTok{\textless{}{-}}\NormalTok{ absenteeism\_at\_work\_factored }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\StringTok{\textasciigrave{}}\AttributeTok{Social smoker}\StringTok{\textasciigrave{}}\NormalTok{, }\AttributeTok{fill =}\StringTok{\textasciigrave{}}\AttributeTok{Social drinker}\StringTok{\textasciigrave{}}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{() }

\NormalTok{S }\OtherTok{\textless{}{-}}\NormalTok{ absenteeism\_at\_work\_factored }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{   Seasons,}\AttributeTok{fill =}\NormalTok{ Seasons) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{()}

\NormalTok{Day }\OtherTok{\textless{}{-}}\NormalTok{ absenteeism\_at\_work\_factored }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\StringTok{\textasciigrave{}}\AttributeTok{Day of the week}\StringTok{\textasciigrave{}}\NormalTok{, }\AttributeTok{fill =}\StringTok{\textasciigrave{}}\AttributeTok{Day of the week}\StringTok{\textasciigrave{}}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{() }

\FunctionTok{grid.arrange}\NormalTok{(p,s, }\AttributeTok{nrow =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-631-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{grid.arrange}\NormalTok{(SS,S, }\AttributeTok{nrow =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-631-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{grid.arrange}\NormalTok{(Day, }\AttributeTok{nrow =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-631-3} \end{center}

\hypertarget{some-more-digging-into-the-data}{%
\section{Some more digging into the data}\label{some-more-digging-into-the-data}}

\textbf{I have taken those data that consists of Absenteesim in hours that are relavent to the analysis.}\\
\textbf{The 649 obs were found to have absent with respect to total of 740 obs.}

\textbf{Here the proportion of elements of categorical variables that contribute to the target variable.}\\
\textbf{I have taken only certain variable that I thought would come in to process.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{absent }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(absenteeism\_at\_work\_factored }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\FunctionTok{everything}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{filter}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Absenteeism time in hours}\StringTok{\textasciigrave{}} \SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{))}

\NormalTok{season1 }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(absent }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{group\_by}\NormalTok{(Seasons) }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{summarise}\NormalTok{(}\AttributeTok{count=} \FunctionTok{n}\NormalTok{(), }\AttributeTok{percent =} \FunctionTok{round}\NormalTok{(count}\SpecialCharTok{*}\DecValTok{100}\SpecialCharTok{/}\FunctionTok{nrow}\NormalTok{(absent),}\DecValTok{1}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(count)))}

\NormalTok{season1 }\SpecialCharTok{\%\textgreater{}\%}
\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=} \FunctionTok{reorder}\NormalTok{(Seasons,percent), }\AttributeTok{y=}\NormalTok{ percent, }\AttributeTok{fill =}\NormalTok{ Seasons) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat=}\StringTok{\textquotesingle{}identity\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ percent), }\AttributeTok{vjust =} \FloatTok{1.1}\NormalTok{, }\AttributeTok{hjust =} \FloatTok{1.2}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{xlab}\NormalTok{(}\StringTok{\textquotesingle{}Seasons\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-632-1} \end{center}

\textbf{The attribute disciplinary failure is taken into consideration and it was found it had no obvious part on target variable. Basically all disciplinary failures resulted in zero hours absences.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{disciplinary }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(absent }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{group\_by}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Disciplinary failure}\StringTok{\textasciigrave{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{summarise}\NormalTok{(}\AttributeTok{count=} \FunctionTok{n}\NormalTok{(), }\AttributeTok{percent =} \FunctionTok{round}\NormalTok{(count}\SpecialCharTok{*}\DecValTok{100}\SpecialCharTok{/}\FunctionTok{nrow}\NormalTok{(absent),}\DecValTok{1}\NormalTok{))}\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(count)))}

\NormalTok{disciplinary }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=} \FunctionTok{reorder}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Disciplinary failure}\StringTok{\textasciigrave{}}\NormalTok{,percent), }
      \AttributeTok{y=}\NormalTok{ percent, }\AttributeTok{fill =} \StringTok{\textasciigrave{}}\AttributeTok{Disciplinary failure}\StringTok{\textasciigrave{}}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat=}\StringTok{\textquotesingle{}identity\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ percent), }\AttributeTok{vjust =} \FloatTok{1.1}\NormalTok{, }\AttributeTok{hjust =} \FloatTok{1.2}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{xlab}\NormalTok{(}\StringTok{\textquotesingle{}Disciplinary failure\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-633-1} \end{center}

\hypertarget{here-the-various-types-of-reasons-for-absence-attribute-is-analysed}{%
\section{Here the various types of reasons for absence attribute is analysed}\label{here-the-various-types-of-reasons-for-absence-attribute-is-analysed}}

\textbf{NOTE: The top four of them cover 50\% of the resons for absence}\\
* \textbf{medical consultation}\\
* \textbf{dental consultation}\\
* \textbf{physiotherapy}\\
* \textbf{Disease of genitourinary system}\\
\textbf{The unjusitified absence amounts to 4.7\% of total.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Reason }\OtherTok{\textless{}{-}}  \FunctionTok{as.data.frame}\NormalTok{(absent }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{group\_by}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Reason for absence}\StringTok{\textasciigrave{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{summarise}\NormalTok{(}\AttributeTok{count=} \FunctionTok{n}\NormalTok{(), }\AttributeTok{percent =} \FunctionTok{round}\NormalTok{(count}\SpecialCharTok{*}\DecValTok{100}\SpecialCharTok{/}\FunctionTok{nrow}\NormalTok{(absent),}\DecValTok{1}\NormalTok{))}\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(count)))}

\NormalTok{Reason }\SpecialCharTok{\%\textgreater{}\%}
\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{reorder}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Reason for absence}\StringTok{\textasciigrave{}}\NormalTok{,percent), }
      \AttributeTok{y=}\NormalTok{ percent, }\AttributeTok{fill=} \StringTok{\textasciigrave{}}\AttributeTok{Reason for absence}\StringTok{\textasciigrave{}}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{\textquotesingle{}identity\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position=}\StringTok{\textquotesingle{}none\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+}  
  \FunctionTok{geom\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ percent), }\AttributeTok{vjust =} \FloatTok{0.5}\NormalTok{, }\AttributeTok{hjust =} \FloatTok{1.1}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{xlab}\NormalTok{(}\StringTok{\textquotesingle{}Reason for absence\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-634-1} \end{center}

\textbf{Close to the half of employees drink alcohol(320/420), so the attempted analysis can be taken into consideration that the it can be a element that influence the target variable.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{absent }\SpecialCharTok{\%\textgreater{}\%}
\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{ Age,}\AttributeTok{y=} \StringTok{\textasciigrave{}}\AttributeTok{Absenteeism time in hours}\StringTok{\textasciigrave{}}\NormalTok{,}\AttributeTok{fill=} \StringTok{\textasciigrave{}}\AttributeTok{Social drinker}\StringTok{\textasciigrave{}}\NormalTok{)}\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat=}\StringTok{\textquotesingle{}identity\textquotesingle{}}\NormalTok{,}\AttributeTok{position=} \FunctionTok{position\_dodge}\NormalTok{()) }\SpecialCharTok{+} 
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{breaks =}\FunctionTok{c}\NormalTok{(}\FunctionTok{seq}\NormalTok{(}\DecValTok{20}\NormalTok{,}\DecValTok{60}\NormalTok{,}\DecValTok{5}\NormalTok{)),}\AttributeTok{limits=}\FunctionTok{c}\NormalTok{(}\DecValTok{20}\NormalTok{,}\DecValTok{60}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-635-1} \end{center}

\textbf{Service time across hit target}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{absent }\SpecialCharTok{\%\textgreater{}\%}
\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=} \StringTok{\textasciigrave{}}\AttributeTok{Service time}\StringTok{\textasciigrave{}}\NormalTok{,}
      \AttributeTok{y=} \StringTok{\textasciigrave{}}\AttributeTok{Hit target}\StringTok{\textasciigrave{}}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{\textquotesingle{}loess\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{\textquotesingle{}Analysis of Hit target across Service time\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{xlab}\NormalTok{(}\StringTok{\textquotesingle{}Service time(years)\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{ylab}\NormalTok{(}\StringTok{\textquotesingle{}Hit target(\%)\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-636-1} \end{center}

\textbf{Hit target is achieved by the employees of different age segments}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{absent }\SpecialCharTok{\%\textgreater{}\%}
\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{ Age,}\AttributeTok{y=} \StringTok{\textasciigrave{}}\AttributeTok{Hit target}\StringTok{\textasciigrave{}}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{\textquotesingle{}loess\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title=}\StringTok{\textquotesingle{}Analysis of Hit target across Age\textquotesingle{}}\NormalTok{,}
       \AttributeTok{x=}\StringTok{\textquotesingle{}Age\textquotesingle{}}\NormalTok{,}
       \AttributeTok{y=}\StringTok{\textquotesingle{}Hit target(\%)\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-637-1} \end{center}

\textbf{Here trend of service time across age is taken. And they have positive correlation}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{absent }\SpecialCharTok{\%\textgreater{}\%}
\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{ Age,}\AttributeTok{y=} \StringTok{\textasciigrave{}}\AttributeTok{Service time}\StringTok{\textasciigrave{}}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{\textquotesingle{}lm\textquotesingle{}}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title=}\StringTok{\textquotesingle{}Analysis of Service time across Age\textquotesingle{}}\NormalTok{,}
       \AttributeTok{x=}\StringTok{\textquotesingle{}Age\textquotesingle{}}\NormalTok{,}
       \AttributeTok{y=}\StringTok{\textquotesingle{}Service time\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`geom_smooth()` using formula 'y ~ x'
\end{verbatim}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-638-1} \end{center}

\hypertarget{principal-component-analysis-pca}{%
\section{Principal component analysis (PCA)}\label{principal-component-analysis-pca}}

A principal component analysis is used to extract the most important information from a multivariate data table and to express this information as a set of new variables called principal components. Principal component analysis (PCA) reduces the dimensionality of multivariate data, to two or three that can be visualized graphically with minimal loss of information.

The information in a given data set corresponds to the total variation it contains. The goal of PCA is to identify directions along which the variation in the data is maximal. These directions (called also principal components) can be used to visualize graphically the data.

We will be using the FactoMineR (for computing PCA) and factoextra (for PCA visualization) packages.

The first part of this article describes quickly how to compute and visualize principal component analysis using FactoMineR and factoextra
The second part shows how to identify the most important variables that explain the variations in your data

\textbf{Data preparation for PCA}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{absenteeism\_at\_work}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{Work load Average/day }\StringTok{\textasciigrave{}} \OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{as.character}\NormalTok{(absenteeism\_at\_work}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{Work load Average/day }\StringTok{\textasciigrave{}}\NormalTok{ ))}

\NormalTok{d1 }\OtherTok{\textless{}{-}}\NormalTok{ absenteeism\_at\_work }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{ID) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\StringTok{\textasciigrave{}}\AttributeTok{Absenteeism time in hours}\StringTok{\textasciigrave{}}\NormalTok{)}

\NormalTok{d1 }\OtherTok{\textless{}{-}} \FunctionTok{scale}\NormalTok{(d1)  }
\end{Highlighting}
\end{Shaded}

\textbf{PCA}

In the following we will produce a scree plot. The scree plot is a an analysis that shows you how many factors or components you have to retain in your factor or principal components analysis. It is a graphical representation. The assumption is that ``the elbow'' tells you how many factors or components you have to retain. The elbow means when the line of the graph starts to smooth up.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pcaex }\OtherTok{\textless{}{-}} \FunctionTok{PCA}\NormalTok{(d1,}\AttributeTok{graph =}\NormalTok{ F) }

\CommentTok{\#The output of the function PCA() is a list including :}
\FunctionTok{print}\NormalTok{(pcaex)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
**Results for the Principal Component Analysis (PCA)**
The analysis was performed on 740 individuals, described by 19 variables
*The results are available in the following objects:

   name               description                          
1  "$eig"             "eigenvalues"                        
2  "$var"             "results for the variables"          
3  "$var$coord"       "coord. for the variables"           
4  "$var$cor"         "correlations variables - dimensions"
5  "$var$cos2"        "cos2 for the variables"             
6  "$var$contrib"     "contributions of the variables"     
7  "$ind"             "results for the individuals"        
8  "$ind$coord"       "coord. for the individuals"         
9  "$ind$cos2"        "cos2 for the individuals"           
10 "$ind$contrib"     "contributions of the individuals"   
11 "$call"            "summary statistics"                 
12 "$call$centre"     "mean of the variables"              
13 "$call$ecart.type" "standard error of the variables"    
14 "$call$row.w"      "weights for the individuals"        
15 "$call$col.w"      "weights for the variables"          
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#The proportion of variation retained by the principal components (PCs) can be extracted as follow :}

\NormalTok{egv1 }\OtherTok{\textless{}{-}} \FunctionTok{get\_eigenvalue}\NormalTok{(pcaex)}
\FunctionTok{head}\NormalTok{(egv1[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      eigenvalue variance.percent
Dim.1       3.34            17.58
Dim.2       2.26            11.88
Dim.3       1.94            10.21
Dim.4       1.51             7.94
Dim.5       1.39             7.30
Dim.6       1.23             6.49
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#Here, 60\% of the information (variances) contained in the data are retained by the first six principal components.}

\CommentTok{\#The amount of variation retained by each PC is called eigenvalues. The first PC corresponds to the direction with the maximum amount of variation in the data set.}
\CommentTok{\#The importance of PCs can be visualized using a scree plot :}

\CommentTok{\#Plot the eigenvalues/variances against the number of dimensions}

\CommentTok{\# eigen values {-}}
\NormalTok{egv1 }\OtherTok{\textless{}{-}} \FunctionTok{get\_eigenvalue}\NormalTok{(pcaex)}
\FunctionTok{fviz\_eig}\NormalTok{(pcaex,}\AttributeTok{addlabels=}\NormalTok{T)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-640-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# correlation of variables with PCA components{-}}
\FunctionTok{fviz\_pca\_var}\NormalTok{(pcaex,}\AttributeTok{col.var=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-641-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pcaex}\SpecialCharTok{$}\NormalTok{var}\SpecialCharTok{$}\NormalTok{contrib}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                                  Dim.1    Dim.2    Dim.3    Dim.4  Dim.5
Reason for absence               0.0795  1.17165 25.29507  5.32313  1.849
Month of absence                 0.0128  8.41604  7.75872 21.03291  4.322
Day of the week                  0.1891  0.32499  1.76749  0.70991 13.827
Seasons                          0.0428  2.16080  9.28314  6.51084  3.818
Transportation expense           3.3278 21.18599  0.26722  1.22436  2.164
Distance from Residence to Work  0.3434 13.77449 16.09334  0.00193  0.306
Service time                    18.4990  0.72408  0.03471  2.54520 10.232
Age                             14.3386  0.23638  1.85544  4.78763  7.462
Work load Average/day            0.1028  0.00934  2.01150  3.14305  0.965
Hit target                       0.3061  5.93894  5.62363 10.88925  1.762
Disciplinary failure             0.1471  3.67096 20.46621  3.84722  0.230
Education                        6.6039  9.08390  1.03185  0.86395  1.020
Son                              0.2764 10.25788  0.00737 12.83974  0.553
Social drinker                   8.9983  8.37065  2.81965  1.34722  1.728
Social smoker                    0.6560  0.01194  1.78505 21.24828  5.712
Pet                              3.8900  8.61414  0.45296  0.01825 10.086
Weight                          20.7634  0.07215  0.41055  0.95704 11.429
Height                           0.1488  5.43333  3.02288  0.79817 20.343
Body mass index                 21.2742  0.54235  0.01320  1.91192  2.190
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# quality of presentation of variables in correlogram{-}}
\FunctionTok{fviz\_cos2}\NormalTok{(pcaex,}\AttributeTok{choice=}\StringTok{\textquotesingle{}var\textquotesingle{}}\NormalTok{,}\AttributeTok{axes=}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-641-2} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# contribution of variables to the respective principal components{-}}
\FunctionTok{fviz\_contrib}\NormalTok{(pcaex,}\AttributeTok{choice=}\StringTok{\textquotesingle{}var\textquotesingle{}}\NormalTok{,}\AttributeTok{axes=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-641-3} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fviz\_contrib}\NormalTok{(pcaex,}\AttributeTok{choice=}\StringTok{\textquotesingle{}var\textquotesingle{}}\NormalTok{,}\AttributeTok{axes=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-641-4} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fviz\_contrib}\NormalTok{(pcaex,}\AttributeTok{choice=}\StringTok{\textquotesingle{}var\textquotesingle{}}\NormalTok{,}\AttributeTok{axes=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-641-5} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fviz\_contrib}\NormalTok{(pcaex,}\AttributeTok{choice=}\StringTok{\textquotesingle{}var\textquotesingle{}}\NormalTok{,}\AttributeTok{axes=}\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-641-6} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fviz\_contrib}\NormalTok{(pcaex,}\AttributeTok{choice=}\StringTok{\textquotesingle{}var\textquotesingle{}}\NormalTok{,}\AttributeTok{axes=}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=\textwidth]{HR_Analytics.live_files/figure-latex/unnamed-chunk-641-7} \end{center}

\hypertarget{accidents-work}{%
\chapter{Accidents at work}\label{accidents-work}}

According to the International Labour Organization (ILO), more than 337 million accidents happen on the job each year, resulting, together with occupational diseases, in more than 2.3 million deaths annually.

Accidents arise from unsafe behavior and/or unsafe conditions. An important factor is the safety climate or safety culture of an organization. Safety culture concerns how workplace safety is managed, consisting of the shared attitudes, beliefs, perceptions, and values among employees.

Your incident and DART (Days Away, Restrictions and Transfers) rates are OSHA safety metrics that help you understand the effectiveness of your safety and risk management programs:
* Your incident rate shows the total number of OSHA-recordable injuries and illness per 200,000 hours. The rate is viewed as a percentage of injuries per 100 employees per year.
* Your DART rate calculates the rate of days away, restrictions and transfers per 200,000 worked.

\#Source: \url{https://data.louisvilleky.gov/dataset/absenteeism}

This dataset shows the number of hours employees were absent from work during a given pay period. The data is broken down by department and type of absence and includes the last 6 years worth of pay periods. Vacation and holiday time is not consider an absence.

Data dictionary

DEPT - The name of the department where employees were absent
BIWEEKLY\_PAY\_PERIOD\_END\_DATE - The end date for the pay period when employees were absent
PARETO\_TYPE - The type of absence (vacation and holiday time is excluded)
TOTAL\_HOURS - The total amount of absent hours for the given type of absence, department, and pay period
TOTAL EMPLOYEE - The total number of employees that worked in the given department and pay period
TOTAL AVAL HOURS - The total number of hours scheduled for work for all employees in the given department and pay period
LOST WORKTIME RATE - The total amount of absent hours for the given type of absence, department, and pay period divided by The total number of hours scheduled for work for all employees in the given department and pay period (metric defined by the Bureau of Labor Statistics)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(readr)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{LTI\_Analysis }\OtherTok{\textless{}{-}} \FunctionTok{read\_delim}\NormalTok{(}\AttributeTok{na =} \StringTok{"NA"}\NormalTok{, }\AttributeTok{delim =} \StringTok{";"}\NormalTok{, }\AttributeTok{col\_types =} \FunctionTok{cols}\NormalTok{(}\AttributeTok{col =} \FunctionTok{col\_character}\NormalTok{()), }\StringTok{"https://hranalytics.netlify.com/data/LTI Analysis Data.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Your incident and DART (Days Away, Restrictions and Transfers) rates are OSHA safety metrics that help you understand the effectiveness of your safety and risk management programs:
* Your incident rate shows the total number of OSHA-recordable injuries and illness per 200,000 hours. The rate is viewed as a percentage of injuries per 100 employees per year.
* Your DART rate calculates the rate of days away, restrictions and transfers per 200,000 worked.

Data dictionary

DEPT - The name of the department where employees were absent
BIWEEKLY\_PAY\_PERIOD\_END\_DATE = col\_character(),
LTI = col\_character(),
DART = col\_character(),
DESCRIPTION = col\_character(),
Number\_OF\_Incidents = col\_integer(),
HOURS\_YOY = col\_double(),
LTI\_YOY = col\_integer(),
DART\_YOY = col\_integer(),
OSHA\_YOY = col\_integer(),

LTI\_Rate = Lost Time Incident Rate: Total number of boxes checked in Column H X 200,000 / Total hours worked by all employees = OSHA Lost Time Incident Rate

DART\_Rate = DART Rate (Days Away Restricted Time); Total number of boxes checked in Columns H and I X 200,000 / Total hours worked by all employees = OSHA DART Rate

OSHA\_Rate = OSHA Incident Rate: Total number of boxes checked in Column G, H, I and J X 200,000 / Total hours worked by all employees = OSHA Lost Time Incident Rate

\hypertarget{attrition}{%
\chapter{Attrition}\label{attrition}}

Here we introduce attrition.

\hypertarget{interview-attendance}{%
\chapter{Interview attendance problem}\label{interview-attendance}}

The dataset consists of details of nore than 1200 candidates and the interviews they have attended during the course of the period 2014-2016.

The following are the variables columns:

\begin{itemize}
\tightlist
\item
  Date of Interview refers to the day the candidates were scheduled for the interview. The formats vary.
\item
  Client that gave the recruitment vendor the requisite mandate
\item
  Industry refers to the sector the client belongs to (Candidates can job hunt in vrious industries.)
\item
  Location refers to the current location of the candidate.
\item
  Position to be closed: Niche refers to rare skill sets, while routine refers to more common skill sets.
\item
  Nature of Skillset refers to the skill the client has and specifies the same.
\item
  Interview Type: There are three interview types:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi}.}
  \tightlist
  \item
    Walk in drives - These are unscheduled. Candidates are either contacted or they come to the interview on their own volition,
  \item
    Scheduled - Here the candidates profiles are screened by the client and subsequent to this, the vendor fixes an appointment between the client and the candidate.
  \item
    The third one is a scheduled walkin. Here the number of candidates is larger and the candidates are informed beforehand of a tentative date to ascertain their availability. The profiles are screened as in a scheduled interview. In a sense it bears features of both a walk-in and a scheduled interview.
  \end{enumerate}
\item
  Name( Cand ID) This is a substitute to keep the candidates identity a secret
\item
  Gender
\item
  Candidate Current Location
\item
  Candidate Job Location
\item
  Interview Venue
\item
  Candidate Native location
\item
  Have you obtained the necessary permission to start at the required time?
\item
  I hope there will be no unscheduled meetings.
\item
  Can I call you three hours before the interview and follow up on your attendance for the interview?
\item
  Can I have an alternative telephone number? I assure you that I will not trouble you too much.
\item
  Have you taken a printout of your updated resume? Have you read the JD and understood it?
\item
  Are you clear with the venue details and the landmark?
\item
  Has the call letter been shared
\item
  Expected Attendance: Whether the candidate was expected to attend the interview. Here the alternatives are yes, no or uncertain.
\item
  Observed Attendance: Whether the candidate attended the interview. This is binary and will form our dependent variable to be predicted.
\item
  Marital Status: Single or married.
\end{itemize}

Source: \url{https://www.kaggle.com/hugohk/learning-ml-with-caret}

\hypertarget{data-reading-1}{%
\section{Data reading}\label{data-reading-1}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{interview\_attendance }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"https://hranalyticslive.netlify.com/data/interview.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(interview\_attendance, }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 5 x 28
  `Date of Interv~ `Client name` Industry Location `Position to be~
  <chr>            <chr>         <chr>    <chr>    <chr>           
1 13.02.2015       Hospira       Pharmac~ Chennai  Production- Ste~
2 13.02.2015       Hospira       Pharmac~ Chennai  Production- Ste~
3 13.02.2015       Hospira       Pharmac~ Chennai  Production- Ste~
4 13.02.2015       Hospira       Pharmac~ Chennai  Production- Ste~
5 13.02.2015       Hospira       Pharmac~ Chennai  Production- Ste~
# ... with 23 more variables: `Nature of Skillset` <chr>, `Interview
#   Type` <chr>, `Name(Cand ID)` <chr>, Gender <chr>, `Candidate Current
#   Location` <chr>, `Candidate Job Location` <chr>, `Interview
#   Venue` <chr>, `Candidate Native location` <chr>, `Have you obtained
#   the necessary permission to start at the required time` <chr>, `Hope
#   there will be no unscheduled meetings` <chr>, `Can I Call you three
#   hours before the interview and follow up on your attendance for the
#   interview` <chr>, `Can I have an alternative number/ desk number. I
#   assure you that I will not trouble you too much` <chr>, `Have you
#   taken a printout of your updated resume. Have you read the JD and
#   understood the same` <chr>, `Are you clear with the venue details and
#   the landmark.` <chr>, `Has the call letter been shared` <chr>,
#   `Expected Attendance` <chr>, `Observed Attendance` <chr>, `Marital
#   Status` <chr>, X24 <lgl>, X25 <lgl>, X26 <lgl>, X27 <lgl>, X28 <lgl>
\end{verbatim}

\hypertarget{data-cleaning}{%
\section{Data cleaning}\label{data-cleaning}}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{interview\_attendance }\OtherTok{\textless{}{-}}\NormalTok{ interview\_attendance[}\SpecialCharTok{{-}}\DecValTok{1234}\NormalTok{,] }\CommentTok{\#remove the last row that contains only missing values}

\NormalTok{interview\_attendance}\SpecialCharTok{$}\NormalTok{X24   }\OtherTok{\textless{}{-}} \ConstantTok{NULL} \CommentTok{\#get rid of unnecesary columns on the right side}
\NormalTok{interview\_attendance}\SpecialCharTok{$}\NormalTok{X25 }\OtherTok{\textless{}{-}} \ConstantTok{NULL} \CommentTok{\#get rid of unnecesary columns on the right side}
\NormalTok{interview\_attendance}\SpecialCharTok{$}\NormalTok{X26 }\OtherTok{\textless{}{-}} \ConstantTok{NULL} \CommentTok{\#get rid of unnecesary columns on the right side}
\NormalTok{interview\_attendance}\SpecialCharTok{$}\NormalTok{X27 }\OtherTok{\textless{}{-}} \ConstantTok{NULL} \CommentTok{\#get rid of unnecesary columns on the right side}
\NormalTok{interview\_attendance}\SpecialCharTok{$}\NormalTok{X28 }\OtherTok{\textless{}{-}} \ConstantTok{NULL} \CommentTok{\#get rid of unnecesary columns on the right side}

\CommentTok{\# Here we create a vector with column titles}
\NormalTok{mycolnames }\OtherTok{\textless{}{-}}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}date\_of\_interview\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}client\_name\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}industry\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}location\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}position\textquotesingle{}}\NormalTok{,}
               \StringTok{\textquotesingle{}skillset\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}interview\_type\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}name\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}gender\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}current\_location\textquotesingle{}}\NormalTok{,}
               \StringTok{\textquotesingle{}cjob\_location\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}interview\_venue\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}cnative\_location\textquotesingle{}}\NormalTok{,}
               \StringTok{\textquotesingle{}permission\_obtained\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}unscheduled\_meetings\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}call\_three\_hours\_before\textquotesingle{}}\NormalTok{,}
               \StringTok{\textquotesingle{}alternative\_number\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}printout\_resume\_jd\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}clear\_with\_venue\textquotesingle{}}\NormalTok{,}
               \StringTok{\textquotesingle{}letter\_been\_shared\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}expected\_attendance\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}observed\_attendance\textquotesingle{}}\NormalTok{,}
               \StringTok{\textquotesingle{}marital\_status\textquotesingle{}}\NormalTok{)  }

\CommentTok{\#Here we assign the previously defined column titles}
\FunctionTok{colnames}\NormalTok{(interview\_attendance) }\OtherTok{\textless{}{-}}\NormalTok{ mycolnames }

\FunctionTok{rm}\NormalTok{(mycolnames) }\CommentTok{\# Here we remove the mycolnames vector, as it is not required anymore}

\NormalTok{interview\_attendance}\OtherTok{\textless{}{-}} \FunctionTok{mutate\_all}\NormalTok{(interview\_attendance, }\FunctionTok{funs}\NormalTok{(tolower)) }\CommentTok{\# Sets all words to lower case}

\CommentTok{\# Cancels all empty spaces in the observed attendance column}
\NormalTok{interview\_attendance}\SpecialCharTok{$}\NormalTok{observed\_attendance }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{" "}\NormalTok{, }\StringTok{""}\NormalTok{, interview\_attendance}\SpecialCharTok{$}\NormalTok{observed\_attendance)}

\CommentTok{\# Cancels all empty spaces in the location column}
\NormalTok{interview\_attendance}\SpecialCharTok{$}\NormalTok{location }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{" "}\NormalTok{, }\StringTok{""}\NormalTok{, interview\_attendance}\SpecialCharTok{$}\NormalTok{location) }

\CommentTok{\# Cancels all empty spaces in the interview type column}
\NormalTok{interview\_attendance}\SpecialCharTok{$}\NormalTok{interview\_type }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{" "}\NormalTok{, }\StringTok{""}\NormalTok{, interview\_attendance}\SpecialCharTok{$}\NormalTok{interview\_type)}

\CommentTok{\# Corrects a typo in the interview type column}
\NormalTok{interview\_attendance}\SpecialCharTok{$}\NormalTok{interview\_type }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{"sceduledwalkin"}\NormalTok{, }\StringTok{"scheduledwalkin"}\NormalTok{, interview\_attendance}\SpecialCharTok{$}\NormalTok{interview\_type)}

\CommentTok{\# Cancels all empty spaces in the candidate current location column}
\NormalTok{interview\_attendance}\SpecialCharTok{$}\NormalTok{current\_location }\OtherTok{\textless{}{-}} \FunctionTok{gsub}\NormalTok{(}\StringTok{" "}\NormalTok{, }\StringTok{""}\NormalTok{, interview\_attendance}\SpecialCharTok{$}\NormalTok{current\_location)}


\CommentTok{\#Converts values from character to numbers for Yes/no answers, just to keep things simple.}
\NormalTok{  colstoyesno }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{14}\SpecialCharTok{:}\DecValTok{22}\NormalTok{) }\CommentTok{\# Here we define which column numbers to look at.}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(colstoyesno))\{   }\CommentTok{\# Here we tell R to examine all variables in the previously defined columns}
\NormalTok{  j }\OtherTok{\textless{}{-}}\NormalTok{ colstoyesno[i]}
\NormalTok{  interview\_attendance[,j][interview\_attendance[,j] }\SpecialCharTok{!=}\StringTok{"yes"}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"no"}   
\NormalTok{  interview\_attendance[,j][}\FunctionTok{is.na}\NormalTok{(interview\_attendance[,j]) }\SpecialCharTok{==} \ConstantTok{TRUE}\NormalTok{] }\OtherTok{\textless{}{-}} \StringTok{"no"}
  \CommentTok{\#With the previous two lines all values different to yes, become a no, i.e. "uncertain" and "NA" are set to a "no".}
\NormalTok{\}}
\FunctionTok{rm}\NormalTok{(colstoyesno, i, j) }\CommentTok{\#Here we remove the three just created vectors as a claen up.}
\end{Highlighting}
\end{Shaded}

In the following step we figure out what is the content of each relevant column and identify its unique values. Once we have done that a number is assigned for a later step.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dir.create}\NormalTok{(}\StringTok{"codefiles\_interview\_attendance"}\NormalTok{, }\AttributeTok{showWarnings =} \ConstantTok{FALSE}\NormalTok{)}
\CommentTok{\#detach("package:plyr", unload = TRUE)}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(}\FunctionTok{colnames}\NormalTok{(interview\_attendance)))\{}
\NormalTok{  vvar }\OtherTok{\textless{}{-}} \FunctionTok{colnames}\NormalTok{(interview\_attendance)[i]}
\NormalTok{  outdata }\OtherTok{\textless{}{-}}\NormalTok{ interview\_attendance }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{group\_by}\NormalTok{(}\AttributeTok{.dots =}\NormalTok{ vvar) }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{count}\NormalTok{(}\AttributeTok{.dots =}\NormalTok{ vvar)}
\NormalTok{  outdata}\SpecialCharTok{$}\NormalTok{idt }\OtherTok{\textless{}{-}}\NormalTok{ LETTERS[}\FunctionTok{seq}\NormalTok{(}\AttributeTok{from=}\DecValTok{1}\NormalTok{, }\AttributeTok{to=}\FunctionTok{nrow}\NormalTok{(outdata))]}
\NormalTok{  outdata}\SpecialCharTok{$}\NormalTok{id  }\OtherTok{\textless{}{-}} \FunctionTok{row.names}\NormalTok{(outdata)}
\NormalTok{  outfile }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\StringTok{"codefiles\_interview\_attendance/"}\NormalTok{, vvar, }\StringTok{".csv"}\NormalTok{)}
  \FunctionTok{write.csv}\NormalTok{(outdata, }\AttributeTok{file=}\NormalTok{outfile, }\AttributeTok{row.names =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{\}}
\FunctionTok{rm}\NormalTok{(outdata, i, outfile, vvar)}
\end{Highlighting}
\end{Shaded}

This step uses the csv files created in the previous step and replaces the words with a number (except for the variable that we would like to predict) in order to prepare the data for machine learning.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{colstomap }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\SpecialCharTok{:}\DecValTok{7}\NormalTok{, }\DecValTok{9}\SpecialCharTok{:}\DecValTok{21}\NormalTok{, }\DecValTok{23}\NormalTok{)}
\FunctionTok{library}\NormalTok{(plyr)}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(colstomap))\{}
\NormalTok{    j }\OtherTok{\textless{}{-}}\NormalTok{ colstomap[i]}
\NormalTok{    vfilename }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{(}\StringTok{"codefiles\_interview\_attendance/"}\NormalTok{, }\FunctionTok{colnames}\NormalTok{(interview\_attendance)[j], }\StringTok{".csv"}\NormalTok{)}
\NormalTok{    dfcodes }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(vfilename, }\AttributeTok{stringsAsFactors=}\ConstantTok{FALSE}\NormalTok{)}
\NormalTok{    vfrom }\OtherTok{\textless{}{-}} \FunctionTok{as.vector}\NormalTok{(dfcodes[,}\DecValTok{1}\NormalTok{])}
\NormalTok{    vto }\OtherTok{\textless{}{-}} \FunctionTok{as.vector}\NormalTok{(dfcodes[,}\DecValTok{4}\NormalTok{])}
\NormalTok{    interview\_attendance[,j] }\OtherTok{\textless{}{-}} \FunctionTok{mapvalues}\NormalTok{(interview\_attendance[,j], }\AttributeTok{from=}\NormalTok{vfrom, }\AttributeTok{to=}\NormalTok{vto)}
\NormalTok{    interview\_attendance[,j] }\OtherTok{\textless{}{-}} \FunctionTok{as.integer}\NormalTok{(interview\_attendance[,j])}
\NormalTok{\}}
\FunctionTok{rm}\NormalTok{(colstomap, i, j, vfilename, vfrom, vto, dfcodes)}
\end{Highlighting}
\end{Shaded}

Here we pick the data that are needed for the predictor.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{interview\_attendanceml }\OtherTok{\textless{}{-}}\NormalTok{ interview\_attendance }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{date\_of\_interview, }\SpecialCharTok{{-}}\NormalTok{name)}
\NormalTok{interview\_attendanceml }\OtherTok{\textless{}{-}}\NormalTok{ interview\_attendanceml }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(client\_name}\SpecialCharTok{:}\NormalTok{expected\_attendance, }
\NormalTok{                                observed\_attendance)}
\FunctionTok{head}\NormalTok{(interview\_attendanceml, }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Here we start the real machine learning part. The training dataset is created containing 75\% of the observations {[}interview\_attendanceml\_train{]} and the remaining observations are assigned to the test dataset {[}interview\_attendanceml\_test{]}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(caret) }\CommentTok{\# Calls the caret library}

\FunctionTok{set.seed}\NormalTok{(}\DecValTok{144}\NormalTok{) }\CommentTok{\# Sets a seed for reproducability}

\NormalTok{index }\OtherTok{\textless{}{-}} \FunctionTok{createDataPartition}\NormalTok{(interview\_attendanceml}\SpecialCharTok{$}\NormalTok{observed\_attendance, }\AttributeTok{p=}\FloatTok{0.75}\NormalTok{, }\AttributeTok{list=}\ConstantTok{FALSE}\NormalTok{) }\CommentTok{\#Creates an index vector of the length of all the observations}

\NormalTok{interview\_attendanceml\_train }\OtherTok{\textless{}{-}}\NormalTok{ interview\_attendanceml[index,] }\CommentTok{\# Creates a subset of 75\% of data for training dataset}
\NormalTok{interview\_attendanceml\_test  }\OtherTok{\textless{}{-}}\NormalTok{ interview\_attendanceml[}\SpecialCharTok{{-}}\NormalTok{index,] }\CommentTok{\# Creates a subset of the remaining 25\% of data for test dataset}

\FunctionTok{rm}\NormalTok{(index,interview\_attendanceml)}
\end{Highlighting}
\end{Shaded}

\hypertarget{choosing-a-model}{%
\section{Choosing a model}\label{choosing-a-model}}

We didn't want to tune anything and just let the code figure it out. Since the output is 1 for a no show and 2 for a show, we decided to use the gbm method (generalized boosting machine) from caret for creating the algorithm.

\hypertarget{training}{%
\section{Training}\label{training}}

We use the train function of the caret library to train the training data set determined in the previous step.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{myml\_model }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(interview\_attendanceml\_train[,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{19}\NormalTok{], interview\_attendanceml\_train[,}\DecValTok{20}\NormalTok{], }\AttributeTok{method=}\StringTok{\textquotesingle{}gbm\textquotesingle{}}\NormalTok{)}

\FunctionTok{summary}\NormalTok{(myml\_model)}

\NormalTok{predictions }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(}\AttributeTok{object =}\NormalTok{ myml\_model, interview\_attendanceml\_test,}
                       \AttributeTok{type =} \StringTok{\textquotesingle{}raw\textquotesingle{}}\NormalTok{)}

\FunctionTok{head}\NormalTok{(predictions)}

\FunctionTok{print}\NormalTok{(}\FunctionTok{postResample}\NormalTok{(}\AttributeTok{pred=}\NormalTok{predictions, }\AttributeTok{obs=}\FunctionTok{as.factor}\NormalTok{(interview\_attendanceml\_test[,}\DecValTok{20}\NormalTok{])))}
\end{Highlighting}
\end{Shaded}

\hypertarget{ranking-medical-school}{%
\chapter{Ranking Medical Schools}\label{ranking-medical-school}}

In this module, we will do another type of web scraping, this time we scrape from a pdf document. We will analyse the results of the ``épreuves classantes nationales (ECN)'', which is a nationwide competitive examination at the end of the 6th year of medical schools in France. The candidates ranking first can choose first where they want to continue their medical training. It is a very clean dataset in pdf format containing the results of 8370 medical school examns for a total of 124 pages.

Source code: \url{https://privefl.github.io/blog/scraping-some-french-medical-school-rankings/}

Source pdf document: \url{http://www.remede.org/documents/IMG/pdf/liste_classement_ecn_20170628.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(pdftools)}
\FunctionTok{library}\NormalTok{(data.table)}
\FunctionTok{library}\NormalTok{(bigstatsr)}
\FunctionTok{library}\NormalTok{(plotly)}
\FunctionTok{library}\NormalTok{(stringr)}
\CommentTok{\#library(gsubfn)}
\end{Highlighting}
\end{Shaded}

First, let's get some data from our service desk by exporting a CSV. We can then read this CSV (or excel spreadsheet) into R for us to perform analysis.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{Sys.setenv}\NormalTok{(}\AttributeTok{TZ =} \StringTok{"Europe/London"}\NormalTok{)}
\FunctionTok{Sys.setlocale}\NormalTok{(}\AttributeTok{locale=}\StringTok{"fr\_FR.UTF{-}8"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] ""
\end{verbatim}

We will use package pdftools to get the text from the pdf document. to be on the safe side, I have dowloaded the document already, but it is also possible to dowload it afresh from the internet.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#pdfdocument \textless{}{-} "https://goo.gl/wUXvjk" \#Internet download}
\CommentTok{\#pdfdocument \textless{}{-} "https://hranalyticslive.netlify.com/data/liste\_classement\_ecn\_20170628.pdf"}
\end{Highlighting}
\end{Shaded}

gsub

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{txt }\OtherTok{\textless{}{-}}\NormalTok{ pdftools}\SpecialCharTok{::}\FunctionTok{pdf\_text}\NormalTok{(pdfdocument)}
\FunctionTok{head}\NormalTok{(txt, }\AttributeTok{n =} \DecValTok{1}\NormalTok{) }\CommentTok{\#Inspection of first page}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "                                                                              Paris, le 28 juin 2017\r\n     Liste des étudiants et des internes de médecine, classés par ordre de\r\n     mérite, ayant satisfait aux épreuves classantes nationales anonymes\r\n    donnant accès au troisième cycle des études médicales, organisées au\r\n                            titre de l’année universitaire 2017-2018.\r\n        Nota : Il est demandé aux étudiants de vérifier les informations d’état civil les\r\n  concernant et d’envoyer à la gestionnaire des ECN la copie d’une pièce d’identité pour\r\n  prise en compte des modifications. Ces corrections sont importantes avant parution de\r\n                      cette liste au Journal officiel de la République française.\r\n0001 Mme Beaumont (Anne-Lise), née le 1 septembre 1993.\r\n0002 M. Petitdemange (Arthur, Paul, Joseph), né le 15 septembre 1993.\r\n0003 M. Bacon (Seraphin, Charly, Philippe), né le 29 janvier 1993.\r\n0004 M. Ditac (Geoffroy, Arnaud, André), né le 23 juin 1994.\r\n0005 M. Faure (Guillaume, Thomas), né le 7 août 1992.\r\n0006 Mme Weil (Amandine), née le 4 mars 1992.\r\n0007 M. Ezzouhairi (Nacim), né le 1 juin 1993.\r\n0008 M. Coulon (Antoine), né le 11 juin 1992.\r\n0009 Mme Le Gaudu (Violette, Luce, Catherine), née le 3 juin 1994.\r\n0010 M. Boyer (Jeremy), né le 14 mai 1993.\r\n0011 M. Villemaire (Axel, Michaël, Pierre), né le 24 juillet 1991.\r\n0012 M. Azoulay (Levi-Dan), né le 13 décembre 1993.\r\n0013 M. Assouline (Allan), né le 14 janvier 1994.\r\n0014 M. Rouchaud (Aymeric), né le 16 mai 1993.\r\n0015 M. Padden (Michael, James), né le 6 février 1993.\r\n0016 M. Gavoille (Antoine, Paul, Jean), né le 12 juillet 1994.\r\n0017 M. Marie (Benjamin, Pierre, Alexandre), né le 14 mars 1994.\r\n0018 Mlle Boulle (Charlotte, Marie, Cécile), née le 3 septembre 1988.\r\n0019 Mme Chatelain (Juliette), née le 23 juillet 1993.\r\n0020 Mme Laporte (Amandine, Capucine), née le 9 octobre 1993.\r\n0021 M. D'izarny Gargas (Thibaut, François, Arnaud), né le 2 mai 1993.\r\n0022 Mme Boccon Gibod (Clémentine, Raphaëlle), née le 24 avril 1993.\r\n0023 Mme Chan (Camille, Marie), née le 11 juillet 1990.\r\n0024 M. Lemasle (Aymeric), né le 7 avril 1994.\r\n0025 M. Sulman (David), né le 9 novembre 1992.\r\n0026 Mlle Fresnel (Clémentine), née le 5 mars 1992.\r\n0027 M. Dumortier (Pierre, Antoine, Frédéric), né le 19 juillet 1993.\r\n0028 Mme Torres-Villaros (Héloïse, Laure, Barbara), née le 18 septembre 1992.\r\n0029 M. Memmi (Benjamin), né le 3 décembre 1993.\r\n0030 Mme Kherabi (Yousra), née le 23 décembre 1993.\r\n                                                              1\r\n"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{strsplit}\NormalTok{(txt, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}

\FunctionTok{head}\NormalTok{(data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[1]]
 [1] "                                                                              Paris, le 28 juin 2017\r"
 [2] "     Liste des étudiants et des internes de médecine, classés par ordre de\r"                          
 [3] "     mérite, ayant satisfait aux épreuves classantes nationales anonymes\r"                            
 [4] "    donnant accès au troisième cycle des études médicales, organisées au\r"                            
 [5] "                            titre de l’année universitaire 2017-2018.\r"                               
 [6] "        Nota : Il est demandé aux étudiants de vérifier les informations d’état civil les\r"           
 [7] "  concernant et d’envoyer à la gestionnaire des ECN la copie d’une pièce d’identité pour\r"            
 [8] "  prise en compte des modifications. Ces corrections sont importantes avant parution de\r"             
 [9] "                      cette liste au Journal officiel de la République française.\r"                   
[10] "0001 Mme Beaumont (Anne-Lise), née le 1 septembre 1993.\r"                                             
[11] "0002 M. Petitdemange (Arthur, Paul, Joseph), né le 15 septembre 1993.\r"                               
[12] "0003 M. Bacon (Seraphin, Charly, Philippe), né le 29 janvier 1993.\r"                                  
[13] "0004 M. Ditac (Geoffroy, Arnaud, André), né le 23 juin 1994.\r"                                        
[14] "0005 M. Faure (Guillaume, Thomas), né le 7 août 1992.\r"                                               
[15] "0006 Mme Weil (Amandine), née le 4 mars 1992.\r"                                                       
[16] "0007 M. Ezzouhairi (Nacim), né le 1 juin 1993.\r"                                                      
[17] "0008 M. Coulon (Antoine), né le 11 juin 1992.\r"                                                       
[18] "0009 Mme Le Gaudu (Violette, Luce, Catherine), née le 3 juin 1994.\r"                                  
[19] "0010 M. Boyer (Jeremy), né le 14 mai 1993.\r"                                                          
[20] "0011 M. Villemaire (Axel, Michaël, Pierre), né le 24 juillet 1991.\r"                                  
[21] "0012 M. Azoulay (Levi-Dan), né le 13 décembre 1993.\r"                                                 
[22] "0013 M. Assouline (Allan), né le 14 janvier 1994.\r"                                                   
[23] "0014 M. Rouchaud (Aymeric), né le 16 mai 1993.\r"                                                      
[24] "0015 M. Padden (Michael, James), né le 6 février 1993.\r"                                              
[25] "0016 M. Gavoille (Antoine, Paul, Jean), né le 12 juillet 1994.\r"                                      
[26] "0017 M. Marie (Benjamin, Pierre, Alexandre), né le 14 mars 1994.\r"                                    
[27] "0018 Mlle Boulle (Charlotte, Marie, Cécile), née le 3 septembre 1988.\r"                               
[28] "0019 Mme Chatelain (Juliette), née le 23 juillet 1993.\r"                                              
[29] "0020 Mme Laporte (Amandine, Capucine), née le 9 octobre 1993.\r"                                       
[30] "0021 M. D'izarny Gargas (Thibaut, François, Arnaud), né le 2 mai 1993.\r"                              
[31] "0022 Mme Boccon Gibod (Clémentine, Raphaëlle), née le 24 avril 1993.\r"                                
[32] "0023 Mme Chan (Camille, Marie), née le 11 juillet 1990.\r"                                             
[33] "0024 M. Lemasle (Aymeric), né le 7 avril 1994.\r"                                                      
[34] "0025 M. Sulman (David), né le 9 novembre 1992.\r"                                                      
[35] "0026 Mlle Fresnel (Clémentine), née le 5 mars 1992.\r"                                                 
[36] "0027 M. Dumortier (Pierre, Antoine, Frédéric), né le 19 juillet 1993.\r"                               
[37] "0028 Mme Torres-Villaros (Héloïse, Laure, Barbara), née le 18 septembre 1992.\r"                       
[38] "0029 M. Memmi (Benjamin), né le 3 décembre 1993.\r"                                                    
[39] "0030 Mme Kherabi (Yousra), née le 23 décembre 1993.\r"                                                 
[40] "                                                              1\r"                                     

[[2]]
 [1] "0031 Mme Bourhis (Amélie, Marie-Sophie), née le 20 décembre 1993.\r"                    
 [2] "0032 M. Montardi (Camille, Raphaël), né le 22 février 1994.\r"                          
 [3] "0033 Mme Tian (Yuan), née le 28 juillet 1993.\r"                                        
 [4] "0034 M. Dighiero Brecht (Thomas, Pablo), né le 7 septembre 1993.\r"                     
 [5] "0035 M. Perraud (Ludovic), né le 23 mars 1993.\r"                                       
 [6] "0036 M. Roussotte (Mickaël, Jean, Alain), né le 9 mars 1993.\r"                         
 [7] "0037 M. Hoellinger (Baptiste, André, Joseph), né le 9 juillet 1993.\r"                  
 [8] "0038 M. Alloune (Rayan), né le 25 mars 1993.\r"                                         
 [9] "0039 M. Mesny (Emmanuel, André), né le 7 mai 1993.\r"                                   
[10] "0040 M. Lognon (Pierre, Antoine), né le 8 juin 1993.\r"                                 
[11] "0041 Mme Tuaz (Estelle), née le 27 juin 1992.\r"                                        
[12] "0042 M. Roulleaux Dugage (Matthieu, Marie), né le 8 mars 1993.\r"                       
[13] "0043 M. Hanotin (Clément), né le 14 janvier 1992.\r"                                    
[14] "0044 M. Hage (Alexandre), né le 22 juillet 1993.\r"                                     
[15] "0045 M. Robbe (Guillaume), né le 17 février 1994.\r"                                    
[16] "0046 M. Derot (Simon, Robin, François), né le 3 juin 1993.\r"                           
[17] "0047 M. Finsterbach (Sonny, Philippe, Arsène), né le 28 janvier 1993.\r"                
[18] "0048 M. Niddam (Samuel, Abraham, Henri), né le 17 août 1993.\r"                         
[19] "0049 Mme Hertzog (Tatiana, Marine), née le 28 février 1993.\r"                          
[20] "0050 Mlle Liu (Alice), née le 12 janvier 1994.\r"                                       
[21] "0051 Mme Landré (Sophie, Anaïs), née le 17 février 1993.\r"                             
[22] "0052 M. Perol (Louis), né le 19 janvier 1990.\r"                                        
[23] "0053 M. Cavaillez (Thibaud, Antoine), né le 2 janvier 1993.\r"                          
[24] "0054 M. Loeb (Jules), né le 25 juillet 1993.\r"                                         
[25] "0055 Mme Bonello (Kim), née le 12 février 1992.\r"                                      
[26] "0056 Mme Agouzoul (Sara), née le 5 avril 1993.\r"                                       
[27] "0057 M. Blanquart (Erwan, Jean, Jules), né le 28 avril 1993.\r"                         
[28] "0058 Mme Forte (Valentine), née le 9 mai 1993.\r"                                       
[29] "0059 Mlle Wilkin (Marie), née le 24 juillet 1993.\r"                                    
[30] "0060 Mme Vignals (Carole), née le 8 octobre 1993.\r"                                    
[31] "0061 M. Lampros (Alexandre), né le 18 novembre 1993.\r"                                 
[32] "0062 M. Boulanger (Etienne, Marcel, Francis), né le 27 janvier 1994.\r"                 
[33] "0063 Mme Kouki (Inès), née le 30 juin 1994.\r"                                          
[34] "0064 M. Bourreau (Alexis, François), né le 10 mai 1993.\r"                              
[35] "0065 M. Riller (Quentin, Marcel), né le 28 mars 1992.\r"                                
[36] "0066 M. Lefebvre (Erwan, Paul, Jacques), né le 16 octobre 1993.\r"                      
[37] "0067 Mlle Barré (Mathilde, Marie-Hélène), née le 12 mai 1992.\r"                        
[38] "0068 M. Orcel (Thibaud, Charles, Minh), né le 12 décembre 1994.\r"                      
[39] "0069 M. Wajchert (Thibaut, Pierre, Jean), né le 3 mars 1993.\r"                         
[40] "0070 M. Duteau (Vincent, Simon), né le 7 mars 1994.\r"                                  
[41] "0071 Mme Jenvrin (Anaïs, Louisa, Léonie), née le 28 juillet 1993.\r"                    
[42] "0072 M. Gdalia (Raphaël, Félix, David), né le 8 juillet 1993.\r"                        
[43] "0073 M. Bouvarel (Hugo, Maurice, Marcel), né le 19 avril 1994.\r"                       
[44] "0074 M. Borouchaki (Antoine), né le 11 mai 1994.\r"                                     
[45] "0075 M. Fawaz (Sami), né le 10 décembre 1993.\r"                                        
[46] "0076 M. Zarka (Jonathan, Sacha, Menahem), né le 3 juillet 1991.\r"                      
[47] "0077 M. Creon (Antoine, Louis, Aurèle), né le 30 novembre 1992.\r"                      
[48] "0078 M. Chirpaz (Nicolas, Loïc), né le 31 décembre 1993.\r"                             
[49] "0079 M. Sakhi (Hichem), né le 10 octobre 1993.\r"                                       
[50] "0080 Mlle Carayol (Ariane, Louise), née le 30 avril 1993.\r"                            
[51] "0081 Mme Thirouin (Jeanne, Marthe), née le 24 janvier 1995.\r"                          
[52] "0082 Mme Chastagner (Marine, Nathalie), née le 10 juin 1993.\r"                         
[53] "0083 Mme Roux (Camille, Madeleine, Hélène), née le 20 février 1993.\r"                  
[54] "0084 Mme Auger (Raphaëlle, Aurore), née le 1 octobre 1993.\r"                           
[55] "0085 M. Mauny (Philippe), né le 14 juillet 1983.\r"                                     
[56] "0086 Mlle Even (Cecilia, Sophie, Ginette), née le 28 juin 1990.\r"                      
[57] "0087 Mme Lafargue (Marie-Camille), née le 15 octobre 1993.\r"                           
[58] "0088 M. De Guilhem De Lataillade (Adrien, Vincent, Jean-Marie), né le 7 février 1992.\r"
[59] "0089 M. Beneyto (Maxime), né le 1 juin 1994.\r"                                         
[60] "0090 M. Ruiz (Thibault), né le 21 décembre 1992.\r"                                     
[61] "0091 Mme Pietquin (Sophie), née le 6 novembre 1992.\r"                                  
[62] "0092 M. Barde (François), né le 16 janvier 1993.\r"                                     
[63] "0093 M. Pham (Félix, Khoa), né le 2 septembre 1993.\r"                                  
[64] "0094 M. Fajardie (Antoine), né le 11 décembre 1994.\r"                                  
[65] "0095 Mme Bouchard (Pauline, Marie, Charlotte), née le 25 juin 1993.\r"                  
[66] "0096 Mme Desjonqueres (Elvire), née le 22 février 1993.\r"                              
[67] "0097 Mme Mounacq (Audrey), née le 5 septembre 1993.\r"                                  
[68] "0098 Mme Cachera (Laurène, Elise, Renée), née le 27 septembre 1993.\r"                  
[69] "                                                              2\r"                      

[[3]]
 [1] "0099 Mme Meynard (Lucie, Magdeleine, Marcelle), née le 9 août 1994.\r"                       
 [2] "0100 M. Grimault (Dimitri, Olivier), né le 10 août 1992.\r"                                  
 [3] "0101 M. Garric (Antoine, Henri), né le 27 septembre 1993.\r"                                 
 [4] "0102 M. Colas (Quentin, Joël), né le 18 mars 1993.\r"                                        
 [5] "0103 M. Martin De Fremont (Grégoire, Marie), né le 1 septembre 1993.\r"                      
 [6] "0104 M. Belkouche (Alban, Larbi), né le 28 décembre 1993.\r"                                 
 [7] "0105 M. Khitri (Mohammed Yacine), né le 15 novembre 1992.\r"                                 
 [8] "0106 M. Michelin (Bastien), né le 1 novembre 1992.\r"                                        
 [9] "0107 M. David-Muller (Marc), né le 6 juillet 1993.\r"                                        
[10] "0108 Mme Lombes (Amélie), née le 15 décembre 1992.\r"                                        
[11] "0109 Mme Costes (Domitille), née le 5 juillet 1993.\r"                                       
[12] "0110 M. Boulet (Nicolas, Hugo), né le 10 novembre 1993.\r"                                   
[13] "0111 Mme Rambaud (Elise), née le 11 décembre 1993.\r"                                        
[14] "0112 M. Prud'homme (Léo), né le 10 octobre 1992.\r"                                          
[15] "0113 M. Brizard (Antoine), né le 12 décembre 1993.\r"                                        
[16] "0114 Mme Hoppenot (Bérénice, Marie-Brigitte, Christianne), née le 15 juillet 1993.\r"        
[17] "0115 M. Sauer (François, Louis), né le 5 octobre 1993.\r"                                    
[18] "0116 M. Mazhar (Driss, Olivier), né le 21 mai 1993.\r"                                       
[19] "0117 M. Roumeau (Sébastien, Paul, André), né le 11 juin 1993.\r"                             
[20] "0118 M. Azoyan (Loris), né le 25 janvier 1993.\r"                                            
[21] "0119 Mme Drillet (Gaëlle), née le 26 juin 1993.\r"                                           
[22] "0120 M. Galland (Loïck, Philippe), né le 8 août 1993.\r"                                     
[23] "0121 M. Frey (Samuel, Léon, Richard), né le 27 juin 1992.\r"                                 
[24] "0122 Mlle Mausoléo (Aude, Emilie, Marie), née le 2 avril 1993.\r"                            
[25] "0123 M. Trimaille (Antonin), né le 9 janvier 1995.\r"                                        
[26] "0124 Mme Gonsard (Apolline), née le 16 octobre 1993.\r"                                      
[27] "0125 M. Gressens (Simon), né le 26 août 1992.\r"                                             
[28] "0126 Mme Manse (Léa, Muriel), née le 9 juillet 1993.\r"                                      
[29] "0127 Mme Guilloit (Azalais, Bernadette, Solange), née le 2 novembre 1993.\r"                 
[30] "0128 M. Renault (Valentin), né le 24 avril 1994.\r"                                          
[31] "0129 Mme Berardi (Giulia), née le 17 septembre 1993.\r"                                      
[32] "0130 M. Berdugo (Kevin), né le 30 juin 1992.\r"                                              
[33] "0131 M. Fourdinier (Victor, Philippe, Thierry), né le 15 décembre 1993.\r"                   
[34] "0132 Mme Pina (Héloïse, Marceline), née le 28 mai 1994.\r"                                   
[35] "0133 M. Marchiset (Antoine), né le 13 mai 1992.\r"                                           
[36] "0134 M. Monnin (Charles, Germain, Claude), né le 1 avril 1993.\r"                            
[37] "0135 M. Briens (Aurélien), né le 15 juillet 1989.\r"                                         
[38] "0136 M. Ruiz (François, Jean-Claude), né le 19 décembre 1993.\r"                             
[39] "0137 M. Hamon (Antoine), né le 30 novembre 1993.\r"                                          
[40] "0138 M. Delange (Boris, Romain), né le 16 novembre 1992.\r"                                  
[41] "0139 M. Carval (Thibaut, Benoît, François), né le 1 décembre 1993.\r"                        
[42] "0140 Mlle Bach (Emma, Marie, Catherine), née le 29 novembre 1993.\r"                         
[43] "0141 M. Babin (Matthias), né le 3 octobre 1992.\r"                                           
[44] "0142 Mme Doira (Bilitis, Adelaïde), née le 14 août 1993.\r"                                  
[45] "0143 M. Hilezian (Frédéric, Clément, Marc), né le 13 décembre 1994.\r"                       
[46] "0144 Mme Lajaunie (Rebecca, Sarah), née le 14 juillet 1990.\r"                               
[47] "0145 M. Le Carpentier (Aymeric, Jean-Claude, Jules), né le 24 août 1994.\r"                  
[48] "0146 M. Le Boite (Hugo, Charles, Louis), né le 30 mars 1992.\r"                              
[49] "0147 M. Lombardi (Yannis, Jérémie), né le 3 septembre 1993.\r"                               
[50] "0148 Mme Lasvergnas (Julie, Marie, Jeanne), née le 30 juillet 1994.\r"                       
[51] "0149 Mlle O'keane (Aurélie, Deirdre, Nolwenn), née le 27 décembre 1993.\r"                   
[52] "0150 M. Letellier (Thibault, Jean-Claude, René), né le 29 août 1993.\r"                      
[53] "0151 Mme Dudok De Wit (Sarah), née le 8 août 1992.\r"                                        
[54] "0152 M. Olory (Léo, Bruno, Claude), né le 30 juillet 1992.\r"                                
[55] "0153 Mme Vergneault (Hélène, Sarah, Micheline), née le 11 novembre 1993.\r"                  
[56] "0154 M. Le Breton (Guillaume), né le 11 octobre 1993.\r"                                     
[57] "0155 Mme Ortoli (Manon), née le 30 octobre 1993.\r"                                          
[58] "0156 Mme Schoch (Armelle, Christine, Dominique), née le 17 juillet 1992.\r"                  
[59] "0157 Mme Traineau (Hélène, Anne, Odile), née le 17 décembre 1992.\r"                         
[60] "0158 Mme Tessier (Dolores, Céline, Patrick), épouse Jean-Baptiste, née le 29 janvier 1993.\r"
[61] "0159 M. Touboul (Arnaud), né le 23 septembre 1993.\r"                                        
[62] "0160 M. Mairot (Kevin), né le 16 mai 1993.\r"                                                
[63] "0161 M. El Bèze (Nathan, Gabriel), né le 26 avril 1993.\r"                                   
[64] "0162 M. Khansa (Rémi), né le 13 octobre 1992.\r"                                             
[65] "0163 M. Temmar (Yassine), né le 28 septembre 1992.\r"                                        
[66] "0164 M. Legghe (Benoît, Charles, Pierre), né le 9 juillet 1993.\r"                           
[67] "0165 M. Fournier (Nicolas, Julien, Henri), né le 9 septembre 1977.\r"                        
[68] "0166 Mme Lasbleiz (Adèle), née le 7 janvier 1993.\r"                                         
[69] "                                                             3\r"                            

[[4]]
 [1] "0167 M. Orion (Maxime), né le 20 novembre 1993.\r"                                                        
 [2] "0168 Mlle Petit (Clémence, Annie, Alice), née le 27 avril 1993.\r"                                        
 [3] "0169 Mme Brunac (Anne Cecile), née le 20 mars 1993.\r"                                                    
 [4] "0170 M. Laydevant (William), né le 21 septembre 1992.\r"                                                  
 [5] "0171 M. Belkaaloul (Yassine), né le 13 juin 1992.\r"                                                      
 [6] "0172 Mme Le Bras (Pierrine), née le 10 mars 1993.\r"                                                      
 [7] "0173 M. Nicolas (Louis), né le 28 novembre 1992.\r"                                                       
 [8] "0174 M. Chebib (Emilien), né le 20 décembre 1992.\r"                                                      
 [9] "0175 M. Galliot (Gaël, Michel, Paul), né le 1 octobre 1993.\r"                                            
[10] "0176 Mme Raia (Lisa), née le 29 octobre 1993.\r"                                                          
[11] "0177 Mme Noroy (Louise), née le 20 juillet 1993.\r"                                                       
[12] "0178 Mlle Fabbri (Camille, Marie), née le 4 septembre 1993.\r"                                            
[13] "0179 M. Thizy (Guillaume, Jean), né le 4 octobre 1993.\r"                                                 
[14] "0180 Mme Bougault (Mathilde), née le 29 janvier 1993.\r"                                                  
[15] "0181 Mlle Fedi (Mathilde), née le 23 juillet 1994.\r"                                                     
[16] "0182 Mme Allou (Violaine), née le 12 janvier 1994.\r"                                                     
[17] "0183 M. Richaud (Rémi, Jean-Claude), né le 18 juillet 1988.\r"                                            
[18] "0184 Mme Messmer (Elodie, Sandrine, Yvette), née le 15 novembre 1993.\r"                                  
[19] "0185 Mlle Patchinsky (Alexandra, Nathalie, Frédérique), née le 14 avril 1993.\r"                          
[20] "0186 Mme Archer (Gabrielle), née le 20 août 1994.\r"                                                      
[21] "0187 M. Martin-Champetier (Antoine, Jean, Arnaud), né le 20 septembre 1993.\r"                            
[22] "0188 M. Lasserre (Matthieu, Claude, Pierre), né le 9 septembre 1993.\r"                                   
[23] "0189 Mlle Prie (Héloïse, Virginie, Marie-Anne), née le 19 mars 1994.\r"                                   
[24] "0190 M. Baumgarten (Clément), né le 18 janvier 1993.\r"                                                   
[25] "0191 M. Guizouarn (Luca, Michel, Charles), né le 1 septembre 1993.\r"                                     
[26] "0192 M. Debourdeau (Eloi, Grégoire, Marius), né le 22 juillet 1992.\r"                                    
[27] "0193 M. Dumoulin (Nicolas, Henri, Vincent), né le 11 juin 1993.\r"                                        
[28] "0194 M. Mernier (Thibaud, Georges, Martin), né le 9 mai 1994.\r"                                          
[29] "0195 M. Lambert (Simon, Pablo), né le 5 mai 1993.\r"                                                      
[30] "0196 Mme Mazeau (Lucile, Marie, Stanislava), née le 18 avril 1993.\r"                                     
[31] "0197 Mme Buzzi (Marie, Marguerite), née le 11 mai 1994.\r"                                                
[32] "0198 M. Manchart (Tom, Victor), né le 10 juillet 1993.\r"                                                 
[33] "0199 Mme Gillon (Elisa), née le 10 décembre 1993.\r"                                                      
[34] "0200 M. Roubertou (Yoann), né le 28 octobre 1992.\r"                                                      
[35] "0201 M. Chean (Dara, Paul), né le 7 septembre 1993.\r"                                                    
[36] "0202 M. Moutou (Alan, Nicolas), né le 21 mars 1994.\r"                                                    
[37] "0203 Mlle Zeriouh (Sarah), née le 1 juillet 1993.\r"                                                      
[38] "0204 M. Tuil (Anthony), né le 25 juillet 1992.\r"                                                         
[39] "0205 Mme Guyot D Asnieres De Salins (Victoire, Fleur, Marie), usage De Salins, née le 18 novembre 1993.\r"
[40] "0206 M. Massad (Pierre-Alexandre, Yves, Michel), né le 4 octobre 1993.\r"                                 
[41] "0207 M. Mifsud (François), né le 26 mars 1992.\r"                                                         
[42] "0208 Mme Faure (Pascale), née le 17 juillet 1992.\r"                                                      
[43] "0209 Mme Parrot (Laurène), née le 12 février 1994.\r"                                                     
[44] "0210 M. Rousseau (Theophile), né le 21 octobre 1992.\r"                                                   
[45] "0211 M. Nicolas (Eliot, Maurice, Jean Marie), né le 11 septembre 1990.\r"                                 
[46] "0212 Mlle Malézieux (Emilie, Anne, Marie), née le 14 juillet 1993.\r"                                     
[47] "0213 M. Bellier (Julien), né le 8 décembre 1992.\r"                                                       
[48] "0214 M. Machet (Thomas, René), né le 17 février 1993.\r"                                                  
[49] "0215 Mme Evain (Manon, Louise), née le 20 mars 1992.\r"                                                   
[50] "0216 Mme Lepee (Aurélie, Gabrielle, Joséphine), née le 17 octobre 1992.\r"                                
[51] "0217 Mme Bontemps (Emmanuelle, Chantal), née le 30 juillet 1994.\r"                                       
[52] "0218 Mlle Bel-Viel (Marion, Catherine), née le 19 septembre 1993.\r"                                      
[53] "0219 M. Duriez (Benjamin, Laurent), né le 18 décembre 1993.\r"                                            
[54] "0220 Mlle Patras (Marion), née le 3 décembre 1994.\r"                                                     
[55] "0221 M. Soulier (Theodore), né le 29 décembre 1990.\r"                                                    
[56] "0222 Mme Charret (Léa), née le 26 septembre 1993.\r"                                                      
[57] "0223 M. Courtin (Edouard), né le 24 janvier 1991.\r"                                                      
[58] "0224 M. Klein (Clément, Pierre), né le 3 juin 1992.\r"                                                    
[59] "0225 M. Helary (Alois), né le 23 juillet 1993.\r"                                                         
[60] "0226 M. Zito (Alexandre, Hugo), né le 21 septembre 1993.\r"                                               
[61] "0227 M. Blanckaert (Edouard, Philippe, Gilbert), né le 29 juillet 1993.\r"                                
[62] "0228 M. Pommerolle (Pierre, Adrien, Géry), né le 27 octobre 1993.\r"                                      
[63] "0229 M. Blachier (Mathieu, Philippe, Michel), né le 29 décembre 1993.\r"                                  
[64] "0230 Mme Louar (Margaux, Caroline), née le 4 décembre 1993.\r"                                            
[65] "0231 M. Baret (Florian), né le 18 septembre 1993.\r"                                                      
[66] "0232 Mme Serrar (Yasmine), née le 11 décembre 1994.\r"                                                    
[67] "0233 Mme Lozano (Aude, Virginie), née le 26 juillet 1993.\r"                                              
[68] "0234 M. Gomes De Pinho (Quentin), né le 10 mars 1993.\r"                                                  
[69] "                                                            4\r"                                          

[[5]]
 [1] "0235 Mme Lemonnier (Lisa), née le 17 septembre 1993.\r"                                    
 [2] "0236 Mme Faermark (Nicole, Esther), née le 15 mai 1992.\r"                                 
 [3] "0237 Mme Liu (Cécile, Feifei), née le 8 décembre 1993.\r"                                  
 [4] "0238 M. Majoulet (Alexandre, Elie, Boris), né le 15 décembre 1992.\r"                      
 [5] "0239 Mme Proux (Eve-Anne, Danièle, Colombe), née le 13 septembre 1993.\r"                  
 [6] "0240 Mme Macaire (Camille, Inès), née le 23 mai 1993.\r"                                   
 [7] "0241 Mme Foucard (Cendrine), née le 2 avril 1993.\r"                                       
 [8] "0242 Mme Grimal (Alice), née le 11 janvier 1992.\r"                                        
 [9] "0243 Mme Bourdin (Juliette, Isabelle), née le 19 décembre 1993.\r"                         
[10] "0244 Mme Labarre (Marion), née le 13 mai 1992.\r"                                          
[11] "0245 M. Belin (Cesar, Eric), né le 9 novembre 1993.\r"                                     
[12] "0246 M. Eyike (Julien, Michael), né le 2 février 1994.\r"                                  
[13] "0247 M. De Baynast De Septfontaines (Quentin, Jacques, Philippe), né le 16 février 1994.\r"
[14] "0248 M. Guerra (Xavier, Jacques, Antoine), né le 24 juillet 1992.\r"                       
[15] "0249 Mme Brunet (Dorothée), épouse Mariani, née le 25 juin 1991.\r"                        
[16] "0250 Mme Evin (Cécile), née le 17 mai 1993.\r"                                             
[17] "0251 Mlle Blum (Laurène, Yasmina, Claudia), née le 26 avril 1993.\r"                       
[18] "0252 M. Quehan (Romain), né le 18 novembre 1992.\r"                                        
[19] "0253 Mme Etchamendy (Aizane), née le 17 juin 1993.\r"                                      
[20] "0254 M. Figoni (Hugo), né le 1 juin 1992.\r"                                               
[21] "0255 Mme Crozet (Audrey), née le 6 décembre 1993.\r"                                       
[22] "0256 Mme Chung (Cécile, Koung Houy), née le 19 avril 1994.\r"                              
[23] "0257 Mlle Canu (Dorine, Lucie), née le 2 avril 1993.\r"                                    
[24] "0258 Mme Picard (Léa, Mélusine), née le 3 décembre 1992.\r"                                
[25] "0259 Mme Poulain (Cécile, Catherine, Madeleine), née le 25 mai 1993.\r"                    
[26] "0260 Mlle Dabbak (Imene), née le 2 novembre 1994.\r"                                       
[27] "0261 Mme Pourtau (Laetitia, Laure), née le 15 juillet 1993.\r"                             
[28] "0262 M. Desal (Raphaël, Jack, Henri), né le 31 janvier 1993.\r"                            
[29] "0263 Mme De Ganay (Marie), née le 12 novembre 1993.\r"                                     
[30] "0264 M. Pin (Grégoire, Jean, Edouard), né le 30 septembre 1993.\r"                         
[31] "0265 Mme Fernandes (Amélie, Laura), née le 22 novembre 1992.\r"                            
[32] "0266 Mme Tortonese (Sarah), née le 8 juin 1993.\r"                                         
[33] "0267 M. Ferrand (Benjamin, Christophe), né le 18 juillet 1993.\r"                          
[34] "0268 M. Varnier (Quentin, Rémy), né le 23 février 1992.\r"                                 
[35] "0269 Mme Cornée (Charlène, Anaïs), née le 9 décembre 1993.\r"                              
[36] "0270 Mme Masson (Laure), née le 10 mars 1993.\r"                                           
[37] "0271 Mme Chehem Daoud Chehem (Ferida), née le 3 février 1993.\r"                           
[38] "0272 Mme Trintignac (Mathilde, Isabelle, Sabine), née le 30 mars 1993.\r"                  
[39] "0273 Mlle Le Hir (Anne-Sophie, Marie, Mai Lan), née le 19 septembre 1989.\r"               
[40] "0274 M. Kielwasser (Gauthier, Christian, Léon), né le 23 janvier 1993.\r"                  
[41] "0275 Mme Frenais De Coutard (Alix, Marie Antoinette, Flamine), née le 9 août 1993.\r"      
[42] "0276 M. Grolleau (Emmanuel, Bernard, François), né le 17 mai 1993.\r"                      
[43] "0277 M. Tondu (Maxence), né le 9 avril 1993.\r"                                            
[44] "0278 Mme Mallart (Elise), née le 18 août 1993.\r"                                          
[45] "0279 M. Barbarroux (Aymeric), né le 20 décembre 1994.\r"                                   
[46] "0280 Mme Carrer (Mathilde, Marie), née le 28 juin 1993.\r"                                 
[47] "0281 Mme Nilles (Christelle), née le 19 avril 1993.\r"                                     
[48] "0282 Mme Martinet (Pauline, Juliette), née le 1 octobre 1993.\r"                           
[49] "0283 Mme Bhujoo-Spiegelhoff (Zoé), née le 1 août 1993.\r"                                  
[50] "0284 M. Receveur (Matthieu), né le 4 octobre 1994.\r"                                      
[51] "0285 M. Salardaine (Quentin), né le 24 juillet 1993.\r"                                    
[52] "0286 M. Gravier (Alban, Pierre), né le 19 mai 1993.\r"                                     
[53] "0287 Mme Miladi (Sarra), née le 11 mai 1994.\r"                                            
[54] "0288 M. Gobert (Baptiste, Valère), né le 21 avril 1994.\r"                                 
[55] "0289 M. L'official (Guillaume, Yvon, Jean-Pierre), né le 30 décembre 1993.\r"              
[56] "0290 M. Baba (Aki), né le 22 mai 1993.\r"                                                  
[57] "0291 Mme Leherpeur (Hélène, Thuy, Geneviève), née le 26 avril 1993.\r"                     
[58] "0292 Mme Roger (Clémentine), née le 9 décembre 1994.\r"                                    
[59] "0293 M. Di Felici (Fabien, Guy, Franz), né le 11 février 1992.\r"                          
[60] "0294 Mme M'henni (Rania), née le 21 octobre 1989.\r"                                       
[61] "0295 Mme Haikal (Christelle, Claude, Marie), née le 9 décembre 1993.\r"                    
[62] "0296 M. Gall (Emmanuel), né le 12 mars 1992.\r"                                            
[63] "0297 Mme Bardoux (Elise, Marie, Germaine), née le 10 septembre 1992.\r"                    
[64] "0298 Mlle Nykolyszyn (Charlotte), née le 25 août 1994.\r"                                  
[65] "0299 M. Dallery (Romain), né le 24 octobre 1993.\r"                                        
[66] "0300 M. Bon (Grégoire, Louis, Gerald), né le 17 novembre 1992.\r"                          
[67] "0301 M. Durrechou (Quentin), né le 20 juin 1991.\r"                                        
[68] "0302 M. Delemar (Victor, Philippe, Yves), né le 18 mars 1993.\r"                           
[69] "                                                             5\r"                          

[[6]]
 [1] "0303 M. Pantalacci (Thibaut, Luc, Maxime), né le 19 mai 1992.\r"           
 [2] "0304 M. Bauer (Jules, Henri, Claude), né le 8 juillet 1992.\r"             
 [3] "0305 Mme Zegrari (Samira), née le 24 juillet 1994.\r"                      
 [4] "0306 Mme Suc (Violette), née le 27 juillet 1993.\r"                        
 [5] "0307 Mme Marcin (Kennie, Chanice), née le 16 septembre 1992.\r"            
 [6] "0308 M. Crequit (Simon), né le 11 juin 1990.\r"                            
 [7] "0309 Mlle Le Guyader (Maud), née le 23 août 1994.\r"                       
 [8] "0310 Mme Auger (Apolline), née le 4 février 1994.\r"                       
 [9] "0311 Mme Trouillard (Marion, Evy), née le 11 septembre 1993.\r"            
[10] "0312 M. Henry (Guillaume), né le 13 mai 1993.\r"                           
[11] "0313 Mme Martin (Clarisse, Marie), née le 21 janvier 1994.\r"              
[12] "0314 M. Michel (Florian, Benoît, Valentin), né le 2 septembre 1993.\r"     
[13] "0315 M. Ejjelthi (Hamza), né le 20 avril 1994.\r"                          
[14] "0316 M. Crespin (Nikolas), né le 22 avril 1992.\r"                         
[15] "0317 M. Denis (Thomas, Louis, Henri), né le 8 juin 1993.\r"                
[16] "0318 Mme Winock (Auranne, Geneviève, Gisèle), née le 5 décembre 1993.\r"   
[17] "0319 M. Lassozé (Simon, Pierre), né le 2 mai 1993.\r"                      
[18] "0320 Mme Cordeil (Stéphanie, Jacqueline), née le 1 décembre 1986.\r"       
[19] "0321 Mme Trystram (Noémie), née le 17 décembre 1993.\r"                    
[20] "0322 M. Lanore (Aymeric, Christophe, Hervé), né le 19 décembre 1993.\r"    
[21] "0323 Mme Planty-Bonjour (Alexia), née le 30 janvier 1994.\r"               
[22] "0324 M. Sermet (Kevin), né le 22 avril 1993.\r"                            
[23] "0325 M. Cavaille (Mathieu), né le 21 mars 1992.\r"                         
[24] "0326 M. Hermitte (Laurent), né le 1 mars 1993.\r"                          
[25] "0327 M. Tredez (Grégoire, Émmanuel, Bernard), né le 6 décembre 1993.\r"    
[26] "0328 M. Daire (Emilien, Sacha, Louis), né le 31 juillet 1993.\r"           
[27] "0329 M. Monnin (Boris, Alain, Maxime), né le 14 mai 1993.\r"               
[28] "0330 Mlle Toubol (Amélie, Muriel), née le 4 mars 1994.\r"                  
[29] "0331 M. Aden Hassan (Anthony, Libaneayanleh, Aden), né le 25 avril 1994.\r"
[30] "0332 Mme Dangien (Ambre), née le 23 janvier 1993.\r"                       
[31] "0333 Mme Vennier (Alice), née le 1 mai 1994.\r"                            
[32] "0334 Mme Fort (Justine, Marie), née le 1 mai 1994.\r"                      
[33] "0335 Mme Fabre (Juliette, Marjolaine, Frédérique), née le 22 juin 1992.\r" 
[34] "0336 Mme Rouchosse (Ninon), née le 4 janvier 1993.\r"                      
[35] "0337 M. Le Boedec (Antoine, Pierre), né le 10 juin 1993.\r"                
[36] "0338 Mme Rolland (Maud, Marine, Anne), née le 15 mars 1993.\r"             
[37] "0339 Mme Ivashchenko (Véronique), née le 29 septembre 1992.\r"             
[38] "0340 M. Naveau (Thibaut, Marie), né le 21 octobre 1993.\r"                 
[39] "0341 M. Cervantes (Baptiste, Alain, Pierre), né le 4 novembre 1992.\r"     
[40] "0342 Mme Peyre (Marion, Noëlle), née le 24 janvier 1993.\r"                
[41] "0343 Mme Ortuno (Sofia, Elisabeth, Luce), née le 1 novembre 1993.\r"       
[42] "0344 Mme Uguen (Justine), née le 25 octobre 1993.\r"                       
[43] "0345 Mlle Addi (Alix, Marion, Suzanne), née le 12 novembre 1992.\r"        
[44] "0346 M. Cirotteau (Paul, Emile, Bernard), né le 22 novembre 1992.\r"       
[45] "0347 Mme Fuentes (Inès, Marie), née le 25 août 1994.\r"                    
[46] "0348 Mme Fitoussi (Léa, Gisèle), née le 24 juillet 1992.\r"                
[47] "0349 Mme Dedieu (Daphné), née le 4 avril 1992.\r"                          
[48] "0350 Mme Noret (Clara), née le 26 juin 1993.\r"                            
[49] "0351 M. Degouge (Théophane, Pierre, Alain), né le 21 juillet 1993.\r"      
[50] "0352 M. Lamirault (Rémy, Cédric), né le 9 juillet 1994.\r"                 
[51] "0353 Mme Devilliers (Marie-Juliette), née le 25 juillet 1993.\r"           
[52] "0354 M. Bernier (Raphaël, Jean-Sébastien, Pierre), né le 23 avril 1992.\r" 
[53] "0355 M. Testory (Maxime), né le 3 avril 1992.\r"                           
[54] "0356 Mme Augoyard (Laureline, Marie), née le 31 décembre 1993.\r"          
[55] "0357 M. Dubost (Tanguy, Bruno, Marie), né le 9 août 1992.\r"               
[56] "0358 Mme Plassais (Caroline), née le 8 février 1994.\r"                    
[57] "0359 Mme Kedra (Alice), née le 14 décembre 1993.\r"                        
[58] "0360 M. Voilque (Pierre), né le 24 mars 1993.\r"                           
[59] "0361 Mme O'shaughnessy (Emma, Chloé, Elaine), née le 5 juillet 1992.\r"    
[60] "0362 M. Touboul (Olivier), né le 23 septembre 1993.\r"                     
[61] "0363 M. Morlon (Quentin), né le 4 novembre 1993.\r"                        
[62] "0364 M. Bassompierre (Adrien), né le 12 novembre 1993.\r"                  
[63] "0365 Mlle Ménard (Coline, Elane), née le 7 décembre 1993.\r"               
[64] "0366 Mlle Kauffmann (Maëlis, Coralie, Anaïs), née le 7 décembre 1992.\r"   
[65] "0367 M. Abou Khalil (Kassem), né le 10 juillet 1993.\r"                    
[66] "0368 M. Dutertre (Martin, Victor), né le 18 septembre 1992.\r"             
[67] "0369 Mlle Dupessey (Florence), née le 26 février 1993.\r"                  
[68] "0370 Mme Delaie (Camille, Angele, Geneviève), née le 21 août 1993.\r"      
[69] "                                                             6\r"          
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_parsed }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\ConstantTok{NA\_character\_}\NormalTok{, }\FunctionTok{length}\NormalTok{(data), }\DecValTok{7}\NormalTok{)}
\NormalTok{data\_words }\OtherTok{\textless{}{-}} \FunctionTok{str\_extract\_all}\NormalTok{(data, }\FunctionTok{boundary}\NormalTok{(}\StringTok{"word"}\NormalTok{))}
\NormalTok{data\_parsed[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(}\FunctionTok{sapply}\NormalTok{(data\_words, head, }\AttributeTok{n =} \DecValTok{4}\NormalTok{))}
\NormalTok{data\_parsed[, }\DecValTok{5}\SpecialCharTok{:}\DecValTok{7}\NormalTok{] }\OtherTok{\textless{}{-}} \FunctionTok{t}\NormalTok{(}\FunctionTok{sapply}\NormalTok{(data\_words, tail, }\AttributeTok{n =} \DecValTok{3}\NormalTok{))}
\FunctionTok{head}\NormalTok{(data\_parsed)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     [,1] [,2]    [,3]  [,4]         [,5] [,6] [,7]
[1,] "c"  "Paris" "le"  "28"         "r"  "1"  "r" 
[2,] "c"  "0031"  "Mme" "Bourhis"    "r"  "2"  "r" 
[3,] "c"  "0099"  "Mme" "Meynard"    "r"  "3"  "r" 
[4,] "c"  "0167"  "M"   "Orion"      "r"  "4"  "r" 
[5,] "c"  "0235"  "Mme" "Lemonnier"  "r"  "5"  "r" 
[6,] "c"  "0303"  "M"   "Pantalacci" "r"  "6"  "r" 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_parsed2 }\OtherTok{\textless{}{-}} \FunctionTok{as\_tibble}\NormalTok{(data\_parsed) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{transmute}\NormalTok{(}
    \AttributeTok{ranking =} \FunctionTok{as.integer}\NormalTok{(V1),}
    \AttributeTok{is\_male =}\NormalTok{ (V2 }\SpecialCharTok{==} \StringTok{"M"}\NormalTok{),}
    \AttributeTok{family\_name =}\NormalTok{ V3,}
    \AttributeTok{first\_name =}\NormalTok{ V4,}
    \AttributeTok{birth\_date =} \FunctionTok{pmap}\NormalTok{(}\FunctionTok{list}\NormalTok{(V5, V6, V7), }\ControlFlowTok{function}\NormalTok{(d, m, y) \{}
      \FunctionTok{paste}\NormalTok{(d, m, y, }\AttributeTok{collapse =} \StringTok{" "}\NormalTok{)}
\NormalTok{    \}) }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ lubridate}\SpecialCharTok{::}\FunctionTok{dmy}\NormalTok{()}
\NormalTok{  ) }

\NormalTok{data\_parsed2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 124 x 5
   ranking is_male family_name first_name     birth_date
     <int> <lgl>   <chr>       <chr>          <date>    
 1      NA FALSE   le          28             NA        
 2      NA FALSE   Mme         Bourhis        NA        
 3      NA FALSE   Mme         Meynard        NA        
 4      NA FALSE   M           Orion          NA        
 5      NA FALSE   Mme         Lemonnier      NA        
 6      NA FALSE   M           Pantalacci     NA        
 7      NA FALSE   M           Jean           NA        
 8      NA FALSE   Mme         Chaligne       NA        
 9      NA FALSE   Mme         Brillat        NA        
10      NA FALSE   M           Meguerditchian NA        
# ... with 114 more rows
\end{verbatim}

Note: there is a problem with people who have a family name composed of multiple words.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Proportion male/female}
\FunctionTok{mean}\NormalTok{(data\_parsed2}\SpecialCharTok{$}\NormalTok{is\_male)}
\CommentTok{\# 43\% of males.}


\NormalTok{myggplot }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(...) bigstatsr}\SpecialCharTok{:::}\FunctionTok{MY\_THEME}\NormalTok{(}\FunctionTok{ggplot}\NormalTok{(...))}

\FunctionTok{myggplot}\NormalTok{(data\_parsed2) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ birth\_date), }\AttributeTok{bins =} \DecValTok{100}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

If a recent A-level graduate attempts the exam for the first time, they would be born in 1993. Due to the very competitive nature of the exam, a second and more attempts are also possible. Those who attempts the exam for the second time were born in 1992. Nonetheless, one can attempt the exam at any age, so a lot of older people are also appearing in the dataset.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{myggplot}\NormalTok{(}\FunctionTok{mutate}\NormalTok{(data\_parsed2, }\AttributeTok{prop\_male =} \FunctionTok{cummean}\NormalTok{(data\_parsed2}\SpecialCharTok{$}\NormalTok{is\_male))) }\SpecialCharTok{+} 
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \FunctionTok{mean}\NormalTok{(data\_parsed2}\SpecialCharTok{$}\NormalTok{is\_male), }\AttributeTok{col =} \StringTok{"red"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ ranking, }\AttributeTok{y =}\NormalTok{ prop\_male))}

\NormalTok{(}\FunctionTok{myggplot}\NormalTok{(data\_parsed2) }\SpecialCharTok{+}
   \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(ranking, birth\_date, }\AttributeTok{color =}\NormalTok{ is\_male)) }\SpecialCharTok{+}
   \FunctionTok{aes}\NormalTok{(}\AttributeTok{text =}\NormalTok{ bigstatsr}\SpecialCharTok{::}\FunctionTok{asPlotlyText}\NormalTok{(data\_parsed2))) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  plotly}\SpecialCharTok{::}\FunctionTok{ggplotly}\NormalTok{(}\AttributeTok{tooltip =} \StringTok{"text"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can see a female 19 year old (with a really high ranking!) and a 54-year old man (with a poor ranking).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{myggplot}\NormalTok{(data\_parsed2, }\FunctionTok{aes}\NormalTok{(ranking, birth\_date)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color =}\NormalTok{ is\_male), }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{webscraping-linkedin}{%
\chapter{Webscraping LinkedIn}\label{webscraping-linkedin}}

Did you know that the internet is the greatest source of publicly available data. One of the key skills to being able to obtain data from the web is ``web-scraping'', where you use a piece of software to run through a website and collect information.

This technique can be used for collecting data from databases or to collect data that is scattered across a website.

In the following scenario, you are being asked to extract from LinkedIn all the education qualifications and later compare them with those from your own company. Your senior management is seriously worried that the employees working for your main competitor are better skilled.

You are tasked to extract all the forenames, the surnames, the job titles and the education qualifications, including the ``from to''" dates and the education organisations.

In our example, we have chosen to look at \href{https://www.innocentdrinks.co.uk/}{Innocent}, the famous drink manufacturer. Innocent ranks third among ``The Sunday Times 100 Best Companies to Work For''.

From the \href{https://appointments.thetimes.co.uk/article/best100companies/}{Sunday Times}
you know that Innocent has 289 employees and that brand managers are earning salaries of around £50,000 plus bonuses. Furthermore the male/female ratio is 40:60, the average age is 39, the amount of voluntary leavers 15\% and the earning are over £35,000 for 70\% of its staff.

Innocent makes a convincing offer to ambitious types. Nicki Garland is the firm's team leader for UK and Ireland finance and enjoys the best of both worlds she gets at Innocent. ``I wanted to work for a brand that had purpose and meaning,'' she says. ``When you walk in and see a wall that says the company's mission is to `help people to live well and die old', you realise this is definitely a very different business.''

This will be your \href{https://www.linkedin.com/search/results/people/v2/?facetCurrentCompany=\%5B\%2224177\%22\%5D\&page=1}{starting page} of your web scraping. After a few checks, you realise that according to LinkedIn the company has 543 employees for a total of 55 pages of records. From looking at the urls you also realise that \%5B\%2224177\%22\%5D is the code used to identify Innocent by LinkedIn.

After lengthy googling on the internet, you find an excellent \href{https://www.datacamp.com/community/tutorials/r-web-scraping-rvest}{tutorial on DataCamp for webscraping} which does mostly what you plan to do, but unfortunately it is scraping Trustpilot, which on the top of things is a website that looks completely different to LinkedIn. You will need to adapt the code given in that tutorial, open each indivual LinkedIn profile for all 55 pages and scrape the required information. The final end product has to be a table which respects \href{https://www.jstatsoft.org/article/view/v059i10/v59i10.pdf}{``Hadley Wickham's tidy data rule''}

Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)     }\CommentTok{\# General purpose data wrangling}
\FunctionTok{library}\NormalTok{(rvest)         }\CommentTok{\# Parsing of html/xml files}
\FunctionTok{library}\NormalTok{(rebus)         }\CommentTok{\# Verbose regular expressions}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{get\_last\_page }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(html)\{}
    
\NormalTok{    pages\_data }\OtherTok{\textless{}{-}}\NormalTok{ html }\SpecialCharTok{\%\textgreater{}\%} 
        \FunctionTok{html\_nodes}\NormalTok{(}\StringTok{\textquotesingle{}.pagination{-}page\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# The \textquotesingle{}.\textquotesingle{} indicates the class}
        \FunctionTok{html\_text}\NormalTok{()                        }\CommentTok{\# Extracts the raw text as a list}
    
\NormalTok{    pages\_data[(}\FunctionTok{length}\NormalTok{(pages\_data)}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)] }\SpecialCharTok{\%\textgreater{}\%}             \CommentTok{\# The second to last of the buttons is the one}
        \FunctionTok{unname}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}                                     \CommentTok{\# Take the raw string}
        \FunctionTok{as.numeric}\NormalTok{()                                     }\CommentTok{\# Convert to number}
\NormalTok{\}}

\NormalTok{get\_reviews }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(html)\{}
\NormalTok{    html }\SpecialCharTok{\%\textgreater{}\%} 
        \FunctionTok{html\_nodes}\NormalTok{(}\StringTok{\textquotesingle{}.review{-}info\_\_body\_\_text\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}       \CommentTok{\# The relevant tag}
        \FunctionTok{html\_text}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
        \FunctionTok{str\_trim}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}                       \CommentTok{\# Trims additional white space}
        \FunctionTok{unlist}\NormalTok{()                             }\CommentTok{\# Converts the list into a vector}
\NormalTok{\}}

\NormalTok{get\_reviewer\_names }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(html)\{}
\NormalTok{    html }\SpecialCharTok{\%\textgreater{}\%} 
        \FunctionTok{html\_nodes}\NormalTok{(}\StringTok{\textquotesingle{}.consumer{-}info\_\_details\_\_name\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
        \FunctionTok{html\_text}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
        \FunctionTok{str\_trim}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
        \FunctionTok{unlist}\NormalTok{()}
\NormalTok{\}}

\NormalTok{get\_review\_dates }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(html)\{}
    
\NormalTok{    status }\OtherTok{\textless{}{-}}\NormalTok{ html }\SpecialCharTok{\%\textgreater{}\%} 
        \FunctionTok{html\_nodes}\NormalTok{(}\StringTok{\textquotesingle{}time\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
        \FunctionTok{html\_attrs}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}              \CommentTok{\# The status information is this time a tag attribute}
        \FunctionTok{map}\NormalTok{(}\DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}                    \CommentTok{\# Extracts the second element}
        \FunctionTok{unlist}\NormalTok{() }
    
\NormalTok{    dates }\OtherTok{\textless{}{-}}\NormalTok{ html }\SpecialCharTok{\%\textgreater{}\%} 
        \FunctionTok{html\_nodes}\NormalTok{(}\StringTok{\textquotesingle{}time\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
        \FunctionTok{html\_attrs}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
        \FunctionTok{map}\NormalTok{(}\DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
        \FunctionTok{ymd\_hms}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}                 \CommentTok{\# Using a lubridate function }
        \CommentTok{\# to parse the string into a datetime object}
        \FunctionTok{unlist}\NormalTok{()}
    
\NormalTok{    return\_dates }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{status =}\NormalTok{ status, }\AttributeTok{dates =}\NormalTok{ dates) }\SpecialCharTok{\%\textgreater{}\%}   \CommentTok{\# You combine the status and the date }
        \CommentTok{\# information to filter one via the other}
        \FunctionTok{filter}\NormalTok{(status }\SpecialCharTok{==} \StringTok{\textquotesingle{}ndate\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}              \CommentTok{\# Only these are actual reviews}
        \FunctionTok{pull}\NormalTok{(dates) }\SpecialCharTok{\%\textgreater{}\%}                            \CommentTok{\# Selects and converts to vector}
        \FunctionTok{as.POSIXct}\NormalTok{(}\AttributeTok{origin =} \StringTok{\textquotesingle{}1970{-}01{-}01 00:00:00\textquotesingle{}}\NormalTok{) }\CommentTok{\# Converts datetimes to POSIX objects}
    
    \CommentTok{\# The lengths still occasionally do not lign up. You then arbitrarily crop the dates to fit}
    \CommentTok{\# This can cause data imperfections, however reviews on one page are generally close in time)}
    
\NormalTok{    length\_reviews }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(}\FunctionTok{get\_reviews}\NormalTok{(html))}
    
\NormalTok{    return\_reviews }\OtherTok{\textless{}{-}} \ControlFlowTok{if}\NormalTok{ (}\FunctionTok{length}\NormalTok{(return\_dates)}\SpecialCharTok{\textgreater{}}\NormalTok{ length\_reviews)\{}
\NormalTok{        return\_dates[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{length\_reviews]}
\NormalTok{    \} }\ControlFlowTok{else}\NormalTok{\{}
\NormalTok{        return\_dates}
\NormalTok{    \}}
\NormalTok{    return\_reviews}
\NormalTok{\}}

\NormalTok{get\_star\_rating }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(html)\{}
    
    
    \CommentTok{\# The pattern we look for}
\NormalTok{    pattern }\OtherTok{=} \StringTok{\textquotesingle{}star{-}rating{-}\textquotesingle{}}\SpecialCharTok{\%R\%} \FunctionTok{capture}\NormalTok{(DIGIT)  }
    
\NormalTok{    vector }\OtherTok{\textless{}{-}}\NormalTok{ html }\SpecialCharTok{\%\textgreater{}\%} 
        \FunctionTok{html\_nodes}\NormalTok{(}\StringTok{\textquotesingle{}.star{-}rating\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
        \FunctionTok{html\_attr}\NormalTok{(}\StringTok{\textquotesingle{}class\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
        \FunctionTok{str\_match}\NormalTok{(pattern) }\SpecialCharTok{\%\textgreater{}\%} 
        \FunctionTok{map}\NormalTok{(}\DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
        \FunctionTok{as.numeric}\NormalTok{()  }
\NormalTok{    vector }\OtherTok{\textless{}{-}}\NormalTok{  vector[}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(vector)]}
\NormalTok{    vector }\OtherTok{\textless{}{-}}\NormalTok{ vector[}\DecValTok{3}\SpecialCharTok{:}\FunctionTok{length}\NormalTok{(vector)]}
\NormalTok{    vector}
\NormalTok{\}}

\NormalTok{get\_data\_table }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(html, company\_name)\{}
    
    \CommentTok{\# Extract the Basic information from the HTML}
\NormalTok{    reviews }\OtherTok{\textless{}{-}} \FunctionTok{get\_reviews}\NormalTok{(html)}
\NormalTok{    reviewer\_names }\OtherTok{\textless{}{-}} \FunctionTok{get\_reviewer\_names}\NormalTok{(html)}
\NormalTok{    dates }\OtherTok{\textless{}{-}} \FunctionTok{get\_review\_dates}\NormalTok{(html)}
\NormalTok{    ratings }\OtherTok{\textless{}{-}} \FunctionTok{get\_star\_rating}\NormalTok{(html)}
    
    \CommentTok{\# Minimum length}
\NormalTok{    min\_length }\OtherTok{\textless{}{-}} \FunctionTok{min}\NormalTok{(}\FunctionTok{length}\NormalTok{(reviews), }\FunctionTok{length}\NormalTok{(reviewer\_names), }\FunctionTok{length}\NormalTok{(dates), }\FunctionTok{length}\NormalTok{(ratings))}
    
    \CommentTok{\# Combine into a tibble}
\NormalTok{    combined\_data }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{reviewer =}\NormalTok{ reviewer\_names[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{min\_length],}
                            \AttributeTok{date =}\NormalTok{ dates[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{min\_length],}
                            \AttributeTok{rating =}\NormalTok{ ratings[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{min\_length],}
                            \AttributeTok{review =}\NormalTok{ reviews[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{min\_length]) }
    
    \CommentTok{\# Tag the individual data with the company name}
\NormalTok{    combined\_data }\SpecialCharTok{\%\textgreater{}\%} 
        \FunctionTok{mutate}\NormalTok{(}\AttributeTok{company =}\NormalTok{ company\_name) }\SpecialCharTok{\%\textgreater{}\%} 
        \FunctionTok{select}\NormalTok{(company, reviewer, date, rating, review)}
\NormalTok{\}}

\NormalTok{get\_data\_from\_url }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(url, company\_name)\{}
\NormalTok{    html }\OtherTok{\textless{}{-}} \FunctionTok{read\_html}\NormalTok{(url)}
    \FunctionTok{get\_data\_table}\NormalTok{(html, company\_name)}
\NormalTok{\}}

\NormalTok{scrape\_write\_table }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(url, company\_name)\{}
    
    \CommentTok{\# Read first page}
\NormalTok{    first\_page }\OtherTok{\textless{}{-}} \FunctionTok{read\_html}\NormalTok{(url)}
    
    \CommentTok{\# Extract the number of pages that have to be queried}
\NormalTok{    latest\_page\_number }\OtherTok{\textless{}{-}} \FunctionTok{get\_last\_page}\NormalTok{(first\_page)}
    
    \CommentTok{\# Generate the target URLs}
\NormalTok{    list\_of\_pages }\OtherTok{\textless{}{-}} \FunctionTok{str\_c}\NormalTok{(url, }\StringTok{\textquotesingle{}?page=\textquotesingle{}}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\NormalTok{latest\_page\_number)}
    
    \CommentTok{\# Apply the extraction and bind the individual results back into one table, }
    \CommentTok{\# which is then written as a tsv file into the working directory}
\NormalTok{    list\_of\_pages }\SpecialCharTok{\%\textgreater{}\%} 
        \FunctionTok{map}\NormalTok{(get\_data\_from\_url, company\_name) }\SpecialCharTok{\%\textgreater{}\%}  \CommentTok{\# Apply to all URLs}
        \FunctionTok{bind\_rows}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}                           \CommentTok{\# Combines the tibbles into one tibble}
        \FunctionTok{write\_tsv}\NormalTok{(}\FunctionTok{str\_c}\NormalTok{(company\_name,}\StringTok{\textquotesingle{}.tsv\textquotesingle{}}\NormalTok{))     }\CommentTok{\# Writes a tab separated file}
\NormalTok{\}}


\CommentTok{\# url \textless{}{-}\textquotesingle{}http://www.trustpilot.com/review/www.amazon.com\textquotesingle{}}
\CommentTok{\# scrape\_write\_table(url, \textquotesingle{}amazon\textquotesingle{})}
\CommentTok{\# amazon\_table \textless{}{-} read\_tsv(\textquotesingle{}amazon.tsv\textquotesingle{})}
\end{Highlighting}
\end{Shaded}

\hypertarget{flexdashboards}{%
\chapter{HR dashboards}\label{flexdashboards}}

Here we introduce flexdashboards

\hypertarget{data-science-product}{%
\chapter{HR Analytics product with Shiny}\label{data-science-product}}

Shiny is a very powerful framework for building web applications based on R. It is out of the scope of this book to make a comprehensive introduction to Shiny (which is too big a topic). We recommend that readers who are not familiar with Shiny learn more about it from the website \url{https://shiny.rstudio.com} before reading this chapter.

Unlike the more traditional workflow of creating static reports, you can create documents that allow your readers to change the parameters underlying your analysis and see the results immediately in Shiny R Markdown documents. In the example shown in Figure \citet{ref}(fig:shiny), the histogram will be automatically updated to reflect the number of bins selected by the reader.

A picture is worth a thousand words, and a Shiny document can potentially show you a thousand pictures as you interact with it. The readers are no longer tied to the fixed analysis and conclusions in the report. They may explore other possibilities by themselves, and possibly make new discoveries or draw different conclusions.

\cleardoublepage

\appendix

\hypertarget{appendixA}{%
\chapter{Statistical Background}\label{appendixA}}

\hypertarget{appendix-stat-terms}{%
\section{Basic statistical terms}\label{appendix-stat-terms}}

Note that all the following statistical terms apply only to \emph{numerical} variables, except the \emph{distribution} which can exist for both numerical and categorical variables.

\hypertarget{mean}{%
\subsection{Mean}\label{mean}}

The \emph{mean} is the most commonly reported measure of center. It is commonly called the \emph{average} though this term can be a little ambiguous. The mean is the sum of all of the data elements divided by how many elements there are. If we have \(n\) data points, the mean is given by:

\[Mean = \frac{x_1 + x_2 + \cdots + x_n}{n}\]

\hypertarget{median}{%
\subsection{Median}\label{median}}

The median is calculated by first sorting a variable's data from smallest to largest. After sorting the data, the middle element in the list is the \emph{median}. If the middle falls between two values, then the median is the mean of those two middle values.

\hypertarget{standard-deviation}{%
\subsection{Standard deviation}\label{standard-deviation}}

We will next discuss the \emph{standard deviation} (\(sd\)) of a variable. The formula can be a little intimidating at first but it is important to remember that it is essentially a measure of how far we expect a given data value will be from its mean:

\[sd = \sqrt{\frac{(x_1 - Mean)^2 + (x_2 - Mean)^2 + \cdots + (x_n - Mean)^2}{n - 1}}\]

\hypertarget{five-number-summary}{%
\subsection{Five-number summary}\label{five-number-summary}}

The \emph{five-number summary} consists of five summary statistics: the minimum, the first quantile AKA 25th percentile, the second quantile AKA median or 50th percentile, the third quantile AKA 75th, and the maximum. The five-number summary of a variable is used when constructing boxplots, as seen in Section \ref{boxplots}.

The quantiles are calculated as

\begin{itemize}
\tightlist
\item
  first quantile (\(Q_1\)): the median of the first half of the sorted data
\item
  third quantile (\(Q_3\)): the median of the second half of the sorted data
\end{itemize}

The \emph{interquartile range (IQR)}\index{interquartile range (IQR)} is defined as \(Q_3 - Q_1\) and is a measure of how spread out the middle 50\% of values are. The IQR corresponds to the length of the box in a boxplot.

The median and the IQR are not influenced by the presence of outliers in the ways that the mean and standard deviation are. They are, thus, recommended for skewed datasets. We say in this case that the median and IQR are more \emph{robust to outliers}.

\hypertarget{distribution}{%
\subsection{Distribution}\label{distribution}}

The \emph{distribution} of a variable shows how frequently different values of a variable occur. Looking at the visualization of a distribution can show where the values are centered, show how the values vary, and give some information about where a typical value might fall. It can also alert you to the presence of outliers.

Recall from Chapter \ref{viz} that we can visualize the distribution of a numerical variable using binning in a histogram and that we can visualize the distribution of a categorical variable using a barplot.

\hypertarget{outliers}{%
\subsection{Outliers}\label{outliers}}

\emph{Outliers} correspond to values in the dataset that fall far outside the range of ``ordinary'' values. In the context of a boxplot, by default they correspond to values below \(Q_1 - (1.5 \cdot IQR)\) or above \(Q_3 + (1.5 \cdot IQR)\).

\hypertarget{appendix-normal-curve}{%
\section{Normal distribution}\label{appendix-normal-curve}}

Let's next discuss one particular kind of distribution: \index{distribution!normal} \emph{normal distributions}. Such bell-shaped distributions are defined by two values: (1) the \emph{mean} \(\mu\) (``mu'') which locates the center of the distribution and (2) the \emph{standard deviation} \(\sigma\) (``sigma'') which determines the variation of the distribution. In Figure \ref{fig:normal-curves}, we plot three normal distributions where:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The solid normal curve has mean \(\mu = 5\) \& standard deviation \(\sigma = 2\).
\item
  The dotted normal curve has mean \(\mu = 5\) \& standard deviation \(\sigma = 5\).
\item
  The dashed normal curve has mean \(\mu = 15\) \& standard deviation \(\sigma = 2\).
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{HR_Analytics.live_files/figure-latex/normal-curves-1} 

}

\caption{Three normal distributions.}\label{fig:normal-curves}
\end{figure}

Notice how the solid and dotted line normal curves have the same center due to their common mean \(\mu\) = 5. However, the dotted line normal curve is wider due to its larger standard deviation of \(\sigma\) = 5. On the other hand, the solid and dashed line normal curves have the same variation due to their common standard deviation \(\sigma\) = 2. However, they are centered at different locations.

When the mean \(\mu\) = 0 and the standard deviation \(\sigma\) = 1, the normal distribution has a special name. It's called the \emph{standard normal distribution} or the \emph{\(z\)-curve}\index{distribution!standard normal}.

Furthermore, if a variable follows a normal curve, there are \emph{three rules of thumb} we can use:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  68\% of values will lie within \(\pm\) 1 standard deviation of the mean.
\item
  95\% of values will lie within \(\pm\) 1.96 \(\approx\) 2 standard deviations of the mean.
\item
  99.7\% of values will lie within \(\pm\) 3 standard deviations of the mean.
\end{enumerate}

Let's illustrate this on a standard normal curve in Figure \ref{fig:normal-rule-of-thumb}. The dashed lines are at -3, -1.96, -1, 0, 1, 1.96, and 3. These 7 lines cut up the x-axis into 8 segments. The areas under the normal curve for each of the 8 segments are marked and add up to 100\%. For example:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The middle two segments represent the interval -1 to 1. The shaded area above this interval represents 34\% + 34\% = 68\% of the area under the curve. In other words, 68\% of values.
\item
  The middle four segments represent the interval -1.96 to 1.96. The shaded area above this interval represents 13.5\% + 34\% + 34\% + 13.5\% = 95\% of the area under the curve. In other words, 95\% of values.
\item
  The middle six segments represent the interval -3 to 3. The shaded area above this interval represents 2.35\% + 13.5\% + 34\% + 34\% + 13.5\% + 2.35\% = 99.7\% of the area under the curve. In other words, 99.7\% of values.
\end{enumerate}

\begin{figure}

{\centering \includegraphics[width=0.8\linewidth]{HR_Analytics.live_files/figure-latex/normal-rule-of-thumb-1} 

}

\caption{Rules of thumb about areas under normal curves.}\label{fig:normal-rule-of-thumb}
\end{figure}

\vspace{0.1in}

\begin{learncheck}
\vspace{-0.15in}

\textbf{\emph{Learning check}} \vspace{-0.1in}
\end{learncheck}

Say you have a normal distribution with mean \(\mu = 6\) and standard deviation \(\sigma = 3\).

\textbf{(LCA.1)} What proportion of the area under the normal curve is less than 3? Greater than 12? Between 0 and 12?

\textbf{(LCA.2)} What is the 2.5th percentile of the area under the normal curve? The 97.5th percentile? The 100th percentile?

\begin{learncheck}
\vspace{-0.25in}
\vspace{-0.25in}
\end{learncheck}

\vspace{0.1in}

\hypertarget{appendix-log10-transformations}{%
\section{log10 transformations}\label{appendix-log10-transformations}}

At its simplest, log10 transformations return base 10 \emph{logarithms}. For example, since \(1000 = 10^3\), running \texttt{log10(1000)} returns \texttt{3} in R. To undo a log10 transformation, we raise 10 to this value. For example, to undo the previous log10 transformation and return the original value of 1000, we raise 10 to the power of 3 by running \texttt{10\^{}(3)\ =\ 1000} in R. \index{log transformations}

Log transformations allow us to focus on changes in \emph{orders of magnitude}. In other words, they allow us to focus on \emph{multiplicative changes} instead of \emph{additive ones}. Let's illustrate this idea in Table \ref{tab:logten} with examples of prices of consumer goods in 2019 US dollars.

\begin{table}[!h]

\caption{\label{tab:logten}log10 transformed prices, orders of magnitude, and examples}
\centering
\fontsize{10}{12}\selectfont
\begin{tabular}[t]{lrll}
\toprule
Price & log10(Price) & Order of magnitude & Examples\\
\midrule
\$1 & 0 & Singles & Cups of coffee\\
\$10 & 1 & Tens & Books\\
\$100 & 2 & Hundreds & Mobile phones\\
\$1,000 & 3 & Thousands & High definition TVs\\
\$10,000 & 4 & Tens of thousands & Cars\\
\$100,000 & 5 & Hundreds of thousands & Luxury cars and houses\\
\$1,000,000 & 6 & Millions & Luxury houses\\
\bottomrule
\end{tabular}
\end{table}

Let's make some remarks about log10 transformations based on Table \ref{tab:logten}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  When purchasing a cup of coffee, we tend to think of prices ranging in single dollars, such as \$2 or \$3. However, when purchasing a mobile phone, we don't tend to think of their prices in units of single dollars such as \$313 or \$727. Instead, we tend to think of their prices in units of hundreds of dollars like \$300 or \$700. Thus, cups of coffee and mobile phones are of different \emph{orders of magnitude} in price.
\item
  Let's say we want to know the log10 transformed value of \$76. This would be hard to compute exactly without a calculator. However, since \$76 is between \$10 and \$100 and since log10(10) = 1 and log10(100) = 2, we know log10(76) will be between 1 and 2. In fact, log10(76) is 1.880814.
\item
  log10 transformations are \emph{monotonic}, meaning they preserve orders. So if Price A is lower than Price B, then log10(Price A) will also be lower than log10(Price B).
\item
  Most importantly, increments of one in log10-scale correspond to \emph{relative multiplicative changes} in the original scale and not \emph{absolute additive changes}. For example, increasing a log10(Price) from 3 to 4 corresponds to a multiplicative increase by a factor of 10: \$100 to \$1000.
\end{enumerate}

\hypertarget{appendixE}{%
\chapter{Versions of R Packages Used}\label{appendixE}}

If you are seeing different results than what is in the book, we recommend installing the exact version of the packages we used. This can be done by first installing the \texttt{remotes} package via \texttt{install.packages("remotes")}. Then, use \texttt{install\_version()} replacing the \texttt{package} argument with the package name in quotes and the \texttt{version} argument with the particular version number to install.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{remotes}\SpecialCharTok{::}\FunctionTok{install\_version}\NormalTok{(}\AttributeTok{package =} \StringTok{"skimr"}\NormalTok{, }\AttributeTok{version =} \StringTok{"1.0.6"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begingroup\fontsize{9}{11}\selectfont

\begin{longtable}{ll}
\toprule
package & version\\
\midrule
bigstatsr & 1.2.3\\
bitops & 1.0-6\\
bookdown & 0.21\\
broom & 0.7.1\\
data.table & 1.13.0\\
devtools & 2.3.2\\
dials & 0.0.9\\
dplyr & 1.0.2\\
fivethirtyeight & 0.6.1\\
forcats & 0.5.0\\
gapminder & 0.3.0\\
ggplot2 & 3.3.2\\
ggplot2movies & 0.0.1\\
ggrepel & 0.8.2\\
ggthemes & 4.2.0\\
googleway & 2.7.1\\
gridExtra & 2.3\\
httr & 1.4.2\\
ibmsunburst & 0.1.1\\
igraph & 1.2.6\\
infer & 0.5.3\\
ISLR & 1.2\\
janitor & 2.0.1\\
kableExtra & 1.2.1\\
knitr & 1.30\\
lattice & 0.20-38\\
lubridate & 1.7.9\\
MASS & 7.3-51.4\\
memoise & 1.1.0\\
modeldata & 0.0.2\\
modelr & 0.1.8\\
moderndive & 0.5.0\\
mvtnorm & 1.1-1\\
nnet & 7.3-12\\
nycflights13 & 1.0.1\\
parsnip & 0.1.3\\
patchwork & 1.0.1\\
pdftools & 2.3.1\\
plotly & 4.9.2.1\\
pROC & 1.16.2\\
purrr & 0.3.4\\
rattle & 5.4.0\\
readr & 1.4.0\\
rebus & 0.1-3\\
recipes & 0.1.14\\
rpart & 4.1-15\\
rpart.plot & 3.0.9\\
rsample & 0.0.8\\
rvest & 0.3.6\\
scales & 1.1.1\\
sessioninfo & 1.1.1\\
skimr & 2.1.2\\
stringr & 1.4.0\\
tibble & 3.0.4\\
tidymodels & 0.1.1\\
tidyr & 1.1.2\\
tidyverse & 1.3.0\\
tune & 0.1.1\\
usethis & 1.6.3\\
viridis & 0.5.1\\
viridisLite & 0.3.0\\
workflows & 0.2.1\\
xml2 & 1.3.2\\
yardstick & 0.0.7\\
\bottomrule
\end{longtable}
\endgroup{}

\vspace{-2in}

\hypertarget{appendixF}{%
\chapter{Archive HR datasets}\label{appendixF}}

The appendix describes the datasets used in this companion book.

\hypertarget{gender_pay_gap}{%
\section{Gender Pay Gap}\label{gender_pay_gap}}

The Gender Pay Gap dataset comes from the ``Glassdor Research'' website. It is contains the salary details for an hypothetical employer with 1,000 employees, spread across 10 job roles and 5 company departments.

The dataset can be accessed using:

``\url{https://glassdoor.box.com/shared/static/beukjzgrsu35fqe59f7502hruribd5tt.csv}''

Here are sample rows from this dataset:

jobTitle

gender

age

perfEval

edu

dept

seniority

basePay

bonus

Graphic Designer

Female

18

5

College

Operations

2

42363

9938

Software Engineer

Male

21

5

College

Management

5

108476

11128

Warehouse Associate

Female

19

4

PhD

Administration

5

90208

9268

Software Engineer

Male

20

5

Masters

Sales

4

108080

10154

Graphic Designer

Male

26

5

Masters

Engineering

5

99464

9319

IT

Female

20

5

PhD

Operations

4

70890

10126

\hypertarget{overhead}{%
\section{Overhead value analysis}\label{overhead}}

\hypertarget{service_desk_data}{%
\section{HR Service Desk}\label{service_desk_data}}

There are two publicly available datasets on the HR service desk.

``\url{https://www.ibm.com/communities/analytics/watson-analytics-blog/it-help-desk/}''"

``\url{https://www.kaggle.com/lyndonsundmark/service-request-analysis/data}''"

The datasets can be accessed using:

``\url{https://community.watsonanalytics.com/wp-content/uploads/2015/03/WA_Fn-UseC_-IT-Help-Desk.xlsx}''

Here are sample rows from this dataset:

The following 5 lines are not working, so I commented them until I have time to look into it. Hendrik
\#require(gdata)
\#servicedesk \textless- read.xls(``\url{https://community.watsonanalytics.com/wp-content/uploads/2015/03/WA_Fn-UseC_-IT-Help-Desk.xlsx}'', sheet = 1, header = TRUE, method=``csv'')
\#knitr::kable(head(servicedesk), ``html'')

\hypertarget{HRrecruitment}{%
\section{HR recruitment, selection and performance data}\label{HRrecruitment}}

Large dataset of selected HR applicants and performance data purchased from the Data and Sons website. Row Count: 1312450

IMPORTANT: this file was generated solely for pedagogical purposes. Due to the method of generation (R: BinOrdNonNor), it should NOT be used for research purposes. Note that the files will need to be joined in order to fully explore most relevant questions. This was intentionally left to the students to do as an exercise in order to further develop relevant skills. Selection Data Description provides a description of the variables contained in each of the remaining files.

Dataset Terms \& Conditions: Creative Commons Attribution-ShareAlike 4.0 International Public License

There are two publicly available datasets on the HR service desk.

``\url{https://www.dataandsons.com/dataset/preview/90}''

\hypertarget{job-classification-1}{%
\section{Job classification}\label{job-classification-1}}

The Job classification dataset comes from a blog article from Lyndon Sundmark. It is contains the salary details for an hypothetical employer with 1,000 employees, spread across 10 job roles and 5 company departments.

The dataset can be accessed using:

\url{https://onedrive.live.com/?authkey=\%21ABv-gHg5jVluYpc\&cid=4EF2CCBEDB98D0F5\&id=4EF2CCBEDB98D0F5\%216440\&parId=4EF2CCBEDB98D0F5\%216433\&o=OneUp}

Here are ten sample rows from this dataset:

ID JobFamily JobFamilyDescription JobClass JobClassDescription PayGrade\\
-- --------- -------------------- -------- ------------------- --------

EducationLevel Experience OrgImpact ProblemSolving Supervision ContactLevel FinancialBudget PG
-------------- ---------- --------- -------------- ----------- ------------ --------------- --

\hypertarget{absenteeism-at-work}{%
\section{Absenteeism at work}\label{absenteeism-at-work}}

The Abesnteeism at work dataset can be accessed from the UC Irvine Machine Learning Repository. The data set allows for several new combinations of attributes and attribute exclusions, or the modification of the attribute type (categorical, integer, or real) depending on the purpose of the research.

The dataset can be accessed using:

\url{https://archive.ics.uci.edu/ml/datasets/Absenteeism+at+work}

Here are ten sample rows from this dataset:

ID JobFamily JobFamilyDescription JobClass JobClassDescription PayGrade\\
-- --------- -------------------- -------- ------------------- --------

EducationLevel Experience OrgImpact ProblemSolving Supervision ContactLevel FinancialBudget PG
-------------- ---------- --------- -------------- ----------- ------------ --------------- --

The database was created with records of absenteeism at work from July 2007 to July 2010 at a courier company in Brazil.

Creators original owner and donors: Andrea Martiniano (1), Ricardo Pinto Ferreira (2), and Renato Jose Sassi (3).

E-mail address:
\href{mailto:andrea.martiniano@gmail.com}{\nolinkurl{andrea.martiniano@gmail.com}} (1) - PhD student;
\href{mailto:log.kasparov@gmail.com}{\nolinkurl{log.kasparov@gmail.com}} (2) - PhD student;
\href{mailto:sassi@uni9.pro.br}{\nolinkurl{sassi@uni9.pro.br}} (3) - Prof.~Doctor.

Universidade Nove de Julho - Postgraduate Program in Informatics and Knowledge Management.

Address: Rua Vergueiro, 235/249 Liberdade, Sao Paulo, SP, Brazil. Zip code: 01504-001.

Website: \url{http://www.uninove.br/curso/informatica-e-gestao-do-conhecimento/}

\backmatter

  \bibliography{bib/books.bib,bib/packages.bib,bib/articles.bib}

\backmatter
\printindex

\end{document}
